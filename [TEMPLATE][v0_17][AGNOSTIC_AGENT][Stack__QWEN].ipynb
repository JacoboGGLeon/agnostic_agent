{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqZU7tnBLHH"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zbpyIXulE3sS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.4/948.4 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Building wheel for langchain-qwq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m584.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h✅ Dependencias base instaladas:\n",
            "   - vLLM / Qwen wrapper / LangChain / LangGraph\n",
            "   - Pydantic 2.x + PyYAML (setup.yaml como panel de control)\n",
            "   - Streamlit\n",
            "   - sqlite-vec + pandas + numpy para KB tabular/vectorial sobre SQLite\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# @title INSTALL – Infra Qwen3 + LangGraph + Tabular KB (SQLite-vec) + setup.yaml\n",
        "# ============================================================\n",
        "# %%capture\n",
        "# LLM / Infra Qwen3 + vLLM (razonamiento + tool-calling)\n",
        "!pip -q install \"vllm>=0.9.0\" \"huggingface_hub>=0.23.0\" \"openai==1.108.2\" \"jedi>=0.16\"\n",
        "!pip -q install \"transformers>=4.57.3,<5\"\n",
        "!pip -q install langchain langchain-core langchain-openai langgraph typing_extensions\n",
        "!pip -q install \"git+https://github.com/whynpc9/langchain-qwq-vllm.git\"\n",
        "\n",
        "# Pydantic 2.x + YAML para setup.yaml + frontend mínimo\n",
        "!pip -q install \"pydantic>=2.7.0\" \"pyyaml>=6.0\"\n",
        "!pip -q install streamlit\n",
        "\n",
        "# SQLite / VDB tabular (para KBs tipo FAOSTAT_SQLITE / FAOSTAT_VECTOR)\n",
        "!pip -q install sqlite-vec pandas numpy\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import sys\n",
        "import time\n",
        "import subprocess\n",
        "import json\n",
        "import socket\n",
        "import urllib.request\n",
        "\n",
        "import streamlit  # noqa: F401\n",
        "import sqlite3    # stdlib\n",
        "import yaml       # para leer setup.yaml\n",
        "\n",
        "print(\"✅ Dependencias base instaladas:\")\n",
        "print(\"   - vLLM / Qwen wrapper / LangChain / LangGraph\")\n",
        "print(\"   - Pydantic 2.x + PyYAML (setup.yaml como panel de control)\")\n",
        "print(\"   - Streamlit\")\n",
        "print(\"   - sqlite-vec + pandas + numpy para KB tabular/vectorial sobre SQLite\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:41 .\n",
            "drwxr-xr-x 1 root root 4096 Feb  6 19:38 ..\n",
            "drwxr-xr-x 4 root root 4096 Dec  9 14:41 .config\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:42 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "ZJsvrZxnMqRG"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# @title `agnostic_agent`\n",
        "# ============================================================\n",
        "!mkdir -p agnostic_agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lUAKrJ6KPNRv",
        "outputId": "f8ea43e8-2ef3-481c-a1ca-32e25d705ff4"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/schemas.py\n",
        "%%writefile agnostic_agent/schemas.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Esquemas internos del Agnostic Deep Agent 2026.\n",
        "\n",
        "Aquí definimos el \"lenguaje interno\" del grafo:\n",
        "\n",
        "- AnalyzerIntent      → resultado del sub-grafo ANALYZER.\n",
        "- PlannerPlan         → plan de pasos del sub-grafo PLANNER.\n",
        "- ValidationResult    → veredicto del sub-grafo VALIDATOR.\n",
        "- MemoryContext       → contexto recuperado por MEMORY_READ.\n",
        "- AgentState          → TypedDict compartido por todos los nodos del grafo.\n",
        "\n",
        "NOTA:\n",
        "- Estos modelos NO son el I/O externo; para eso está `communication.py`.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Any, Dict, List, Optional, TypedDict\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "from .communication import ToolRun, AgentSummary\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# ANALYZER\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class RequiredItem(BaseModel):\n",
        "    \"\"\"\n",
        "    Ítem que DEBE quedar cubierto en la respuesta final.\n",
        "\n",
        "    - id: identificador corto (ej. \"q1\", \"step_b\", ...).\n",
        "    - description: texto que describe qué debe resolverse.\n",
        "    - must_be_answered: si es obligatorio o sólo deseable.\n",
        "    \"\"\"\n",
        "    id: str = Field(..., description=\"Identificador corto del ítem requerido.\")\n",
        "    description: str = Field(..., description=\"Descripción del ítem requerido.\")\n",
        "    must_be_answered: bool = Field(\n",
        "        default=True,\n",
        "        description=\"Si True, la respuesta final DEBE cubrir este ítem.\",\n",
        "    )\n",
        "\n",
        "\n",
        "class AnalyzerIntent(BaseModel):\n",
        "    \"\"\"\n",
        "    Resultado del ANALYZER:\n",
        "\n",
        "    - logic_form: representación lógica (ej. 'q1 ∧ q2 ∧ (q3 ∨ q4)').\n",
        "    - subqueries: lista de subconsultas / subtareas en texto.\n",
        "    - required_items: lista de RequiredItem que SUMMARIZER debe cubrir.\n",
        "    - wants_tool_trace: si el usuario quiere ver trazas técnicas.\n",
        "    - language: idioma dominante del usuario (si se detecta).\n",
        "    \"\"\"\n",
        "    logic_form: str = \"\"\n",
        "    subqueries: List[str] = Field(default_factory=list)\n",
        "    required_items: List[RequiredItem] = Field(default_factory=list)\n",
        "    wants_tool_trace: bool = False\n",
        "    language: Optional[str] = None\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# PLANNER\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class PlannedStep(BaseModel):\n",
        "    \"\"\"\n",
        "    Un paso del plan del PLANNER.\n",
        "\n",
        "    - id: identificador único del step (ej. 'step1').\n",
        "    - tool_name: nombre de la tool a invocar.\n",
        "    - args: argumentos (posiblemente con referencias tipo '$stepX.result').\n",
        "    - depends_on: ids de otros steps de los que depende.\n",
        "    \"\"\"\n",
        "    id: str\n",
        "    tool_name: str\n",
        "    args: Dict[str, Any] = Field(default_factory=dict)\n",
        "    depends_on: List[str] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "class PlannerPlan(BaseModel):\n",
        "    \"\"\"\n",
        "    Plan completo devuelto por el PLANNER como DAG de steps.\n",
        "    \"\"\"\n",
        "    steps: List[PlannedStep] = Field(default_factory=list)\n",
        "    notes: Optional[str] = None\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# VALIDATOR\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class ValidationResult(BaseModel):\n",
        "    \"\"\"\n",
        "    Resultado del VALIDATOR:\n",
        "\n",
        "    - all_covered: True si todos los RequiredItem se consideran cubiertos.\n",
        "    - missing_item_ids: ids de RequiredItem que faltan por cubrir.\n",
        "    - comments: texto adicional (por ejemplo, sugerencias al SUMMARIZER).\n",
        "    \"\"\"\n",
        "    all_covered: bool\n",
        "    missing_item_ids: List[str] = Field(default_factory=list)\n",
        "    comments: Optional[str] = None\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# MEMORY\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class MemoryContext(BaseModel):\n",
        "    \"\"\"\n",
        "    Contexto de memoria agregado por MEMORY_READ.\n",
        "\n",
        "    - session_history: extractos de la conversación actual.\n",
        "    - short_term_snippets: resúmenes o puntos clave recientes.\n",
        "    - long_term_snippets: recuerdos persistentes relevantes (vector store, etc.).\n",
        "    \"\"\"\n",
        "    session_history: List[str] = Field(default_factory=list)\n",
        "    short_term_snippets: List[str] = Field(default_factory=list)\n",
        "    long_term_snippets: List[str] = Field(default_factory=list)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# STATE GLOBAL (LangGraph)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class AgentState(TypedDict, total=False):\n",
        "    \"\"\"\n",
        "    Estado compartido entre nodos del grafo.\n",
        "\n",
        "    Claves más relevantes:\n",
        "    - user_prompt: texto original del usuario (último turno).\n",
        "    - session_id, user_id: identificadores lógicos.\n",
        "    - setup_path: ruta al setup.yaml activo.\n",
        "    - kb_names: nombres de KBs solicitadas por el usuario.\n",
        "\n",
        "    - memory_context: MemoryContext completado por MEMORY_READ.\n",
        "    - analyzer_intent: AnalyzerIntent generado por ANALYZER.\n",
        "    - planner_plan: PlannerPlan generado por PLANNER.\n",
        "\n",
        "    - step_results: resultados crudos por id de step.\n",
        "    - tool_runs: lista de ToolRun (normalizados en CATCHER).\n",
        "    - validation: ValidationResult del VALIDATOR.\n",
        "\n",
        "    - pipeline_summary: AgentSummary opcional con resumen por fase.\n",
        "    - user_out / deep_out / dev_out: textos finales por vista.\n",
        "    \"\"\"\n",
        "    # Identificadores y config\n",
        "    user_prompt: str\n",
        "    session_id: str\n",
        "    user_id: Optional[str]\n",
        "    setup_path: str\n",
        "    kb_names: List[str]\n",
        "\n",
        "    # Contexto de memoria\n",
        "    memory_context: MemoryContext\n",
        "\n",
        "    # Resultados de sub-grafos\n",
        "    analyzer_intent: AnalyzerIntent\n",
        "    planner_plan: PlannerPlan\n",
        "    validation: ValidationResult\n",
        "\n",
        "    # Ejecución de tools\n",
        "    step_results: Dict[str, Any]\n",
        "    tool_runs: List[ToolRun]\n",
        "\n",
        "    # Resúmenes y salidas finales\n",
        "    pipeline_summary: AgentSummary\n",
        "    user_out: str\n",
        "    deep_out: str\n",
        "    dev_out: str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "nE2TmR63PYqs",
        "outputId": "0fe711e6-7f17-4fc4-be04-e8de59cf494b"
      },
      "outputs": [],
      "source": [
        "# @title agnostic_agent/memory.py\n",
        "%%writefile agnostic_agent/memory.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Memoria del Agnostic Deep Agent 2026.\n",
        "\n",
        "Diseño multi-nivel (versión mínima):\n",
        "\n",
        "- Session memory:\n",
        "    Historial reciente de la conversación (texto plano).\n",
        "- Short-term memory:\n",
        "    Snippets/resúmenes recientes cada N turnos.\n",
        "- Long-term memory:\n",
        "    Placeholder para recuerdos persistentes (vector store, SQL, etc.).\n",
        "\n",
        "Este módulo NO depende de LangGraph directamente; sólo expone funciones\n",
        "para leer/escribir memoria usando estructuras sencillas, en términos de:\n",
        "\n",
        "- session_id: identificador de la conversación.\n",
        "- MemoryContext: modelo Pydantic definido en schemas.py.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Any, Dict, List, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "from .schemas import MemoryContext\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Almacenamiento in-memory (versión Colab / prototipo)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "# Historial de mensajes por sesión (texto plano)\n",
        "#   _SESSION_STORE[session_id] = [\"USER: ...\", \"AGENT: ...\", ...]\n",
        "_SESSION_STORE: Dict[str, List[str]] = {}\n",
        "\n",
        "# Snippets/resúmenes de corto plazo por sesión\n",
        "#   _SHORT_TERM_STORE[session_id] = [\"resumen 1\", \"resumen 2\", ...]\n",
        "_SHORT_TERM_STORE: Dict[str, List[str]] = {}\n",
        "\n",
        "# Long-term docs por usuario (placeholder; lo normal será un vector store)\n",
        "#   _LONG_TERM_STORE[user_id] = [\n",
        "#       {\"text\": \"...\", \"created_at\": \"...\", \"meta\": {...}},\n",
        "#       ...\n",
        "#   ]\n",
        "_LONG_TERM_STORE: Dict[str, List[Dict[str, Any]]] = {}\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Helpers internos\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _now_iso() -> str:\n",
        "    return datetime.utcnow().isoformat() + \"Z\"\n",
        "\n",
        "\n",
        "def _truncate_list(items: List[Any], max_len: int) -> List[Any]:\n",
        "    if max_len <= 0:\n",
        "        return []\n",
        "    if len(items) <= max_len:\n",
        "        return items\n",
        "    return items[-max_len:]\n",
        "\n",
        "\n",
        "def _build_short_term_snippet(user_text: str, agent_text: str, turn_idx: int) -> str:\n",
        "    \"\"\"\n",
        "    Snippet simple de corto plazo: pensado como placeholder.\n",
        "\n",
        "    Puedes reemplazar esto más adelante por un LLM summarizer dedicado.\n",
        "    \"\"\"\n",
        "    u = user_text.strip().replace(\"\\n\", \" \")\n",
        "    a = agent_text.strip().replace(\"\\n\", \" \")\n",
        "    if len(u) > 120:\n",
        "        u = u[:117] + \"...\"\n",
        "    if len(a) > 160:\n",
        "        a = a[:157] + \"...\"\n",
        "    return f\"[turn {turn_idx}] USER: {u} | AGENT: {a}\"\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Lectura de memoria\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def read_memory(\n",
        "    session_id: str,\n",
        "    *,\n",
        "    max_session_messages: int = 50,\n",
        "    max_short_snippets: int = 20,\n",
        ") -> MemoryContext:\n",
        "    \"\"\"\n",
        "    Construye un MemoryContext para una sesión dada.\n",
        "\n",
        "    - max_session_messages: cuántas líneas de historial traer (USER/AGENT mezcladas).\n",
        "    - max_short_snippets:  cuántos snippets de short-term traer.\n",
        "    \"\"\"\n",
        "    session_msgs = _SESSION_STORE.get(session_id, [])\n",
        "    short_snips = _SHORT_TERM_STORE.get(session_id, [])\n",
        "\n",
        "    session_history = _truncate_list(session_msgs, max_session_messages)\n",
        "    short_term_snippets = _truncate_list(short_snips, max_short_snippets)\n",
        "\n",
        "    # Long-term: de momento no filtramos por relevancia aquí; eso se hará\n",
        "    # en otro módulo (vector store, RAG, etc.). Por ahora sólo devolvemos textos.\n",
        "    long_term_texts: List[str] = []\n",
        "    # Nota: la clave para LONG_TERM_STORE es user_id, no session_id;\n",
        "    # aquí no lo conocemos, así que devolvemos lista vacía.\n",
        "    # Más adelante se puede ampliar la firma para incluir user_id.\n",
        "\n",
        "    return MemoryContext(\n",
        "        session_history=session_history,\n",
        "        short_term_snippets=short_term_snippets,\n",
        "        long_term_snippets=long_term_texts,\n",
        "    )\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Escritura / actualización de memoria\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def write_memory(\n",
        "    session_id: str,\n",
        "    *,\n",
        "    user_prompt: str,\n",
        "    user_out: str,\n",
        "    user_id: Optional[str] = None,\n",
        "    memory_cfg: Optional[Dict[str, Any]] = None,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Actualiza las memorias tras un turno completado.\n",
        "\n",
        "    Parámetros:\n",
        "    - session_id: id de sesión.\n",
        "    - user_prompt: último mensaje del usuario.\n",
        "    - user_out: respuesta final del agente para ese turno.\n",
        "    - user_id: id lógico del usuario (para long-term).\n",
        "    - memory_cfg: configuración opcional con la forma:\n",
        "\n",
        "        {\n",
        "          \"session\": {\n",
        "            \"enabled\": true,\n",
        "            \"max_messages\": 50,\n",
        "          },\n",
        "          \"short_term\": {\n",
        "            \"enabled\": true,\n",
        "            \"summarize_every_n_turns\": 5,\n",
        "            \"window_turns\": 20,\n",
        "          },\n",
        "          \"long_term\": {\n",
        "            \"enabled\": false,\n",
        "            \"max_docs\": 200,\n",
        "          }\n",
        "        }\n",
        "\n",
        "      Si algún bloque o campo no está presente, se usan valores por defecto.\n",
        "    \"\"\"\n",
        "    cfg = memory_cfg or {}\n",
        "\n",
        "    sess_cfg = cfg.get(\"session\", {}) or {}\n",
        "    st_cfg = cfg.get(\"short_term\", {}) or {}\n",
        "    lt_cfg = cfg.get(\"long_term\", {}) or {}\n",
        "\n",
        "    session_enabled: bool = bool(sess_cfg.get(\"enabled\", True))\n",
        "    max_messages: int = int(sess_cfg.get(\"max_messages\", 50))\n",
        "\n",
        "    short_enabled: bool = bool(st_cfg.get(\"enabled\", True))\n",
        "    summarize_every_n_turns: int = int(st_cfg.get(\"summarize_every_n_turns\", 5))\n",
        "    window_turns: int = int(st_cfg.get(\"window_turns\", 20))\n",
        "\n",
        "    long_enabled: bool = bool(lt_cfg.get(\"enabled\", False))\n",
        "    long_max_docs: int = int(lt_cfg.get(\"max_docs\", 200))\n",
        "\n",
        "    # -------------------------\n",
        "    # 1) Session memory\n",
        "    # -------------------------\n",
        "    if session_enabled:\n",
        "        msgs = _SESSION_STORE.setdefault(session_id, [])\n",
        "        msgs.append(f\"USER: {user_prompt}\")\n",
        "        msgs.append(f\"AGENT: {user_out}\")\n",
        "        # Recortamos a las últimas `max_messages` líneas\n",
        "        _SESSION_STORE[session_id] = _truncate_list(msgs, max_messages)\n",
        "\n",
        "    # Número de turnos aproximado (USER/AGENT cuentan como dos mensajes)\n",
        "    num_turns = len(_SESSION_STORE.get(session_id, [])) // 2 if session_enabled else 0\n",
        "\n",
        "    # -------------------------\n",
        "    # 2) Short-term memory\n",
        "    # -------------------------\n",
        "    if short_enabled and num_turns > 0:\n",
        "        # Cada N turnos, generamos un nuevo snippet\n",
        "        if summarize_every_n_turns > 0 and num_turns % summarize_every_n_turns == 0:\n",
        "            snippet = _build_short_term_snippet(user_prompt, user_out, num_turns)\n",
        "            st_list = _SHORT_TERM_STORE.setdefault(session_id, [])\n",
        "            st_list.append(snippet)\n",
        "            # Recortamos por ventana de turnos (como proxy)\n",
        "            _SHORT_TERM_STORE[session_id] = _truncate_list(st_list, window_turns)\n",
        "\n",
        "    # -------------------------\n",
        "    # 3) Long-term memory (placeholder)\n",
        "    # -------------------------\n",
        "    if long_enabled and user_id:\n",
        "        docs = _LONG_TERM_STORE.setdefault(user_id, [])\n",
        "        # Heurística mínima: guardar sólo turnos cuyo user_out tenga cierta longitud\n",
        "        if len(user_out.strip()) >= 40:\n",
        "            docs.append(\n",
        "                {\n",
        "                    \"text\": f\"USER: {user_prompt.strip()}\\nAGENT: {user_out.strip()}\",\n",
        "                    \"created_at\": _now_iso(),\n",
        "                    \"meta\": {\n",
        "                        \"session_id\": session_id,\n",
        "                        \"turn_index\": num_turns,\n",
        "                    },\n",
        "                }\n",
        "            )\n",
        "            _LONG_TERM_STORE[user_id] = _truncate_list(docs, long_max_docs)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Utilidades varias\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def clear_session_memory(session_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Elimina TODA la memoria asociada a una sesión concreta\n",
        "    (session history + short-term snippets).\n",
        "    \"\"\"\n",
        "    _SESSION_STORE.pop(session_id, None)\n",
        "    _SHORT_TERM_STORE.pop(session_id, None)\n",
        "\n",
        "\n",
        "def clear_long_term_memory(user_id: str) -> None:\n",
        "    \"\"\"\n",
        "    Elimina la memoria de largo plazo de un usuario concreto.\n",
        "    \"\"\"\n",
        "    _LONG_TERM_STORE.pop(user_id, None)\n",
        "\n",
        "\n",
        "def debug_dump_memory() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Devuelve una vista simple de todas las memorias en memoria (para depuración).\n",
        "    NO usar en producción si el contenido es sensible.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"session_store\": dict(_SESSION_STORE),\n",
        "        \"short_term_store\": dict(_SHORT_TERM_STORE),\n",
        "        \"long_term_store\": dict(_LONG_TERM_STORE),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "qCggq_K6Pkq3",
        "outputId": "1b29dc41-3a84-4013-a1c9-d45393fcee38"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/prompts.py\n",
        "%%writefile agnostic_agent/prompts.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Prompts de sistema para el Agnostic Deep Agent 2026.\n",
        "\n",
        "Aquí centralizamos TODO el comportamiento de alto nivel en texto:\n",
        "\n",
        "- ANALYZER_SYSTEM_PROMPT\n",
        "- SUMMARIZER_*_SYSTEM_PROMPT  (user / deep / dev)\n",
        "- VALIDATOR_SYSTEM_PROMPT\n",
        "- MEMORY_WRITE_SYSTEM_PROMPT  (decidir qué guardar a largo plazo)\n",
        "\n",
        "NOTA:\n",
        "- Estos prompts son agnósticos de dominio (no asumen FAOSTAT, banca, etc.),\n",
        "  pero están pensados para trabajar con Knowledge Bases (KBs) y tablas\n",
        "  estructuradas (CSV/SQL) como contexto adicional.\n",
        "- El wiring con modelos (SystemMessage, etc.) se hace fuera (p.ej. en\n",
        "  capabilities.py o logic.py). Aquí solo definimos textos y helpers ligeros.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# ANALYZER – de texto libre a AnalyzerIntent\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "ANALYZER_SYSTEM_PROMPT: str = \"\"\"\n",
        "Eres el ANALYZER de un agente de IA de propósito general.\n",
        "\n",
        "Tu trabajo es LEER con cuidado la petición del usuario y devolver un\n",
        "objeto JSON que represente su intención de forma estructurada.\n",
        "\n",
        "La entrada que recibirás incluye, como mínimo:\n",
        "- user_prompt: texto completo del usuario.\n",
        "- memory_context: resúmenes y fragmentos de contexto previos (si existen).\n",
        "\n",
        "Y el sistema puede disponer además de:\n",
        "- knowledge_bases: descriptores de BDs/tablas/KBs disponibles\n",
        "  (por ejemplo, tablas SQL, vectores, diccionarios de negocio).\n",
        "- context_tables: rutas a tablas CSV de contexto (por ejemplo,\n",
        "  tablas de parametrías, diccionarios de abreviaturas/definiciones,\n",
        "  catálogos de atributos, etc.).\n",
        "\n",
        "NO debes devolver estos campos en el JSON, pero sí debes tener en mente\n",
        "que parte del problema puede requerir:\n",
        "- cruzar una \"tabla de atributos\" (input A) con una o varias tablas\n",
        "  de contexto (input B) como parametrías o diccionarios;\n",
        "- aplicar reglas de negocio, abreviaturas o definiciones que residen\n",
        "  en esas tablas.\n",
        "\n",
        "DEBES devolver UN ÚNICO objeto JSON con esta estructura EXACTA:\n",
        "\n",
        "{\n",
        "  \"logic_form\": \"<cadena que represente la lógica proposicional de las subconsultas>\",\n",
        "  \"subqueries\": [\n",
        "    \"<subconsulta 1 en texto>\",\n",
        "    \"<subconsulta 2 en texto>\",\n",
        "    ...\n",
        "  ],\n",
        "  \"required_items\": [\n",
        "    {\n",
        "      \"id\": \"<id_corto_ej_q1>\",\n",
        "      \"description\": \"<qué debe responderse en lenguaje natural>\",\n",
        "      \"must_be_answered\": true\n",
        "    },\n",
        "    ...\n",
        "  ],\n",
        "  \"wants_tool_trace\": true,\n",
        "  \"language\": \"<idioma_dominante_ej_es_o_en>\"\n",
        "}\n",
        "\n",
        "Instrucciones:\n",
        "\n",
        "1) Descompón el mensaje en subconsultas claras y numeradas cuando tenga\n",
        "   varias partes (por ejemplo: \"primero haz A, luego B...\").\n",
        "\n",
        "   - Si el usuario habla de una TABLA de atributos A y una TABLA de\n",
        "     contexto B (parametrías, abreviaturas, diccionarios), refleja eso\n",
        "     en las subqueries y en los required_items, indicando qué juicio o\n",
        "     salida se espera a partir de ese cruce.\n",
        "\n",
        "2) Para cada subconsulta importante, crea un RequiredItem con:\n",
        "   - id: \"q1\", \"q2\", \"q3\", etc.\n",
        "   - description: qué espera exactamente el usuario como respuesta,\n",
        "     incluso si la respuesta se obtendrá aplicando reglas sobre tablas.\n",
        "   - must_be_answered: true si es obligatorio, false si es opcional.\n",
        "\n",
        "3) logic_form puede usar conectores lógicos simples:\n",
        "   - \"q1 ∧ q2\", \"q1 ∧ (q2 ∨ q3)\", etc.\n",
        "   - Si hay dependencias (por ejemplo: primero identificar el contrato,\n",
        "     luego aplicar parametrías), refleja esa estructura en logic_form.\n",
        "\n",
        "4) wants_tool_trace:\n",
        "   - true si el usuario pide explícitamente ver \"cómo razonaste\",\n",
        "     \"qué herramientas usaste\", \"explica el proceso\", etc.\n",
        "   - false en caso contrario.\n",
        "\n",
        "5) language:\n",
        "   - \"es\" si el usuario escribe principalmente en español,\n",
        "   - \"en\" si escribe en inglés,\n",
        "   - otro código ISO simple si detectas otro idioma.\n",
        "\n",
        "6) No añadas comentarios fuera del JSON. Devuelve SOLO el JSON.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def build_analyzer_system_message() -> SystemMessage:\n",
        "    \"\"\"Devuelve el SystemMessage para el rol ANALYZER.\"\"\"\n",
        "    return SystemMessage(content=ANALYZER_SYSTEM_PROMPT)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# SUMMARIZER – user / deep / dev\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "SUMMARIZER_USER_SYSTEM_PROMPT: str = \"\"\"\n",
        "Eres el SUMMARIZER (vista USUARIO) de un agente de IA.\n",
        "\n",
        "Recibirás:\n",
        "- analyzer_intent: con required_items y lógica.\n",
        "- tool_runs: lista de ejecuciones de tools (ya normalizadas).\n",
        "- step_results: resultados por id de step.\n",
        "- memory_context: contexto de la conversación.\n",
        "- hints del VALIDADOR (missing_items), si se trata de un reintento.\n",
        "\n",
        "El sistema puede haber consultado:\n",
        "- Knowledge Bases (KBs) tabulares o vectoriales.\n",
        "- Tablas de contexto (parametrías, diccionarios de abreviaturas, etc.).\n",
        "- Documentos que simulan OCR de contratos u otros textos.\n",
        "\n",
        "Tu objetivo es producir UNA ÚNICA respuesta en lenguaje natural para el usuario:\n",
        "\n",
        "- Clara, breve y orientada a la acción.\n",
        "- En el mismo idioma que el usuario (campo analyzer_intent.language).\n",
        "- Cubriendo TODOS los required_items marcados como must_be_answered=true.\n",
        "- Si analyzer_intent.wants_tool_trace es true, incluye una sección breve\n",
        "  explicando qué se hizo (sin entrar en detalles técnicos extremos).\n",
        "\n",
        "Instrucciones:\n",
        "\n",
        "1) Empieza por responder directamente a la petición principal.\n",
        "\n",
        "   - Si la respuesta depende de cruzar una fila de atributos (ej. número\n",
        "     de contrato, tipo de operación, etc.) con tablas de parametrías o\n",
        "     abreviaturas, deja claro que tu juicio se basa en esas reglas\n",
        "     y diccionarios, NO en opiniones arbitrarias.\n",
        "\n",
        "2) Asegúrate de cubrir cada RequiredItem obligatorio (puedes usar viñetas).\n",
        "\n",
        "3) Si hay errores de alguna tool (por ejemplo, fallo al leer una tabla,\n",
        "   problemas en embeddings o en búsquedas), explícalos de forma amable y\n",
        "   propone alternativas o aclaraciones.\n",
        "\n",
        "4) Si el usuario pide trazas (wants_tool_trace=true), añade al final una\n",
        "   sección breve tipo:\n",
        "   - \"Resumen del proceso\" → indicando:\n",
        "     - qué tablas / KBs se consultaron,\n",
        "     - qué tipo de matching se hizo (semántico, exacto, etc.),\n",
        "     - y cómo se aplicaron las reglas o definiciones.\n",
        "\n",
        "5) No incluyas el JSON interno ni IDs de pasos a menos que el usuario pida\n",
        "   explícitamente detalles técnicos.\n",
        "\n",
        "6) No agregues nada fuera del texto final dirigido al usuario.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "SUMMARIZER_DEEP_SYSTEM_PROMPT: str = \"\"\"\n",
        "Eres el SUMMARIZER (vista DEEP) de un agente de IA.\n",
        "\n",
        "Tu audiencia es una persona TÉCNICA que quiere entender qué hizo el agente,\n",
        "no sólo ver la respuesta final.\n",
        "\n",
        "Recibirás:\n",
        "- analyzer_intent (logic_form, subqueries, required_items, etc.).\n",
        "- planner_plan (si existe).\n",
        "- tool_runs normalizados.\n",
        "- step_results.\n",
        "- memory_context (fragmentos relevantes).\n",
        "- Información indirecta sobre qué KBs y tablas de contexto se usaron\n",
        "  (por ejemplo, semantic_search_in_csv, consultas SQL, vector search, etc.).\n",
        "\n",
        "Debes devolver un texto en formato markdown, estructurado más o menos así:\n",
        "\n",
        "## Resumen de alto nivel\n",
        "(una o dos frases sobre qué se hizo)\n",
        "\n",
        "### ANALYZER\n",
        "- Lógica proposicional: ...\n",
        "- Subconsultas detectadas: ...\n",
        "- Required items: ...\n",
        "- Relación entre input A (atributos/tablas) e input B (tablas de contexto)\n",
        "  si aplica (por ejemplo: \"fila de contrato vs. tablas de parametrías y\n",
        "  diccionario de abreviaturas\").\n",
        "\n",
        "### PLANNER\n",
        "- Descripción general del plan\n",
        "- Pasos planificados (en orden lógico)\n",
        "- Cómo se decidió usar ciertas KBs / tablas de contexto (si se ve en el plan).\n",
        "\n",
        "### EXECUTOR\n",
        "- Lista de tools efectivamente llamadas y para qué se usaron.\n",
        "  - Incluye, cuando existan:\n",
        "    - búsquedas semánticas en CSV,\n",
        "    - consultas SQL a KBs tabulares,\n",
        "    - re-rankers o embeddings aplicados sobre documentos (ej. OCR).\n",
        "- Comentarios sobre errores o reintentos, si los hubo.\n",
        "- Explica cómo se cruzó la información de:\n",
        "  - registros tabulares (input A),\n",
        "  - tablas de parametrías / diccionarios (context_tables),\n",
        "  - y documentos de texto (OCR, contratos, etc.).\n",
        "\n",
        "### CATCHER\n",
        "- Notas sobre normalización / truncado / saneamiento (si aplica).\n",
        "\n",
        "### SUMMARIZER\n",
        "- Cómo se construyó la respuesta final para el usuario,\n",
        "  incluyendo cómo se tradujeron las reglas/tablas a lenguaje natural.\n",
        "\n",
        "### Respuesta final (resumen)\n",
        "- Pequeño resumen de lo que recibió el usuario (sin repetirlo completo).\n",
        "\n",
        "Instrucciones:\n",
        "- Usa un tono técnico pero legible.\n",
        "- No vuelvas a listar datos gigantes (listas enormes, matrices…); sólo\n",
        "  describe su rol o muestra pequeños extractos representativos.\n",
        "- NO devuelvas JSON; solo markdown.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "SUMMARIZER_DEV_SYSTEM_PROMPT: str = \"\"\"\n",
        "Eres el SUMMARIZER (vista DEV) de un agente de IA.\n",
        "\n",
        "Tu audiencia son desarrolladores que quieren depurar o auditar el comportamiento.\n",
        "\n",
        "Recibirás:\n",
        "- analyzer_intent\n",
        "- planner_plan\n",
        "- tool_runs\n",
        "- step_results\n",
        "- memory_context\n",
        "- fragmentos del estado crudo del grafo (si el llamador lo incluye)\n",
        "\n",
        "Debes devolver un texto en formato markdown con énfasis en:\n",
        "\n",
        "- IDs de steps, tools y tiempo de ejecución (si se proveen).\n",
        "- Inputs y outputs relevantes (resumen de payloads grandes).\n",
        "- Errores, excepciones o casos no cubiertos.\n",
        "- Cualquier inconsistencia detectada.\n",
        "- Uso concreto de KBs / tablas de contexto (qué backend y qué tabla se\n",
        "  consultó: SQLite, sqlite-vec, CSV, etc., según se vea en los tool_runs).\n",
        "\n",
        "Estructura sugerida:\n",
        "\n",
        "## DEV TRACE (alto nivel)\n",
        "- Descripción breve del turno (qué se intentó hacer).\n",
        "\n",
        "## ANALYZER\n",
        "- Payload relevante (resumen).\n",
        "- Cómo se mapearon las partes del prompt a required_items (q1, q2, ...).\n",
        "\n",
        "## PLANNER\n",
        "- Plan final (steps, depends_on).\n",
        "- Decisiones relevantes (ej. \"primero localizar contrato, luego aplicar\n",
        "  parametrías y validar abreviaturas\").\n",
        "\n",
        "## EXECUTOR / TOOLS\n",
        "- Tabla o lista de tool_runs con:\n",
        "  - step_id / tool_name\n",
        "  - args relevantes (truncados)\n",
        "  - KB / backend implicado (si es claro: csv, sqlite, sqlite-vec, etc.)\n",
        "  - tipo de salida (embedding, texto, número, filas tabulares, etc.)\n",
        "  - errores (si los hubo) y cómo se gestionaron.\n",
        "\n",
        "## STATE SNAPSHOT\n",
        "- Notas sobre campos importantes del estado (state), por ejemplo:\n",
        "  - kb_selected, context_tables, context_cfg, flags de validación, etc.\n",
        "\n",
        "No incluyas credenciales, PII o datos sensibles si aparecen en el estado.\n",
        "Trúncalos o marca que fueron redacted.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def build_summarizer_system_message(\n",
        "    view: Literal[\"user\", \"deep\", \"dev\"] = \"user\",\n",
        ") -> SystemMessage:\n",
        "    \"\"\"\n",
        "    Devuelve el SystemMessage adecuado para el SUMMARIZER según vista.\n",
        "    \"\"\"\n",
        "    if view == \"deep\":\n",
        "        return SystemMessage(content=SUMMARIZER_DEEP_SYSTEM_PROMPT)\n",
        "    if view == \"dev\":\n",
        "        return SystemMessage(content=SUMMARIZER_DEV_SYSTEM_PROMPT)\n",
        "    # por defecto, vista usuario\n",
        "    return SystemMessage(content=SUMMARIZER_USER_SYSTEM_PROMPT)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# VALIDATOR – comprobar RequiredItems vs respuesta\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "VALIDATOR_SYSTEM_PROMPT: str = \"\"\"\n",
        "Eres el VALIDATOR de un agente de IA.\n",
        "\n",
        "Tu trabajo es revisar si la respuesta final generada para el usuario\n",
        "cubre TODOS los ítems requeridos (RequiredItem) proporcionados por el ANALYZER.\n",
        "\n",
        "Recibirás:\n",
        "- required_items: lista de objetos con campos {id, description, must_be_answered}.\n",
        "- draft_answer: texto de la respuesta propuesta para el usuario.\n",
        "\n",
        "Debes devolver UN ÚNICO objeto JSON con esta forma EXACTA:\n",
        "\n",
        "{\n",
        "  \"all_covered\": true,\n",
        "  \"missing_item_ids\": [\"q2\", \"q3\"],\n",
        "  \"comments\": \"Texto libre opcional.\"\n",
        "}\n",
        "\n",
        "Reglas:\n",
        "\n",
        "1) all_covered:\n",
        "   - true si TODOS los RequiredItem con must_be_answered=true\n",
        "     están razonablemente cubiertos en draft_answer.\n",
        "   - false en caso contrario.\n",
        "\n",
        "   Ten en cuenta que algunas descriptions pueden hacer referencia a\n",
        "   resultados de aplicar reglas de negocio sobre tablas (parametrías,\n",
        "   diccionarios, etc.). No necesitas conocer las tablas; solo verificar\n",
        "   que el draft_answer responde a lo que se describe en cada item.\n",
        "\n",
        "2) missing_item_ids:\n",
        "   - lista de los ids de RequiredItem que consideres que NO están\n",
        "     bien cubiertos (sólo los que tienen must_be_answered=true).\n",
        "\n",
        "3) comments:\n",
        "   - texto opcional explicando por qué falta algo, o sugerencias\n",
        "     de cómo mejorar la respuesta.\n",
        "\n",
        "4) No añadas nada fuera del JSON. Devuelve SOLO el JSON.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def build_validator_system_message() -> SystemMessage:\n",
        "    \"\"\"Devuelve el SystemMessage para el rol VALIDATOR.\"\"\"\n",
        "    return SystemMessage(content=VALIDATOR_SYSTEM_PROMPT)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# MEMORY_WRITE – decidir qué guardar a largo plazo\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "MEMORY_WRITE_SYSTEM_PROMPT: str = \"\"\"\n",
        "Eres el módulo de decisión de MEMORIA DE LARGO PLAZO de un agente de IA.\n",
        "\n",
        "Tu tarea es decidir si la interacción actual merece ser almacenada como\n",
        "recuerdo persistente (long-term memory).\n",
        "\n",
        "Recibirás:\n",
        "- user_prompt: mensaje del usuario.\n",
        "- user_out: respuesta final del agente.\n",
        "- metadata opcional (ej. etiquetas, importancia, etc.).\n",
        "\n",
        "Debes devolver UN ÚNICO objeto JSON con la forma:\n",
        "\n",
        "{\n",
        "  \"should_store\": true,\n",
        "  \"summary\": \"Resumen breve del conocimiento o preferencia a guardar.\",\n",
        "  \"tags\": [\"preferencia\", \"definicion\", \"dato_importante\"]\n",
        "}\n",
        "\n",
        "Criterios para should_store:\n",
        "\n",
        "- true si:\n",
        "  - el usuario revela una preferencia estable (gustos, estilo, idioma),\n",
        "  - se define una regla que se usará en el futuro (por ejemplo,\n",
        "    una nueva parametría o criterio de evaluación para contratos),\n",
        "  - se captura un conocimiento útil que probablemente se reutilice\n",
        "    (por ejemplo, cómo interpretar cierto atributo tabular específico).\n",
        "\n",
        "- false si:\n",
        "  - es una pregunta puntual sin relevancia futura,\n",
        "  - es información obsoleta o muy específica de un contexto efímero.\n",
        "\n",
        "summary:\n",
        "- una o dos frases como máximo.\n",
        "- NO repitas el diálogo entero; sólo el conocimiento clave.\n",
        "\n",
        "tags:\n",
        "- lista corta de etiquetas en minúsculas (ej. [\"preferencia\", \"contratos\"],\n",
        "  [\"regla\", \"parametrias\"], [\"kb\", \"tabular\"]).\n",
        "\n",
        "No añadas nada fuera del JSON. Devuelve SOLO el JSON.\n",
        "\"\"\".strip()\n",
        "\n",
        "\n",
        "def build_memory_write_system_message() -> SystemMessage:\n",
        "    \"\"\"Devuelve el SystemMessage para el rol MEMORY_WRITE.\"\"\"\n",
        "    return SystemMessage(content=MEMORY_WRITE_SYSTEM_PROMPT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TYXQsupMAfSl",
        "outputId": "4a4ab7e3-7dab-4081-a41e-db24e8a1fdaf"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/capabilities.py\n",
        "%%writefile agnostic_agent/capabilities.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Infraestructura de *capacidades* (modelos/servidores) para el Agnostic Deep Agent 2026.\n",
        "\n",
        "Incluye:\n",
        "- Descarga de modelos Qwen3 (LLM / Embeddings / Reranker) desde Hugging Face.\n",
        "- Lanzamiento de servidores vLLM OpenAI-compatible (LLM / EMB / RERANK).\n",
        "- Configuración del planner (PlannerConfig) y construcción del LLM planner\n",
        "  (build_planner_llm + build_planner_system_message) sobre ChatQwenVllm.\n",
        "\n",
        "NOTA:\n",
        "- Esta lógica es agnóstica del dominio; sólo gestiona modelos y servidores.\n",
        "- El wiring con LangGraph y el resto del agente se hace en logic.py y agent.py.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Literal\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "import time\n",
        "import socket\n",
        "import json\n",
        "import urllib.request\n",
        "import pathlib\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "from langchain_core.messages import SystemMessage\n",
        "from langchain_qwq import ChatQwenVllm\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Helpers básicos\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _str_to_bool(x: str) -> bool:\n",
        "    return str(x).lower() in (\"1\", \"true\", \"yes\", \"y\", \"on\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Descarga de modelos Qwen3 (HF snapshot)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "@dataclass\n",
        "class QwenModelPaths:\n",
        "    \"\"\"Rutas locales a los modelos Qwen usados por el agente.\"\"\"\n",
        "    llm_dir: str\n",
        "    emb_dir: str\n",
        "    rerank_dir: str\n",
        "\n",
        "\n",
        "def ensure_model_downloaded(model_id: str, local_dir: pathlib.Path) -> str:\n",
        "    \"\"\"\n",
        "    Descarga el modelo de Hugging Face a `local_dir` si no existe.\n",
        "    Devuelve la ruta absoluta como string.\n",
        "    \"\"\"\n",
        "    local_dir = pathlib.Path(local_dir)\n",
        "    if local_dir.is_dir():\n",
        "        return str(local_dir.resolve())\n",
        "\n",
        "    local_dir.parent.mkdir(parents=True, exist_ok=True)\n",
        "    snapshot_download(\n",
        "        repo_id=model_id,\n",
        "        local_dir=str(local_dir),\n",
        "        local_dir_use_symlinks=False,\n",
        "        ignore_patterns=[\"*.md\", \"*.png\", \"*.jpg\", \"LICENSE*\"],\n",
        "    )\n",
        "    return str(local_dir.resolve())\n",
        "\n",
        "\n",
        "def prepare_qwen_models(\n",
        "    llm_model_id: Optional[str] = None,\n",
        "    emb_model_id: Optional[str] = None,\n",
        "    rerank_model_id: Optional[str] = None,\n",
        "    base_dir: str | os.PathLike = \"LM_MODEL\",\n",
        ") -> QwenModelPaths:\n",
        "    \"\"\"\n",
        "    Descarga (si es necesario) los modelos Qwen y devuelve sus rutas locales.\n",
        "\n",
        "    Si algún ID no se pasa, se lee de las variables de entorno o se usan defaults:\n",
        "    - LLM_MODEL_ID      (por defecto: Qwen/Qwen3-0.6B)\n",
        "    - EMB_MODEL_ID      (por defecto: Qwen/Qwen3-Embedding-0.6B)\n",
        "    - RERANK_MODEL_ID   (por defecto: Qwen/Qwen3-Reranker-0.6B)\n",
        "    \"\"\"\n",
        "    base_dir_path = pathlib.Path(base_dir)\n",
        "    base_dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    llm_model_id = llm_model_id or os.getenv(\"LLM_MODEL_ID\", \"Qwen/Qwen3-0.6B\")\n",
        "    emb_model_id = emb_model_id or os.getenv(\"EMB_MODEL_ID\", \"Qwen/Qwen3-Embedding-0.6B\")\n",
        "    rerank_model_id = rerank_model_id or os.getenv(\"RERANK_MODEL_ID\", \"Qwen/Qwen3-Reranker-0.6B\")\n",
        "\n",
        "    llm_dir = ensure_model_downloaded(llm_model_id, base_dir_path / \"Qwen3_LLM_MAIN\")\n",
        "    emb_dir = ensure_model_downloaded(emb_model_id, base_dir_path / \"Qwen3_Embedding_0.6B\")\n",
        "    rerank_dir = ensure_model_downloaded(rerank_model_id, base_dir_path / \"Qwen3_Reranker_0.6B\")\n",
        "\n",
        "    return QwenModelPaths(llm_dir=llm_dir, emb_dir=emb_dir, rerank_dir=rerank_dir)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Servidores vLLM (OpenAI-compatible)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "@dataclass\n",
        "class VllmConfig:\n",
        "    # Host / puertos\n",
        "    host: str = \"127.0.0.1\"\n",
        "    llm_port: int = int(os.getenv(\"LLM_PORT\", \"8000\"))\n",
        "    emb_port: int = int(os.getenv(\"EMB_PORT\", \"8001\"))\n",
        "    rerank_port: int = int(os.getenv(\"RERANK_PORT\", \"8002\"))\n",
        "\n",
        "    # Presupuesto de VRAM por servidor\n",
        "    llm_gpu_util: float = float(os.getenv(\"VLLM_LLM_GPU_UTIL\", \"0.4\"))\n",
        "    emb_gpu_util: float = float(os.getenv(\"VLLM_EMB_GPU_UTIL\", \"0.3\"))\n",
        "    rerank_gpu_util: float = float(os.getenv(\"VLLM_RERANK_GPU_UTIL\", \"0.3\"))\n",
        "\n",
        "    # Longitud máxima de contexto por servidor\n",
        "    llm_max_len: int = int(os.getenv(\"VLLM_LLM_MAX_LEN\", \"2048\"))\n",
        "    emb_max_len: int = int(os.getenv(\"VLLM_EMB_MAX_LEN\", \"1024\"))\n",
        "    rerank_max_len: int = int(os.getenv(\"VLLM_RERANK_MAX_LEN\", \"1024\"))\n",
        "\n",
        "    # Concurrencia (nº de secuencias simultáneas)\n",
        "    llm_max_num_seqs: int = int(os.getenv(\"VLLM_LLM_MAX_NUM_SEQS\", \"4\"))\n",
        "    emb_max_num_seqs: int = int(os.getenv(\"VLLM_EMB_MAX_NUM_SEQS\", \"4\"))\n",
        "    rerank_max_num_seqs: int = int(os.getenv(\"VLLM_RERANK_MAX_NUM_SEQS\", \"4\"))\n",
        "\n",
        "    # Nombres \"served_model_name\" dentro de vLLM\n",
        "    llm_served_name: str = os.getenv(\"LLM_SERVED_NAME\", \"qwen3-0.6b\")\n",
        "    emb_served_name: str = os.getenv(\"EMB_SERVED_NAME\", \"qwen3-embedding-0.6b\")\n",
        "    rerank_served_name: str = os.getenv(\"RERANK_SERVED_NAME\", \"qwen3-reranker-0.6b\")\n",
        "\n",
        "    # Parser de tool-calls y razonamiento (alineado Qwen3 + langchain-qwq)\n",
        "    # Por defecto usamos el parser XML de Qwen3, que es el que mejor encaja con langchain-qwq.\n",
        "    tool_call_parser: str = os.getenv(\"VLLM_TOOL_CALL_PARSER\", \"qwen3_xml\")\n",
        "    enable_reasoning: bool = _str_to_bool(os.getenv(\"VLLM_ENABLE_REASONING\", \"1\"))\n",
        "    reasoning_parser: Optional[str] = os.getenv(\"VLLM_REASONING_PARSER\", \"qwen3\")\n",
        "\n",
        "    # Flags para arrancar (o no) servidores extra\n",
        "    start_emb_server: bool = _str_to_bool(os.getenv(\"VLLM_START_EMB_SERVER\", \"0\"))\n",
        "    start_rerank_server: bool = _str_to_bool(os.getenv(\"VLLM_START_RERANK_SERVER\", \"0\"))\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class VllmServers:\n",
        "    llm_proc: subprocess.Popen\n",
        "    emb_proc: Optional[subprocess.Popen] = None\n",
        "    rerank_proc: Optional[subprocess.Popen] = None\n",
        "    llm_log_path: str = \"\"\n",
        "    emb_log_path: Optional[str] = None\n",
        "    rerank_log_path: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class VllmEndpoints:\n",
        "    llm_base_url: str\n",
        "    emb_base_url: Optional[str] = None\n",
        "    rerank_base_url: Optional[str] = None\n",
        "\n",
        "\n",
        "def _free_port_if_needed(host: str, port: int) -> None:\n",
        "    try:\n",
        "        s = socket.socket()\n",
        "        s.settimeout(0.5)\n",
        "        if s.connect_ex((host, port)) == 0:\n",
        "            _ = os.system(f\"fuser -k {port}/tcp || true\")\n",
        "        s.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "\n",
        "def _url_open_no_proxy(url: str, timeout: float = 2.0) -> dict:\n",
        "    opener = urllib.request.build_opener(urllib.request.ProxyHandler({}))\n",
        "    urllib.request.install_opener(opener)\n",
        "    with urllib.request.urlopen(url, timeout=timeout) as resp:\n",
        "        return json.load(resp)\n",
        "\n",
        "\n",
        "def _wait_until_ready(\n",
        "    url: str,\n",
        "    server_proc: subprocess.Popen,\n",
        "    log_path: str,\n",
        "    seconds: int = 180,\n",
        "    sleep: float = 2.0,\n",
        ") -> bool:\n",
        "    start = time.time()\n",
        "    while time.time() - start < seconds:\n",
        "        if server_proc.poll() is not None:\n",
        "            print(f\"❌ Servidor terminó con código {server_proc.returncode}. Log tail:\")\n",
        "            try:\n",
        "                with open(log_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                    lines = f.read().splitlines()\n",
        "                print(\"\\n\".join(lines[-80:]))\n",
        "            except Exception:\n",
        "                pass\n",
        "            return False\n",
        "        try:\n",
        "            _ = _url_open_no_proxy(url, timeout=2.5)\n",
        "            return True\n",
        "        except Exception:\n",
        "            time.sleep(sleep)\n",
        "    print(\"⏰ Timeout esperando servidor, mostrando tail del log:\")\n",
        "    try:\n",
        "        with open(log_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            lines = f.read().splitlines()\n",
        "        print(\"\\n\".join(lines[-80:]))\n",
        "    except Exception:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "\n",
        "def _launch_vllm_server(\n",
        "    name: str,\n",
        "    model_dir: str,\n",
        "    host: str,\n",
        "    port: int,\n",
        "    served_model_name: Optional[str],\n",
        "    gpu_util: float,\n",
        "    max_model_len: Optional[int],\n",
        "    max_num_seqs: Optional[int],\n",
        "    extra_flags: Optional[list[str]] = None,\n",
        ") -> tuple[subprocess.Popen, str]:\n",
        "    \"\"\"\n",
        "    Lanza un servidor vLLM OpenAI-compatible para un modelo dado.\n",
        "    \"\"\"\n",
        "    _free_port_if_needed(host, port)\n",
        "    log_path = f\"vllm_{name}.log\"\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"-m\",\n",
        "        \"vllm.entrypoints.openai.api_server\",\n",
        "        \"--model\",\n",
        "        model_dir,\n",
        "        \"--host\",\n",
        "        host,\n",
        "        \"--port\",\n",
        "        str(port),\n",
        "        \"--gpu-memory-utilization\",\n",
        "        str(gpu_util),\n",
        "    ]\n",
        "    if max_model_len is not None:\n",
        "        cmd += [\"--max-model-len\", str(max_model_len)]\n",
        "    if max_num_seqs is not None:\n",
        "        cmd += [\"--max-num-seqs\", str(max_num_seqs)]\n",
        "    if served_model_name:\n",
        "        cmd += [\"--served-model-name\", served_model_name]\n",
        "    if extra_flags:\n",
        "        cmd += list(extra_flags)\n",
        "\n",
        "    print(f\"\\n🚀 Lanzando servidor vLLM [{name}] en puerto {port}\")\n",
        "    print(\" \".join(cmd))\n",
        "    proc = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=open(log_path, \"w\"),\n",
        "        stderr=subprocess.STDOUT,\n",
        "    )\n",
        "    return proc, log_path\n",
        "\n",
        "\n",
        "def start_qwen_vllm_servers(\n",
        "    model_paths: QwenModelPaths,\n",
        "    config: Optional[VllmConfig] = None,\n",
        "    set_env: bool = True,\n",
        ") -> tuple[VllmEndpoints, VllmServers]:\n",
        "    \"\"\"\n",
        "    Lanza servidores vLLM:\n",
        "\n",
        "      - LLM (generate, con tool-calling + reasoning de Qwen3)\n",
        "      - (Opcional) Embeddings (embed)\n",
        "      - (Opcional) Reranker (score)\n",
        "    \"\"\"\n",
        "    cfg = config or VllmConfig()\n",
        "\n",
        "    # LLM – flags alineados a Qwen3 + langchain-qwq\n",
        "    llm_extra_flags: list[str] = [\n",
        "        \"--enable-auto-tool-choice\",\n",
        "        \"--tool-call-parser\",\n",
        "        cfg.tool_call_parser,\n",
        "    ]\n",
        "    if cfg.enable_reasoning and cfg.reasoning_parser:\n",
        "        llm_extra_flags += [\"--reasoning-parser\", cfg.reasoning_parser]\n",
        "\n",
        "    llm_proc, llm_log = _launch_vllm_server(\n",
        "        name=\"language\",\n",
        "        model_dir=model_paths.llm_dir,\n",
        "        host=cfg.host,\n",
        "        port=cfg.llm_port,\n",
        "        served_model_name=cfg.llm_served_name,\n",
        "        gpu_util=cfg.llm_gpu_util,\n",
        "        max_model_len=cfg.llm_max_len,\n",
        "        max_num_seqs=cfg.llm_max_num_seqs,\n",
        "        extra_flags=llm_extra_flags,\n",
        "    )\n",
        "\n",
        "    emb_proc = None\n",
        "    emb_log = None\n",
        "    if cfg.start_emb_server:\n",
        "        emb_proc, emb_log = _launch_vllm_server(\n",
        "            name=\"embedding\",\n",
        "            model_dir=model_paths.emb_dir,\n",
        "            host=cfg.host,\n",
        "            port=cfg.emb_port,\n",
        "            served_model_name=cfg.emb_served_name,\n",
        "            gpu_util=cfg.emb_gpu_util,\n",
        "            max_model_len=cfg.emb_max_len,\n",
        "            max_num_seqs=cfg.emb_max_num_seqs,\n",
        "        )\n",
        "\n",
        "    rerank_proc = None\n",
        "    rerank_log = None\n",
        "    if cfg.start_rerank_server:\n",
        "        rerank_proc, rerank_log = _launch_vllm_server(\n",
        "            name=\"reranker\",\n",
        "            model_dir=model_paths.rerank_dir,\n",
        "            host=cfg.host,\n",
        "            port=cfg.rerank_port,\n",
        "            served_model_name=cfg.rerank_served_name,\n",
        "            gpu_util=cfg.rerank_gpu_util,\n",
        "            max_model_len=cfg.rerank_max_len,\n",
        "            max_num_seqs=cfg.rerank_max_num_seqs,\n",
        "        )\n",
        "\n",
        "    llm_base = f\"http://{cfg.host}:{cfg.llm_port}/v1\"\n",
        "    emb_base = f\"http://{cfg.host}:{cfg.emb_port}/v1\" if cfg.start_emb_server else None\n",
        "    rerank_base = f\"http://{cfg.host}:{cfg.rerank_port}/v1\" if cfg.start_rerank_server else None\n",
        "\n",
        "    print(\"\\n⏳ Esperando LLM server...\")\n",
        "    ok_llm = _wait_until_ready(f\"{llm_base}/models\", llm_proc, llm_log)\n",
        "\n",
        "    ok_emb = True\n",
        "    if cfg.start_emb_server and emb_proc is not None and emb_log is not None:\n",
        "        print(\"⏳ Esperando Embedding server...\")\n",
        "        ok_emb = _wait_until_ready(f\"{emb_base}/models\", emb_proc, emb_log)\n",
        "\n",
        "    ok_rerank = True\n",
        "    if cfg.start_rerank_server and rerank_proc is not None and rerank_log is not None:\n",
        "        print(\"⏳ Esperando Reranker server...\")\n",
        "        ok_rerank = _wait_until_ready(f\"{rerank_base}/models\", rerank_proc, rerank_log)\n",
        "\n",
        "    if not ok_llm or not ok_emb or not ok_rerank:\n",
        "        raise SystemExit(\"❌ Algún servidor vLLM no quedó listo. Revisa logs.\")\n",
        "\n",
        "    print(\"\\n✅ Servidores vLLM listos.\")\n",
        "    print(\"\\n📋 Modelos en LLM server:\")\n",
        "    print(_url_open_no_proxy(f\"{llm_base}/models\"))\n",
        "    if emb_base is not None:\n",
        "        print(\"\\n📋 Modelos en Embedding server:\")\n",
        "        print(_url_open_no_proxy(f\"{emb_base}/models\"))\n",
        "    if rerank_base is not None:\n",
        "        print(\"\\n📋 Modelos en Reranker server:\")\n",
        "        print(_url_open_no_proxy(f\"{rerank_base}/models\"))\n",
        "\n",
        "    if set_env:\n",
        "        os.environ[\"VLLM_LLM_API_BASE\"] = llm_base\n",
        "        os.environ[\"VLLM_API_BASE\"] = llm_base\n",
        "        if emb_base is not None:\n",
        "            os.environ[\"VLLM_EMB_API_BASE\"] = emb_base\n",
        "        if rerank_base is not None:\n",
        "            os.environ[\"VLLM_RERANK_API_BASE\"] = rerank_base\n",
        "\n",
        "        # Clave dummy: vLLM ignora el valor, sólo requiere que exista.\n",
        "        os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\"OPENAI_API_KEY\", \"EMPTY\")\n",
        "        os.environ[\"LLM_SERVED_NAME\"] = cfg.llm_served_name\n",
        "        os.environ[\"EMB_SERVED_NAME\"] = cfg.emb_served_name\n",
        "        os.environ[\"RERANK_SERVED_NAME\"] = cfg.rerank_served_name\n",
        "\n",
        "        print(\"\\n🌐 Bases URL registradas en ENV:\")\n",
        "        print(\"VLLM_API_BASE       =\", os.environ[\"VLLM_API_BASE\"])\n",
        "        print(\"VLLM_LLM_API_BASE   =\", os.environ[\"VLLM_LLM_API_BASE\"])\n",
        "        if emb_base is not None:\n",
        "            print(\"VLLM_EMB_API_BASE   =\", os.environ[\"VLLM_EMB_API_BASE\"])\n",
        "        if rerank_base is not None:\n",
        "            print(\"VLLM_RERANK_API_BASE=\", os.environ[\"VLLM_RERANK_API_BASE\"])\n",
        "\n",
        "    endpoints = VllmEndpoints(\n",
        "        llm_base_url=llm_base,\n",
        "        emb_base_url=emb_base,\n",
        "        rerank_base_url=rerank_base,\n",
        "    )\n",
        "    servers = VllmServers(\n",
        "        llm_proc=llm_proc,\n",
        "        emb_proc=emb_proc,\n",
        "        rerank_proc=rerank_proc,\n",
        "        llm_log_path=llm_log,\n",
        "        emb_log_path=emb_log,\n",
        "        rerank_log_path=rerank_log,\n",
        "    )\n",
        "    return endpoints, servers\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# PlannerConfig + planner LLM (ChatQwenVllm)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "STRICT_SYSTEM_TEXT = (\n",
        "    \"Eres el PLANNER de herramientas de un agente de IA.\\n\"\n",
        "    \"Tu trabajo es LEER con cuidado la petición completa del usuario, \"\n",
        "    \"descomponerla en subtareas cuando sea necesario y planificar una o \"\n",
        "    \"VARIAS llamadas a herramientas para resolver TODAS las partes.\\n\\n\"\n",
        "    \"Dispones de un conjunto de herramientas (tools). Para cada tool \"\n",
        "    \"conoces su nombre, su descripción y sus parámetros.\\n\\n\"\n",
        "    \"INSTRUCCIONES (agnósticas de dominio):\\n\"\n",
        "    \"1) Usa SIEMPRE herramientas cuando exista alguna relevante.\\n\"\n",
        "    \"2) Devuelves únicamente tool_calls (llamadas a herramientas), \"\n",
        "    \"   nunca una respuesta final en lenguaje natural.\\n\"\n",
        "    \"3) Si la instrucción tiene varias acciones (por ejemplo: \"\n",
        "    \"   'primero A, luego B, al final C'), planifica todas las tools \"\n",
        "    \"   necesarias respetando ese orden.\\n\"\n",
        "    \"4) Si la entrada menciona varios ítems (varios textos, documentos, \"\n",
        "    \"   números, entidades, etc.), considera planear llamadas que cubran \"\n",
        "    \"   CADA ítem relevante.\\n\"\n",
        "    \"5) Siempre que haya al menos una herramienta relevante, llama a UNA \"\n",
        "    \"   O MÁS herramientas; no respondas sólo con razonamiento interno.\\n\"\n",
        "    \"6) Sé explícito y coherente en los argumentos de cada tool_call; \"\n",
        "    \"   respeta nombres y tipos de parámetros.\\n\"\n",
        "    \"7) Si una acción requiere varios pasos, divide el trabajo \"\n",
        "    \"   en varias tool_calls encadenadas.\\n\"\n",
        "    \"8) Decide qué herramientas usar únicamente por su descripción.\\n\"\n",
        ")\n",
        "\n",
        "FREE_SYSTEM_TEXT = (\n",
        "    \"Eres el asistente/PLANNER de un agente.\\n\"\n",
        "    \"Regla principal:\\n\"\n",
        "    \"- Si el usuario pide conversación, creatividad (poesía, historias), identidad del asistente, \"\n",
        "    \"  explicación general o razonamiento que NO requiere datos externos, responde DIRECTO en lenguaje natural.\\n\"\n",
        "    \"- Usa herramientas SOLO cuando sean necesarias (DB, archivos, web, embeddings, cálculos deterministas, etc.).\\n\\n\"\n",
        "    \"Cuando uses herramientas:\\n\"\n",
        "    \"- Genera tool_calls correctas (nombre + argumentos) para resolver lo pedido.\\n\\n\"\n",
        "    \"Cuando NO uses herramientas:\\n\"\n",
        "    \"- NO inventes que necesitas tools.\\n\"\n",
        "    \"- Devuelve una respuesta final clara y útil.\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class PlannerConfig:\n",
        "    \"\"\"Configuración de alto nivel del planner de herramientas.\"\"\"\n",
        "    model_name: str = os.getenv(\"LLM_SERVED_NAME\", \"qwen3-0.6b\")\n",
        "    tool_choice: str = os.getenv(\"PLANNER_TOOL_CHOICE\", \"required\")\n",
        "    max_retries: int = int(os.getenv(\"PLANNER_MAX_RETRIES\", \"1\"))\n",
        "    max_steps: int = int(os.getenv(\"PLANNER_MAX_STEPS\", \"16\"))\n",
        "    temperature: float = float(os.getenv(\"PLANNER_TEMPERATURE\", \"0.0\"))\n",
        "    policy_mode: str = os.getenv(\"PLANNER_POLICY_MODE\", \"tools_strict\")\n",
        "\n",
        "    # Qwen3-style reasoning\n",
        "    enable_thinking: bool = _str_to_bool(os.getenv(\"PLANNER_ENABLE_THINKING\", \"true\"))\n",
        "\n",
        "    # 4) Ajuste “policy prompt”: 2 modos a nivel prompt\n",
        "    policy_mode: Literal[\"tools_strict\", \"free_policies\"] = os.getenv(\n",
        "        \"PLANNER_POLICY_MODE\",\n",
        "        \"tools_strict\",\n",
        "    )  # \"tools_strict\" | \"free_policies\"\n",
        "\n",
        "    # Prompt base del planner (agnóstico de dominio)\n",
        "    system_text: str = (\n",
        "        \"Eres el PLANNER de un agente de IA con acceso opcional a herramientas (tools).\\n\"\n",
        "        \"Tu trabajo es LEER con cuidado la petición completa del usuario, \"\n",
        "        \"descomponerla en subtareas cuando sea necesario y decidir si debes \"\n",
        "        \"hacer UNA o VARIAS llamadas a herramientas.\\n\\n\"\n",
        "        \"Dispones de un conjunto de herramientas (tools). Para cada tool \"\n",
        "        \"conoces su nombre, su descripción y sus parámetros.\\n\\n\"\n",
        "        \"INSTRUCCIONES (agnósticas de dominio):\\n\"\n",
        "        \"1) Si una tool es necesaria para operar sobre datos externos (DB, archivos, web, \"\n",
        "        \"embeddings, búsqueda, parsing, etc.), úsala.\\n\"\n",
        "        \"2) Si la instrucción tiene varias acciones (por ejemplo: \"\n",
        "        \"   'primero A, luego B, al final C'), planifica todas las tools \"\n",
        "        \"   necesarias respetando ese orden.\\n\"\n",
        "        \"3) Si la entrada menciona varios ítems (varios textos, documentos, \"\n",
        "        \"   números, entidades, etc.), considera cubrir CADA ítem relevante.\\n\"\n",
        "        \"4) Sé explícito y coherente en los argumentos de cada tool_call; \"\n",
        "        \"   respeta nombres y tipos de parámetros.\\n\"\n",
        "        \"5) Si una acción requiere varios pasos (por ejemplo: transformar un \"\n",
        "        \"   dato y luego consultarlo en otra herramienta), divide el trabajo \"\n",
        "        \"   en varias tool_calls encadenadas.\\n\"\n",
        "        \"6) No supongas un dominio específico (texto, matemáticas, APIs, \"\n",
        "        \"   bases de datos…); decide qué herramientas usar únicamente por su \"\n",
        "        \"   descripción.\\n\"\n",
        "    )\n",
        "\n",
        "\n",
        "def build_planner_system_message(config: Optional[PlannerConfig] = None) -> SystemMessage:\n",
        "    cfg = config or PlannerConfig()\n",
        "    if cfg.policy_mode == \"free_policies\":\n",
        "        return SystemMessage(content=FREE_SYSTEM_TEXT)\n",
        "    return SystemMessage(content=STRICT_SYSTEM_TEXT)\n",
        "\n",
        "def build_planner_llm(config: Optional[PlannerConfig] = None) -> ChatQwenVllm:\n",
        "    \"\"\"\n",
        "    Construye el LLM planner apuntando al endpoint vLLM (VLLM_API_BASE).\n",
        "    No asume tools; se configuran fuera con .bind_tools().\n",
        "    \"\"\"\n",
        "    cfg = config or PlannerConfig()\n",
        "\n",
        "    # Compatibilidad: si solo se definió VLLM_LLM_API_BASE, lo usamos como VLLM_API_BASE.\n",
        "    if \"VLLM_LLM_API_BASE\" in os.environ and \"VLLM_API_BASE\" not in os.environ:\n",
        "        os.environ[\"VLLM_API_BASE\"] = os.environ[\"VLLM_LLM_API_BASE\"]\n",
        "\n",
        "    return ChatQwenVllm(\n",
        "        model=cfg.model_name,\n",
        "        temperature=cfg.temperature,\n",
        "        enable_thinking=cfg.enable_thinking,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MRrJoMmPAl39",
        "outputId": "131d1edc-65e2-492c-c6cd-b6257267a8a5"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/tools.py\n",
        "%%writefile agnostic_agent/tools.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Catálogo de herramientas (tools) para el Agnostic Deep Agent 2026.\n",
        "\n",
        "Incluye:\n",
        "- Tools \"toy\" (to_upper, word_count, is_palindrome).\n",
        "- Tools matemáticas seguras (eval_math_expression, sum_numbers, average_numbers).\n",
        "- Tools de modelos Qwen3 locales:\n",
        "    - embed_texts              → Qwen3-Embedding (núcleo reutilizable).\n",
        "    - semantic_search          → búsqueda semántica sobre una lista de textos.\n",
        "    - semantic_search_in_csv   → búsqueda semántica sobre filas de un CSV.\n",
        "    - embed_context_tables     → precálculo de embeddings para tablas de contexto.\n",
        "    - judge_row_with_context   → juicio simple de una fila usando contexto tabular.\n",
        "    - rerank_qwen3             → Qwen3-Reranker\n",
        "\n",
        "El registro global TOOL_REGISTRY permite seleccionar tools por nombre\n",
        "y construir subconjuntos según la configuración (p.ej. setup.yaml).\n",
        "\"\"\"\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "import os\n",
        "import ast\n",
        "import operator as _op\n",
        "import numbers\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,              # Qwen3-Embedding\n",
        "    AutoModelForCausalLM,   # Qwen3-Reranker (via logits yes/no)\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# TOOLS \"toy\"\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "@tool\n",
        "def to_upper(text: str) -> str:\n",
        "    \"\"\"Convierte el texto a MAYÚSCULAS.\"\"\"\n",
        "    return text.upper()\n",
        "\n",
        "\n",
        "@tool\n",
        "def word_count(text: str) -> int:\n",
        "    \"\"\"Devuelve el número de palabras en el texto.\"\"\"\n",
        "    return len([w for w in text.split() if w])\n",
        "\n",
        "\n",
        "@tool\n",
        "def is_palindrome(text: str) -> bool:\n",
        "    \"\"\"True si el texto (sin espacios/casos) es palíndromo.\"\"\"\n",
        "    s = \"\".join(ch.lower() for ch in text if ch.isalnum())\n",
        "    return s == s[::-1]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# TOOLS matemáticas (evaluadas en Python)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "# Operadores permitidos en la expresión matemática\n",
        "_ALLOWED_OPS = {\n",
        "    ast.Add: _op.add,\n",
        "    ast.Sub: _op.sub,\n",
        "    ast.Mult: _op.mul,\n",
        "    ast.Div: _op.truediv,\n",
        "    ast.Pow: _op.pow,\n",
        "    ast.Mod: _op.mod,\n",
        "}\n",
        "\n",
        "\n",
        "def _eval_ast(node: ast.AST) -> float:\n",
        "    \"\"\"Evalúa de forma segura un AST restringido a operaciones aritméticas básicas.\"\"\"\n",
        "    if isinstance(node, ast.Num):  # Python <3.8\n",
        "        return node.n\n",
        "    if isinstance(node, ast.Constant):  # números en 3.8+\n",
        "        if isinstance(node.value, (int, float)):\n",
        "            return node.value\n",
        "        raise ValueError(\"Sólo se permiten números en las constantes.\")\n",
        "\n",
        "    if isinstance(node, ast.BinOp):\n",
        "        op_type = type(node.op)\n",
        "        if op_type not in _ALLOWED_OPS:\n",
        "            raise ValueError(f\"Operador no permitido: {op_type.__name__}\")\n",
        "        left = _eval_ast(node.left)\n",
        "        right = _eval_ast(node.right)\n",
        "        return _ALLOWED_OPS[op_type](left, right)\n",
        "\n",
        "    if isinstance(node, ast.UnaryOp) and isinstance(node.op, ast.USub):\n",
        "        return -_eval_ast(node.operand)\n",
        "\n",
        "    raise ValueError(f\"Nodo de AST no permitido: {type(node).__name__}\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def eval_math_expression(expression: str) -> float:\n",
        "    \"\"\"\n",
        "    Evalúa una expresión matemática sencilla usando Python de forma segura.\n",
        "\n",
        "    Soporta:\n",
        "      - suma, resta, multiplicación, división, módulo, potencias\n",
        "      - paréntesis\n",
        "      - signos unarios (p.ej. -3)\n",
        "\n",
        "    Ejemplos válidos:\n",
        "      \"1 + 2 * 3\"\n",
        "      \"(10 - 4) / 2\"\n",
        "      \"2**3 + 5\"\n",
        "\n",
        "    NOTA:\n",
        "      El operador de potencia soportado es **, NO ^ (que en Python es XOR).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        parsed = ast.parse(expression, mode=\"eval\")\n",
        "        result = _eval_ast(parsed.body)\n",
        "        return float(result)\n",
        "    except Exception as exc:\n",
        "        raise ValueError(\n",
        "            f\"No se pudo evaluar la expresión: {expression!r}. Error: {exc}\"\n",
        "        ) from exc\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Helpers numéricos robustos para sum/avg\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _coerce_to_float(x: Any) -> float:\n",
        "    \"\"\"\n",
        "    Intenta convertir un valor genérico a float.\n",
        "\n",
        "    Soporta:\n",
        "    - ints/floats/np.number\n",
        "    - strings numéricos (\"3.14\")\n",
        "    - dicts con claves típicas: \"value\", \"val\", \"number\", \"num\"\n",
        "    \"\"\"\n",
        "    if isinstance(x, numbers.Number):\n",
        "        return float(x)\n",
        "\n",
        "    if isinstance(x, str):\n",
        "        # Permite strings como \"3.14\", \"42\"\n",
        "        return float(x.strip())\n",
        "\n",
        "    if isinstance(x, dict):\n",
        "        for key in (\"value\", \"val\", \"number\", \"num\"):\n",
        "            if key in x:\n",
        "                return _coerce_to_float(x[key])\n",
        "\n",
        "    raise ValueError(f\"No se pudo interpretar {x!r} como número.\")\n",
        "\n",
        "\n",
        "@tool\n",
        "def sum_numbers(numbers: List[Any]) -> float:\n",
        "    \"\"\"\n",
        "    Devuelve la suma de una lista de números.\n",
        "\n",
        "    La tool es robusta: acepta tanto números puros como estructuras\n",
        "    que contengan el número, por ejemplo:\n",
        "\n",
        "      - [1, 2.5, \"3\"]\n",
        "      - [{\"value\": 10}, {\"number\": \"20\"}]\n",
        "\n",
        "    Para evitar errores de validación Pydantic, el tipo es List[Any]\n",
        "    y se hace coerción interna a float.\n",
        "    \"\"\"\n",
        "    if not isinstance(numbers, list):\n",
        "        raise ValueError(\"El parámetro 'numbers' debe ser una lista.\")\n",
        "\n",
        "    vals = [_coerce_to_float(n) for n in numbers]\n",
        "    return float(sum(vals))\n",
        "\n",
        "\n",
        "@tool\n",
        "def average_numbers(numbers: List[Any]) -> float:\n",
        "    \"\"\"\n",
        "    Devuelve la media aritmética de una lista de números.\n",
        "\n",
        "    Mismo comportamiento robusto que sum_numbers:\n",
        "      - [1, 2.5, \"3\"]\n",
        "      - [{\"value\": 10}, {\"number\": \"20\"}]\n",
        "    \"\"\"\n",
        "    if not isinstance(numbers, list):\n",
        "        raise ValueError(\"El parámetro 'numbers' debe ser una lista.\")\n",
        "    if not numbers:\n",
        "        raise ValueError(\"La lista de números está vacía.\")\n",
        "\n",
        "    vals = [_coerce_to_float(n) for n in numbers]\n",
        "    return float(sum(vals) / len(vals))\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# QWEN3-EMBEDDING – Transformers local\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "_EMB_STATE: Dict[str, Any] = {}\n",
        "\n",
        "\n",
        "def _ensure_embedding_loaded() -> None:\n",
        "    \"\"\"Carga Qwen3-Embedding una sola vez en memoria.\"\"\"\n",
        "    global _EMB_STATE\n",
        "    if _EMB_STATE:\n",
        "        return\n",
        "\n",
        "    model_id = os.getenv(\"EMB_MODEL_ID\", \"Qwen/Qwen3-Embedding-0.6B\")\n",
        "\n",
        "    # Permite forzar device vía env si quieres:\n",
        "    #   QWEN_EMB_DEVICE = \"cuda\" | \"cpu\"\n",
        "    forced_device = os.getenv(\"QWEN_EMB_DEVICE\")\n",
        "    if forced_device in (\"cuda\", \"cpu\"):\n",
        "        device = forced_device\n",
        "    else:\n",
        "        use_cuda = (\n",
        "            os.getenv(\"QWEN_EMB_USE_CUDA\", \"0\").lower() in (\"1\", \"true\", \"yes\")\n",
        "            and torch.cuda.is_available()\n",
        "        )\n",
        "        device = \"cuda\" if use_cuda else \"cpu\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModel.from_pretrained(model_id, trust_remote_code=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    max_length = int(os.getenv(\"QWEN_EMB_MAX_LEN\", \"512\"))\n",
        "\n",
        "    _EMB_STATE.update(\n",
        "        {\n",
        "            \"model_id\": model_id,\n",
        "            \"device\": device,\n",
        "            \"tokenizer\": tokenizer,\n",
        "            \"model\": model,\n",
        "            \"max_length\": max_length,\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def _embed_texts_core(inputs: List[str]) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Núcleo de embeddings: recibe una lista de textos y devuelve un array (n, d).\n",
        "\n",
        "    Se usa internamente por:\n",
        "      - embed_texts (tool)\n",
        "      - semantic_search\n",
        "      - semantic_search_in_csv\n",
        "      - embed_context_tables\n",
        "    \"\"\"\n",
        "    _ensure_embedding_loaded()\n",
        "    state = _EMB_STATE\n",
        "\n",
        "    tokenizer = state[\"tokenizer\"]\n",
        "    model = state[\"model\"]\n",
        "    device = state[\"device\"]\n",
        "    max_length = state[\"max_length\"]\n",
        "\n",
        "    if not inputs:\n",
        "        return np.zeros((0, 0), dtype=\"float32\")\n",
        "\n",
        "    enc = tokenizer(\n",
        "        inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**enc)\n",
        "        last_hidden = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
        "        emb = last_hidden.mean(dim=1)           # (batch, hidden)\n",
        "\n",
        "    return emb.cpu().numpy()\n",
        "\n",
        "\n",
        "@tool\n",
        "def embed_texts(texts: List[str]) -> List[List[float]]:\n",
        "    \"\"\"\n",
        "    Devuelve embeddings Qwen3-Embedding para cada texto, usando Transformers local.\n",
        "\n",
        "    - Usa EMB_MODEL_ID (por defecto Qwen/Qwen3-Embedding-0.6B).\n",
        "    - Por defecto corre en CPU (QWEN_EMB_USE_CUDA=0).\n",
        "    - Devuelve una lista de vectores (list[list[float]]).\n",
        "\n",
        "    NOTA:\n",
        "    - Internamente usa _embed_texts_core para compartir el mismo estado\n",
        "      con otras tools (semantic_search, semantic_search_in_csv, embed_context_tables).\n",
        "    \"\"\"\n",
        "    if isinstance(texts, str):\n",
        "        inputs = [texts]\n",
        "    else:\n",
        "        inputs = list(texts)\n",
        "\n",
        "    if not inputs:\n",
        "        return []\n",
        "\n",
        "    emb = _embed_texts_core(inputs)\n",
        "    return emb.tolist()\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# BÚSQUEDA SEMÁNTICA GENÉRICA (en memoria)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _cosine_sim_matrix(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Matriz de similitud coseno entre:\n",
        "      - a: (n, d)\n",
        "      - b: (m, d)\n",
        "\n",
        "    Devuelve: (n, m).\n",
        "    \"\"\"\n",
        "    if a.size == 0 or b.size == 0:\n",
        "        return np.zeros((a.shape[0], b.shape[0]), dtype=\"float32\")\n",
        "\n",
        "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-8)\n",
        "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-8)\n",
        "    return np.matmul(a_norm, b_norm.T)\n",
        "\n",
        "\n",
        "@tool\n",
        "def semantic_search(\n",
        "    query: str,\n",
        "    documents: List[str],\n",
        "    top_k: int = 5,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Búsqueda semántica simple usando Qwen3-Embedding sobre una lista de textos.\n",
        "\n",
        "    Pensado para casos como:\n",
        "      - Dado un query, rankear párrafos, definiciones, cláusulas, etc.\n",
        "      - Integrar tablas ya \"serializadas\" a textos (fila → string).\n",
        "\n",
        "    Parámetros:\n",
        "      - query: texto de búsqueda.\n",
        "      - documents: lista de textos candidatos.\n",
        "      - top_k: número máximo de resultados a devolver.\n",
        "\n",
        "    Devuelve una lista de dicts:\n",
        "      [{ \"index\": int, \"document\": str, \"score\": float }, ...]\n",
        "    ordenada por score descendente.\n",
        "    \"\"\"\n",
        "    if isinstance(documents, str):\n",
        "        docs = [documents]\n",
        "    else:\n",
        "        docs = list(documents)\n",
        "\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    # Embeddings: query (1, d) y docs (n, d)\n",
        "    query_emb = _embed_texts_core([query])          # (1, d)\n",
        "    docs_emb = _embed_texts_core(docs)              # (n, d)\n",
        "\n",
        "    sims = _cosine_sim_matrix(query_emb, docs_emb)  # (1, n)\n",
        "    scores = sims[0]\n",
        "\n",
        "    # Top-k\n",
        "    top_k = max(1, min(top_k, len(docs)))\n",
        "    indices = np.argsort(-scores)[:top_k]\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    for idx in indices:\n",
        "        results.append(\n",
        "            {\n",
        "                \"index\": int(idx),\n",
        "                \"document\": docs[idx],\n",
        "                \"score\": float(scores[idx]),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# BÚSQUEDA SEMÁNTICA EN CSV (parametrías, diccionarios, etc.)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "# Cache por (csv_path, columnas_join) → { \"df\": DataFrame, \"emb\": np.ndarray, \"texts\": List[str] }\n",
        "_CSV_EMB_CACHE: Dict[str, Any] = {}\n",
        "\n",
        "\n",
        "def _get_csv_embeddings(\n",
        "    csv_path: str,\n",
        "    text_columns: List[str],\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Carga (o reutiliza) embeddings por fila para un CSV.\n",
        "\n",
        "    - csv_path: ruta al CSV (p.ej. parametrias.csv, diccionario_abreviaturas.csv).\n",
        "    - text_columns: columnas a concatenar para generar el texto base.\n",
        "\n",
        "    Devuelve un dict:\n",
        "      {\n",
        "        \"df\": DataFrame,\n",
        "        \"emb\": np.ndarray (n_filas, d),\n",
        "        \"texts\": List[str]  # representación textual de cada fila\n",
        "      }\n",
        "    \"\"\"\n",
        "    key = f\"{os.path.abspath(csv_path)}|{'|'.join(text_columns)}\"\n",
        "    if key in _CSV_EMB_CACHE:\n",
        "        return _CSV_EMB_CACHE[key]\n",
        "\n",
        "    if not os.path.exists(csv_path):\n",
        "        raise FileNotFoundError(f\"No se encontró el CSV: {csv_path}\")\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # Normalizamos text_columns a las que existan\n",
        "    cols = [c for c in text_columns if c in df.columns]\n",
        "    if not cols:\n",
        "        raise ValueError(\n",
        "            f\"Ninguna de las columnas {text_columns!r} existe en el CSV {csv_path!r}.\"\n",
        "        )\n",
        "\n",
        "    texts: List[str] = []\n",
        "    for _, row in df.iterrows():\n",
        "        parts = []\n",
        "        for col in cols:\n",
        "            val = row.get(col, \"\")\n",
        "            if pd.isna(val):\n",
        "                continue\n",
        "            parts.append(f\"{col}: {val}\")\n",
        "        texts.append(\" | \".join(parts))\n",
        "\n",
        "    emb = _embed_texts_core(texts)  # (n_filas, d)\n",
        "\n",
        "    payload = {\n",
        "        \"df\": df,\n",
        "        \"emb\": emb,\n",
        "        \"texts\": texts,\n",
        "    }\n",
        "    _CSV_EMB_CACHE[key] = payload\n",
        "    return payload\n",
        "\n",
        "\n",
        "@tool\n",
        "def semantic_search_in_csv(\n",
        "    query: str,\n",
        "    csv_path: str,\n",
        "    text_columns: List[str],\n",
        "    top_k: int = 5,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Búsqueda semántica sobre filas de un CSV usando Qwen3-Embedding.\n",
        "\n",
        "    Casos típicos:\n",
        "      - csv_path=\"parametrias.csv\", text_columns=[\"campo\", \"descripcion\", \"regla\"]\n",
        "      - csv_path=\"diccionario_abreviaturas.csv\", text_columns=[\"abreviatura\", \"definicion\"]\n",
        "\n",
        "    Parámetros:\n",
        "      - query: texto de búsqueda (ej. \"monto máximo de crédito hipotecario\").\n",
        "      - csv_path: ruta al archivo CSV.\n",
        "      - text_columns: columnas que se concatenan para representar cada fila.\n",
        "      - top_k: número máximo de filas a devolver.\n",
        "\n",
        "    Devuelve:\n",
        "      [\n",
        "        {\n",
        "          \"row_index\": int,\n",
        "          \"score\": float,\n",
        "          \"row\": {col: valor, ...},\n",
        "          \"text\": \"col1: ... | col2: ...\"\n",
        "        },\n",
        "        ...\n",
        "      ]\n",
        "    \"\"\"\n",
        "    payload = _get_csv_embeddings(csv_path, text_columns)\n",
        "    df: pd.DataFrame = payload[\"df\"]\n",
        "    emb: np.ndarray = payload[\"emb\"]\n",
        "\n",
        "    if df.empty:\n",
        "        return []\n",
        "\n",
        "    # Embedding del query\n",
        "    query_emb = _embed_texts_core([query])  # (1, d)\n",
        "    sims = _cosine_sim_matrix(query_emb, emb)[0]  # (n_filas,)\n",
        "\n",
        "    top_k = max(1, min(top_k, len(df)))\n",
        "    indices = np.argsort(-sims)[:top_k]\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    for idx in indices:\n",
        "        row_data = df.iloc[int(idx)].to_dict()\n",
        "        results.append(\n",
        "            {\n",
        "                \"row_index\": int(idx),\n",
        "                \"score\": float(sims[idx]),\n",
        "                \"row\": row_data,\n",
        "                \"text\": payload[\"texts\"][int(idx)],\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# CONTEXTO: precálculo de embeddings de tablas\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "@tool\n",
        "def embed_context_tables(\n",
        "    table_paths: List[str],\n",
        "    text_columns: Dict[str, List[str]] | None = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Precálcula embeddings por fila para varias tablas de contexto (CSV):\n",
        "\n",
        "      - Parametrías (reglas, umbrales, categorías, etc.).\n",
        "      - Diccionarios de abreviaturas/definiciones.\n",
        "      - Otras tablas de contexto que quieras.\n",
        "\n",
        "    Parámetros:\n",
        "      - table_paths: lista de rutas a CSVs de contexto.\n",
        "      - text_columns: mapa opcional {path: [cols,...]} con las columnas\n",
        "        que se usarán para generar los textos por fila. Si no se indica\n",
        "        para un path, se usan todas las columnas del CSV.\n",
        "\n",
        "    Efectos:\n",
        "      - Llama internamente a _get_csv_embeddings(...) para cada tabla,\n",
        "        poblando la caché interna _CSV_EMB_CACHE.\n",
        "      - Reutiliza el mismo modelo de embeddings que embed_texts.\n",
        "\n",
        "    Devuelve un resumen:\n",
        "\n",
        "      {\n",
        "        \"tables\": [\n",
        "          {\n",
        "            \"path\": \"<ruta_csv>\",\n",
        "            \"n_rows\": int,\n",
        "            \"n_cols\": int,\n",
        "            \"text_columns\": [ ... ]\n",
        "          },\n",
        "          ...\n",
        "        ],\n",
        "        \"embedding_dim\": int | null\n",
        "      }\n",
        "    \"\"\"\n",
        "    if isinstance(table_paths, str):\n",
        "        paths = [table_paths]\n",
        "    else:\n",
        "        paths = list(table_paths)\n",
        "\n",
        "    tables_info: List[Dict[str, Any]] = []\n",
        "    emb_dim = None\n",
        "\n",
        "    for p in paths:\n",
        "        # Determinar columnas de texto para este path\n",
        "        cols = None\n",
        "        if text_columns and isinstance(text_columns, dict):\n",
        "            cols = text_columns.get(p)\n",
        "\n",
        "        if not cols:\n",
        "            # Si no se especifican columnas, usamos todas\n",
        "            if not os.path.exists(p):\n",
        "                raise FileNotFoundError(f\"No se encontró el CSV: {p}\")\n",
        "            df_head = pd.read_csv(p, nrows=1)\n",
        "            cols = list(df_head.columns)\n",
        "\n",
        "        payload = _get_csv_embeddings(p, cols)\n",
        "        df = payload[\"df\"]\n",
        "        emb = payload[\"emb\"]\n",
        "\n",
        "        if emb.size > 0:\n",
        "            d = emb.shape[1]\n",
        "            if emb_dim is None:\n",
        "                emb_dim = d\n",
        "            elif emb_dim != d:\n",
        "                # Si hay discrepancia, mantenemos el primero y no reventamos\n",
        "                pass\n",
        "\n",
        "        tables_info.append(\n",
        "            {\n",
        "                \"path\": p,\n",
        "                \"n_rows\": int(len(df)),\n",
        "                \"n_cols\": int(len(df.columns)),\n",
        "                \"text_columns\": cols,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return {\n",
        "        \"tables\": tables_info,\n",
        "        \"embedding_dim\": emb_dim,\n",
        "    }\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# QWEN3-RERANKER – Transformers local\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "_RERANK_STATE: Dict[str, Any] = {}\n",
        "\n",
        "\n",
        "def _ensure_reranker_loaded() -> None:\n",
        "    \"\"\"Carga Qwen3-Reranker una sola vez en memoria.\"\"\"\n",
        "    global _RERANK_STATE\n",
        "    if _RERANK_STATE:\n",
        "        return\n",
        "\n",
        "    model_id = os.getenv(\"RERANK_MODEL_ID\", \"Qwen/Qwen3-Reranker-0.6B\")\n",
        "\n",
        "    forced_device = os.getenv(\"QWEN_RERANK_DEVICE\")\n",
        "    if forced_device in (\"cuda\", \"cpu\"):\n",
        "        device = forced_device\n",
        "    else:\n",
        "        use_cuda = (\n",
        "            os.getenv(\"QWEN_RERANK_USE_CUDA\", \"0\").lower() in (\"1\", \"true\", \"yes\")\n",
        "            and torch.cuda.is_available()\n",
        "        )\n",
        "        device = \"cuda\" if use_cuda else \"cpu\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Importante: trust_remote_code=True para Qwen3-Reranker\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    true_token_id = tokenizer(\"yes\", add_special_tokens=False).input_ids[0]\n",
        "    false_token_id = tokenizer(\"no\", add_special_tokens=False).input_ids[0]\n",
        "\n",
        "    max_length = int(os.getenv(\"QWEN_RERANK_MAX_LEN\", \"1024\"))\n",
        "\n",
        "    _RERANK_STATE.update(\n",
        "        {\n",
        "            \"model_id\": model_id,\n",
        "            \"device\": device,\n",
        "            \"tokenizer\": tokenizer,\n",
        "            \"model\": model,\n",
        "            \"true_token_id\": true_token_id,\n",
        "            \"false_token_id\": false_token_id,\n",
        "            \"max_length\": max_length,\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "def _format_rerank_prompts(\n",
        "    query: str,\n",
        "    docs: List[str],\n",
        "    instruction: str,\n",
        ") -> List[str]:\n",
        "    prompts: List[str] = []\n",
        "    for doc in docs:\n",
        "        text = (\n",
        "            \"You are a relevance judge. \"\n",
        "            \"Decide if the document answers the query.\\n\\n\"\n",
        "            f\"Instruction: {instruction}\\n\"\n",
        "            f\"Query: {query}\\n\"\n",
        "            f\"Document: {doc}\\n\\n\"\n",
        "            \"Answer with 'yes' if it is relevant, otherwise 'no'.\"\n",
        "        )\n",
        "        prompts.append(text)\n",
        "    return prompts\n",
        "\n",
        "\n",
        "@tool\n",
        "def rerank_qwen3(query: str, documents: List[str]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Usa Qwen3-Reranker (local, vía Transformers) para ordenar documentos por relevancia.\n",
        "\n",
        "    - Usa RERANK_MODEL_ID (por defecto Qwen/Qwen3-Reranker-0.6B).\n",
        "    - Por defecto corre en CPU (QWEN_RERANK_USE_CUDA=0).\n",
        "    - Devuelve [{index, document, score}] ordenado por score desc.\n",
        "    \"\"\"\n",
        "    _ensure_reranker_loaded()\n",
        "    state = _RERANK_STATE\n",
        "\n",
        "    tokenizer = state[\"tokenizer\"]\n",
        "    model = state[\"model\"]\n",
        "    device = state[\"device\"]\n",
        "    true_token_id = state[\"true_token_id\"]\n",
        "    false_token_id = state[\"false_token_id\"]\n",
        "    max_length = state[\"max_length\"]\n",
        "\n",
        "    if isinstance(documents, str):\n",
        "        docs = [documents]\n",
        "    else:\n",
        "        docs = list(documents)\n",
        "\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    instruction = os.getenv(\n",
        "        \"QWEN_RERANK_INSTRUCT\",\n",
        "        \"Given a web search query, rank documents by how well they answer the query.\",\n",
        "    )\n",
        "\n",
        "    prompts = _format_rerank_prompts(query, docs, instruction)\n",
        "\n",
        "    enc = tokenizer(\n",
        "        prompts,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    enc = {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**enc)\n",
        "        logits = outputs.logits[:, -1, :]   # (batch, vocab)\n",
        "        yes_logits = logits[:, true_token_id]\n",
        "        no_logits = logits[:, false_token_id]\n",
        "        stacked = torch.stack([no_logits, yes_logits], dim=-1)  # (batch, 2)\n",
        "        probs = torch.nn.functional.softmax(stacked, dim=-1)[:, 1].tolist()\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    for idx, (doc, score) in enumerate(zip(docs, probs)):\n",
        "        results.append(\n",
        "            {\n",
        "                \"index\": idx,\n",
        "                \"document\": doc,\n",
        "                \"score\": float(score),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    results.sort(key=lambda x: x[\"score\"], reverse=True)\n",
        "    return results\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# JUICIO FILA + CONTEXTO (parametrías / diccionarios)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "@tool\n",
        "def judge_row_with_context(\n",
        "    row: Dict[str, Any],\n",
        "    param_hits: List[Dict[str, Any]] | None = None,\n",
        "    glossary_hits: List[Dict[str, Any]] | None = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Aplica un juicio simple sobre una fila de atributos usando hits de contexto.\n",
        "\n",
        "    Pensado para el caso:\n",
        "      - row: fila de atributos de un contrato (input A).\n",
        "      - param_hits: resultados de semantic_search_in_csv sobre parametrías.\n",
        "      - glossary_hits: resultados de semantic_search_in_csv sobre diccionario\n",
        "        de abreviaturas / definiciones.\n",
        "\n",
        "    NO usa modelos de lenguaje dentro de la tool; es puramente determinista\n",
        "    y genérico. El LLM puede decidir cómo interpretar el resultado.\n",
        "\n",
        "    Reglas sencillas:\n",
        "      - Si NO hay hits ni en param_hits ni en glossary_hits → \"review_required\".\n",
        "      - Si hay al menos un hit en alguna de las dos → \"ok\".\n",
        "      - Se devuelve también un pequeño resumen de motivos.\n",
        "\n",
        "    Devuelve un dict como:\n",
        "\n",
        "      {\n",
        "        \"contract_id\": str | int | null,\n",
        "        \"judgement\": \"ok\" | \"review_required\",\n",
        "        \"reasons\": [str, ...],\n",
        "        \"row\": row,\n",
        "        \"param_hits\": [...],\n",
        "        \"glossary_hits\": [...],\n",
        "      }\n",
        "    \"\"\"\n",
        "    param_hits = param_hits or []\n",
        "    glossary_hits = glossary_hits or []\n",
        "\n",
        "    # Heurística para encontrar un identificador de contrato/fila\n",
        "    contract_id = (\n",
        "        row.get(\"numero_contrato\")\n",
        "        or row.get(\"numero de contrato\")\n",
        "        or row.get(\"num_contrato\")\n",
        "        or row.get(\"contract_number\")\n",
        "        or row.get(\"contract_id\")\n",
        "        or row.get(\"id\")\n",
        "    )\n",
        "\n",
        "    has_context = bool(param_hits or glossary_hits)\n",
        "    judgement = \"ok\" if has_context else \"review_required\"\n",
        "\n",
        "    reasons: List[str] = []\n",
        "    if not has_context:\n",
        "        reasons.append(\n",
        "            \"No se encontraron coincidencias ni en las parametrías ni en el diccionario; \"\n",
        "            \"se recomienda revisión manual.\"\n",
        "        )\n",
        "    else:\n",
        "        if param_hits:\n",
        "            best_param = param_hits[0]\n",
        "            desc = best_param.get(\"text\") or str(best_param.get(\"row\", \"\"))[:200]\n",
        "            reasons.append(\n",
        "                f\"Se encontraron al menos {len(param_hits)} filas relevantes en la tabla de parametrías. \"\n",
        "                f\"Ejemplo: {desc}\"\n",
        "            )\n",
        "        if glossary_hits:\n",
        "            best_gl = glossary_hits[0]\n",
        "            desc = best_gl.get(\"text\") or str(best_gl.get(\"row\", \"\"))[:200]\n",
        "            reasons.append(\n",
        "                f\"Se encontraron al menos {len(glossary_hits)} filas relevantes en el diccionario. \"\n",
        "                f\"Ejemplo: {desc}\"\n",
        "            )\n",
        "\n",
        "    return {\n",
        "        \"contract_id\": contract_id,\n",
        "        \"judgement\": judgement,\n",
        "        \"reasons\": reasons,\n",
        "        \"row\": row,\n",
        "        \"param_hits\": param_hits,\n",
        "        \"glossary_hits\": glossary_hits,\n",
        "    }\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Registro de tools\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "TOOL_REGISTRY: Dict[str, Any] = {\n",
        "    \"to_upper\": to_upper,\n",
        "    \"word_count\": word_count,\n",
        "    \"is_palindrome\": is_palindrome,\n",
        "    \"eval_math_expression\": eval_math_expression,\n",
        "    \"sum_numbers\": sum_numbers,\n",
        "    \"average_numbers\": average_numbers,\n",
        "    \"embed_texts\": embed_texts,\n",
        "    \"semantic_search\": semantic_search,\n",
        "    \"semantic_search_in_csv\": semantic_search_in_csv,\n",
        "    \"embed_context_tables\": embed_context_tables,\n",
        "    \"rerank_qwen3\": rerank_qwen3,\n",
        "    \"judge_row_with_context\": judge_row_with_context,\n",
        "}\n",
        "\n",
        "\n",
        "def get_default_tools(enabled_names: List[str] | None = None) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Tools por defecto del agente agnóstico.\n",
        "\n",
        "    - Si enabled_names es None → devuelve TODAS las tools registradas.\n",
        "    - Si enabled_names es una lista → sólo devuelve las que estén en TOOL_REGISTRY.\n",
        "    \"\"\"\n",
        "    if enabled_names is None:\n",
        "        return list(TOOL_REGISTRY.values())\n",
        "    return [TOOL_REGISTRY[name] for name in enabled_names if name in TOOL_REGISTRY]\n",
        "\n",
        "\n",
        "def get_tools_by_names(names: List[str]) -> List[Any]:\n",
        "    \"\"\"\n",
        "    Permite seleccionar tools por nombre desde TOOL_REGISTRY.\n",
        "    Equivalente a get_default_tools(names), se mantiene por claridad semántica.\n",
        "    \"\"\"\n",
        "    return get_default_tools(names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "JaERrasTAcau",
        "outputId": "5bfe5845-a029-4ae6-ceee-1ad25529649a"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/communication.py\n",
        "%%writefile agnostic_agent/communication.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Contratos de I/O de alto nivel del Agnostic Deep Agent 2026.\n",
        "\n",
        "Este módulo define:\n",
        "- AgentInput:  payload de entrada para el agente.\n",
        "- ToolRun:     traza de ejecución de herramientas (para vistas deep/dev).\n",
        "- AgentSummary:resumen por fases del pipeline interno.\n",
        "- AgentView:   una vista del resultado para un \"rol\" (user / deep / dev).\n",
        "- AgentOutput: objeto de salida unificado con las tres vistas.\n",
        "\n",
        "NOTAS:\n",
        "- Los contratos internos más ricos (AnalyzerIntent, PlannerPlan, etc.)\n",
        "  vivirán en `schemas.py`. Aquí sólo se define I/O de \"frontera\".\n",
        "- El campo canónico es `user_prompt`, pero se acepta también `user_text`\n",
        "  como alias para compatibilidad.\n",
        "\"\"\"\n",
        "\n",
        "from typing import Any, Dict, List, Optional\n",
        "\n",
        "from pydantic import BaseModel, Field, ConfigDict\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# INPUT\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class AgentInput(BaseModel):\n",
        "    \"\"\"\n",
        "    Input \"oficial\" del agente.\n",
        "\n",
        "    - user_prompt: texto natural del usuario (nombre canónico).\n",
        "    - session_id: id opcional de sesión (por si quieres manejar múltiples).\n",
        "    - kb_names: lista de KBs a considerar (FAOSTAT_WORLD, etc.).\n",
        "    - metadata: extras arbitrarios (canal, idioma detectado, flags, etc.).\n",
        "    - data_payload: payload estructurado para este turno\n",
        "        (por ejemplo, fila de atributos, texto OCR, tablas, etc.).\n",
        "\n",
        "    Compatibilidad:\n",
        "    - También acepta el alias `user_text` al instanciar el modelo.\n",
        "    \"\"\"\n",
        "\n",
        "    # Permite poblar usando el nombre de campo o el alias\n",
        "    model_config = ConfigDict(populate_by_name=True)\n",
        "\n",
        "    user_prompt: str = Field(\n",
        "        ...,\n",
        "        description=\"Texto de entrada del usuario.\",\n",
        "        alias=\"user_text\",  # compat: antes usábamos `user_text`\n",
        "    )\n",
        "    session_id: Optional[str] = Field(\n",
        "        default=None,\n",
        "        description=\"Identificador opcional de sesión.\",\n",
        "    )\n",
        "    kb_names: List[str] = Field(\n",
        "        default_factory=list,\n",
        "        description=\"Nombres de Knowledge Bases relevantes.\",\n",
        "    )\n",
        "    metadata: Dict[str, Any] = Field(\n",
        "        default_factory=dict,\n",
        "        description=\"Metadatos arbitrarios asociados a la petición.\",\n",
        "    )\n",
        "    data_payload: Dict[str, Any] = Field(\n",
        "        default_factory=dict,\n",
        "        description=(\n",
        "            \"Payload estructurado asociado al turno. \"\n",
        "            \"Ejemplos: una fila de atributos, texto OCR, \"\n",
        "            \"tablas de entrada, IDs de contrato, etc.\"\n",
        "        ),\n",
        "    )\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# TRAZAS DE TOOLS + RESUMEN\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class ToolRun(BaseModel):\n",
        "    \"\"\"\n",
        "    Ejecución de una herramienta.\n",
        "\n",
        "    - id: tool_call_id interno o identificador de step.\n",
        "    - name: nombre de la tool.\n",
        "    - args: argumentos con los que se llamó (ya resueltos).\n",
        "    - output: salida cruda (sin postprocesar).\n",
        "    \"\"\"\n",
        "    id: str\n",
        "    name: str\n",
        "    args: Dict[str, Any]\n",
        "    output: Any\n",
        "\n",
        "\n",
        "class AgentSummary(BaseModel):\n",
        "    \"\"\"\n",
        "    Resumen estructurado del pipeline:\n",
        "\n",
        "      ANALYZER → PLANNER → EXECUTOR → CATCHER → VALIDATOR → MEMORY → SUMMARIZER\n",
        "    \"\"\"\n",
        "    analyzer: str = \"\"\n",
        "    planner: str = \"\"\n",
        "    executor: str = \"\"\n",
        "    catcher: str = \"\"\n",
        "    validator: str = \"\"   # nuevo nodo explícito\n",
        "    memory: str = \"\"      # resumen de escritura/uso de memoria\n",
        "    summarizer: str = \"\"\n",
        "    final_answer: str = \"\"\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# VISTAS POR ROL\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class AgentView(BaseModel):\n",
        "    \"\"\"\n",
        "    Vista del agente para un \"rol\" (dev / deep / user).\n",
        "\n",
        "    - final_answer: texto final para ese rol.\n",
        "    - summary: breakdown interno del pipeline.\n",
        "    - tool_runs: ejecuciones de tools (ya normalizadas).\n",
        "    - raw_state: estado crudo del grafo (según rol, puede ir vacío).\n",
        "    \"\"\"\n",
        "    final_answer: str = \"\"\n",
        "    summary: Optional[AgentSummary] = None\n",
        "    tool_runs: List[ToolRun] = Field(default_factory=list)\n",
        "    raw_state: Dict[str, Any] = Field(default_factory=dict)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# OUTPUT UNIFICADO\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class AgentOutput(BaseModel):\n",
        "    \"\"\"\n",
        "    Output \"oficial\" del agente (UN SOLO OBJETO):\n",
        "\n",
        "      {\n",
        "        \"dev_out\":  AgentView(...),  # traza completa (pipeline + raw_state)\n",
        "        \"deep_out\": AgentView(...),  # resumen por sección\n",
        "        \"user_out\": AgentView(...),  # respuesta 1:1 para usuario final\n",
        "      }\n",
        "    \"\"\"\n",
        "    dev_out: AgentView\n",
        "    deep_out: AgentView\n",
        "    user_out: AgentView\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Devuelve un dict puro {dev_out, deep_out, user_out}, listo para JSON / logging.\n",
        "        \"\"\"\n",
        "        return self.model_dump()\n",
        "\n",
        "    # ─────────────────────────────────────\n",
        "    # Helper para construir desde AgentState\n",
        "    # ─────────────────────────────────────\n",
        "    @classmethod\n",
        "    def from_state(cls, state: Dict[str, Any]) -> \"AgentOutput\":\n",
        "        \"\"\"\n",
        "        Crea un AgentOutput a partir del estado interno del grafo.\n",
        "\n",
        "        Convenciones esperadas (pero tolerantes):\n",
        "        - state[\"user_out\"], state[\"deep_out\"], state[\"dev_out\"]:\n",
        "            respuestas finales (str) por rol.\n",
        "          Alternativamente:\n",
        "            state[\"user_answer\"], state[\"deep_answer\"], state[\"dev_answer\"].\n",
        "        - state[\"tool_runs\"]:\n",
        "            lista de ToolRun, dicts o estructuras similares.\n",
        "        - summaries opcionales:\n",
        "            puedes mapear campos del state a AgentSummary si lo deseas\n",
        "            vía `state[\"pipeline_summary\"]`.\n",
        "        \"\"\"\n",
        "        # Tool runs normalizados\n",
        "        tool_runs_norm: List[ToolRun] = []\n",
        "        raw_tool_runs = state.get(\"tool_runs\", []) or []\n",
        "\n",
        "        for tr in raw_tool_runs:\n",
        "            if isinstance(tr, ToolRun):\n",
        "                tool_runs_norm.append(tr)\n",
        "            elif isinstance(tr, BaseModel):\n",
        "                tool_runs_norm.append(ToolRun.model_validate(tr.model_dump()))\n",
        "            elif isinstance(tr, dict):\n",
        "                # Puede venir con claves extra, las ignoramos\n",
        "                tool_runs_norm.append(ToolRun.model_validate(tr))\n",
        "            else:\n",
        "                # Si es algo raro, lo metemos como output genérico\n",
        "                tool_runs_norm.append(\n",
        "                    ToolRun(\n",
        "                        id=str(len(tool_runs_norm)),\n",
        "                        name=\"unknown\",\n",
        "                        args={},\n",
        "                        output=tr,\n",
        "                    )\n",
        "                )\n",
        "\n",
        "        # Summary opcional, si el grafo lo llena\n",
        "        summary: Optional[AgentSummary] = None\n",
        "        summary_data = state.get(\"pipeline_summary\")\n",
        "        if isinstance(summary_data, AgentSummary):\n",
        "            summary = summary_data\n",
        "        elif isinstance(summary_data, BaseModel):\n",
        "            summary = AgentSummary.model_validate(summary_data.model_dump())\n",
        "        elif isinstance(summary_data, dict):\n",
        "            summary = AgentSummary.model_validate(summary_data)\n",
        "\n",
        "        # Vistas\n",
        "        user_view = AgentView(\n",
        "            final_answer=state.get(\"user_out\", \"\") or state.get(\"user_answer\", \"\"),\n",
        "            summary=summary,\n",
        "            tool_runs=[],  # por defecto no cargamos tool_runs en la vista de usuario\n",
        "            raw_state={},\n",
        "        )\n",
        "\n",
        "        deep_view = AgentView(\n",
        "            final_answer=state.get(\"deep_out\", \"\") or state.get(\"deep_answer\", \"\"),\n",
        "            summary=summary,\n",
        "            tool_runs=tool_runs_norm,\n",
        "            raw_state={},  # opcionalmente podrías incluir fragmentos de estado aquí\n",
        "        )\n",
        "\n",
        "        dev_view = AgentView(\n",
        "            final_answer=state.get(\"dev_out\", \"\") or state.get(\"dev_answer\", \"\"),\n",
        "            summary=summary,\n",
        "            tool_runs=tool_runs_norm,\n",
        "            raw_state=state,  # vista de desarrollo: estado crudo completo\n",
        "        )\n",
        "\n",
        "        return cls(dev_out=dev_view, deep_out=deep_view, user_out=user_view)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "dKOVfmDfAoVY",
        "outputId": "eca3da18-f2ea-4400-9c47-08b981b1dff7"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/context.py\n",
        "%%writefile agnostic_agent/context.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Contexto externo y Knowledge Bases para el Agnostic Deep Agent 2026.\n",
        "\n",
        "Este módulo define:\n",
        "- KnowledgeBase: descriptor ligero de una KB (faostat, sql, vector, api, etc.).\n",
        "- get_default_context(): lista base de KBs disponibles (opcionalmente desde setup.yaml).\n",
        "- get_kb_by_names(): helper para filtrar KBs según AgentInput.kb_names.\n",
        "- build_kb_from_paths(): helper para crear KBs a partir de rutas (CSV, SQLite, etc.).\n",
        "- build_kb_from_setup(): helper para mapear la sección `knowledge_bases`\n",
        "  de setup.yaml a objetos KnowledgeBase.\n",
        "\n",
        "Casos típicos:\n",
        "- kind=\"sqlite\"      → BD tabular (SQLite clásica).\n",
        "- kind=\"sqlite-vec\"  → VDB vectorial sobre sqlite-vec.\n",
        "- kind=\"table\"       → tabla externa (CSV, parquet) tratada como KB tabular.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Dict, List, Optional, Iterable\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    \"KnowledgeBase\",\n",
        "    \"build_kb_from_paths\",\n",
        "    \"build_kb_from_setup\",\n",
        "    \"get_default_context\",\n",
        "    \"get_kb_by_names\",\n",
        "]\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class KnowledgeBase:\n",
        "    \"\"\"\n",
        "    Descriptor simple de una KB tabular/vectorial/etc.\n",
        "\n",
        "    - name: identificador único de la KB (ej. 'FAOSTAT_WORLD').\n",
        "    - kind: tipo de backend (ej. 'sqlite', 'sqlite-vec', 'table', 'api', ...).\n",
        "    - config: parámetros para conectar (rutas, DSNs, tablas, índices, etc.).\n",
        "\n",
        "      Convenciones suaves para `config` (no obligatorias, pero recomendadas):\n",
        "      - \"path\": ruta a archivo o DB (CSV, .db, parquet, etc.).\n",
        "      - \"table\": nombre de tabla (si aplica, p.ej. en SQLite).\n",
        "      - \"role\": rol lógico ('param_table', 'rules_table', 'dictionary', ...).\n",
        "    \"\"\"\n",
        "    name: str\n",
        "    kind: str = \"generic\"\n",
        "    config: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    # Helpers de conveniencia (no obligatorios)\n",
        "    @property\n",
        "    def path(self) -> Optional[str]:\n",
        "        \"\"\"Ruta principal asociada a la KB (si existe en config).\"\"\"\n",
        "        return self.config.get(\"path\")\n",
        "\n",
        "    @property\n",
        "    def table(self) -> Optional[str]:\n",
        "        \"\"\"Nombre de tabla asociado (si aplica en backends tabulares).\"\"\"\n",
        "        return self.config.get(\"table\")\n",
        "\n",
        "    @property\n",
        "    def role(self) -> Optional[str]:\n",
        "        \"\"\"Rol lógico de la KB (param_table, rules_table, dictionary, ...).\"\"\"\n",
        "        return self.config.get(\"role\")\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Builders de KB\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def build_kb_from_paths(\n",
        "    paths: Iterable[str],\n",
        "    *,\n",
        "    kind: str = \"table\",\n",
        "    default_role: Optional[str] = None,\n",
        ") -> List[KnowledgeBase]:\n",
        "    \"\"\"\n",
        "    Crea KnowledgeBase a partir de una lista de rutas (CSV, parquet, etc.).\n",
        "\n",
        "    Pensado para casos como:\n",
        "      context = [\"parametrias.csv\", \"diccionario_abreviaturas.csv\"]\n",
        "\n",
        "    Ejemplo:\n",
        "        kbs = build_kb_from_paths(\n",
        "            [\"parametrias.csv\", \"diccionario.csv\"],\n",
        "            kind=\"table\",\n",
        "        )\n",
        "\n",
        "    Cada KB tendrá:\n",
        "      - name: nombre derivado del stem del archivo (MAYÚSCULAS).\n",
        "      - kind: el proporcionado (por defecto \"table\").\n",
        "      - config: {\"path\": <ruta>, \"role\": default_role} si se pasa.\n",
        "    \"\"\"\n",
        "    kb_list: List[KnowledgeBase] = []\n",
        "    for p in paths:\n",
        "        if not p:\n",
        "            continue\n",
        "        path_obj = Path(p)\n",
        "        name = path_obj.stem.upper()\n",
        "        cfg: Dict[str, Any] = {\"path\": str(path_obj)}\n",
        "        if default_role is not None:\n",
        "            cfg[\"role\"] = default_role\n",
        "        kb_list.append(\n",
        "            KnowledgeBase(\n",
        "                name=name,\n",
        "                kind=kind,\n",
        "                config=cfg,\n",
        "            )\n",
        "        )\n",
        "    return kb_list\n",
        "\n",
        "\n",
        "def build_kb_from_setup(setup_cfg: Dict[str, Any]) -> List[KnowledgeBase]:\n",
        "    \"\"\"\n",
        "    Construye una lista de KnowledgeBase a partir de la sección `knowledge_bases`\n",
        "    de setup.yaml ya cargado en un dict.\n",
        "\n",
        "    Forma esperada (tolerante):\n",
        "\n",
        "    knowledge_bases:\n",
        "      - name: PARAM_TABLE\n",
        "        kind: table\n",
        "        config:\n",
        "          path: /content/parametrias.csv\n",
        "          role: param_table\n",
        "\n",
        "      - name: RULES_TABLE\n",
        "        kind: table\n",
        "        config:\n",
        "          path: /content/reglas_calidad.csv\n",
        "          role: rules_table\n",
        "\n",
        "      - name: FAOSTAT_SQLITE\n",
        "        kind: sqlite\n",
        "        config:\n",
        "          db_path: /content/faostat_world.db\n",
        "\n",
        "    También acepta formato dict:\n",
        "\n",
        "    knowledge_bases:\n",
        "      PARAM_TABLE:\n",
        "        kind: table\n",
        "        config:\n",
        "          path: /content/parametrias.csv\n",
        "    \"\"\"\n",
        "    kb_section = setup_cfg.get(\"knowledge_bases\") or []\n",
        "    kb_list: List[KnowledgeBase] = []\n",
        "\n",
        "    # Normalizamos a lista de dicts\n",
        "    if isinstance(kb_section, dict):\n",
        "        items: List[Dict[str, Any]] = []\n",
        "        for name, cfg in kb_section.items():\n",
        "            if not isinstance(cfg, dict):\n",
        "                continue\n",
        "            item = dict(cfg)\n",
        "            item.setdefault(\"name\", name)\n",
        "            items.append(item)\n",
        "    elif isinstance(kb_section, list):\n",
        "        items = [x for x in kb_section if isinstance(x, dict)]\n",
        "    else:\n",
        "        items = []\n",
        "\n",
        "    for item in items:\n",
        "        name = item.get(\"name\")\n",
        "        if not name:\n",
        "            continue\n",
        "        kind = item.get(\"kind\", \"generic\")\n",
        "        config = item.get(\"config\") or {}\n",
        "        if not isinstance(config, dict):\n",
        "            config = {}\n",
        "        kb_list.append(\n",
        "            KnowledgeBase(\n",
        "                name=str(name),\n",
        "                kind=str(kind),\n",
        "                config=config,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return kb_list\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Contexto por defecto\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def get_default_context(\n",
        "    setup_cfg: Optional[Dict[str, Any]] = None,\n",
        ") -> List[KnowledgeBase]:\n",
        "    \"\"\"\n",
        "    Devuelve la lista de KBs disponibles por defecto.\n",
        "\n",
        "    Modos de uso:\n",
        "\n",
        "      # 1) Sin setup.yaml (modo antiguo / hardcoded)\n",
        "      kbs = get_default_context()\n",
        "\n",
        "      # 2) Pasando el dict de setup.yaml ya cargado\n",
        "      from pathlib import Path\n",
        "      import yaml\n",
        "\n",
        "      with Path(\"setup.yaml\").open(\"r\", encoding=\"utf-8\") as f:\n",
        "          cfg = yaml.safe_load(f) or {}\n",
        "\n",
        "      kbs = get_default_context(cfg)\n",
        "\n",
        "    Si `setup_cfg` se pasa y contiene `knowledge_bases`,\n",
        "    se usa `build_kb_from_setup(setup_cfg)`.\n",
        "\n",
        "    Si no, se devuelve una lista vacía (o podrías hardcodear aquí KBs\n",
        "    globales tipo FAOSTAT_SQLITE/FAOSTAT_VECTOR).\n",
        "    \"\"\"\n",
        "    if setup_cfg:\n",
        "        kbs = build_kb_from_setup(setup_cfg)\n",
        "        if kbs:\n",
        "            return kbs\n",
        "\n",
        "    # Ejemplo de hardcode si quisieras algo global:\n",
        "    # return [\n",
        "    #     KnowledgeBase(\n",
        "    #         name=\"FAOSTAT_SQLITE\",\n",
        "    #         kind=\"sqlite\",\n",
        "    #         config={\"db_path\": \"/content/faostat_world.db\"},\n",
        "    #     ),\n",
        "    #     KnowledgeBase(\n",
        "    #         name=\"FAOSTAT_VECTOR\",\n",
        "    #         kind=\"sqlite-vec\",\n",
        "    #         config={\n",
        "    #             \"db_path\": \"/content/faostat_world.db\",\n",
        "    #             \"table\": \"faostat_vec\",\n",
        "    #         },\n",
        "    #     ),\n",
        "    # ]\n",
        "    return []\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Filtro por nombres\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def get_kb_by_names(\n",
        "    kb_names: List[str],\n",
        "    all_kb: Optional[List[KnowledgeBase]] = None,\n",
        ") -> List[KnowledgeBase]:\n",
        "    \"\"\"\n",
        "    Devuelve las KnowledgeBase cuyo name esté en kb_names.\n",
        "\n",
        "    - kb_names: nombres solicitados (p.ej. AgentInput.kb_names).\n",
        "    - all_kb:  lista total de KBs disponibles; si es None, usamos get_default_context().\n",
        "\n",
        "    Si kb_names está vacío, se devuelve all_kb completa.\n",
        "    \"\"\"\n",
        "    kb_list = all_kb if all_kb is not None else get_default_context()\n",
        "    if not kb_names:\n",
        "        return kb_list\n",
        "\n",
        "    name_set = set(kb_names)\n",
        "    return [kb for kb in kb_list if kb.name in name_set]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mjYnSF39AuJd",
        "outputId": "da8022ba-e52f-4cfd-e81e-a0bda283bcc5"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/logic.py\n",
        "%%writefile agnostic_agent/logic.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Lógica principal (grafo LangGraph) del Agnostic Deep Agent.\n",
        "\n",
        "Sub-grafos actuales:\n",
        "- ANALYZER  → descompone el prompt (rule-based sencillo por ahora).\n",
        "- PLANNER   → usa Planner LLM (Qwen3+vLLM) para generar tool_calls.\n",
        "- EXECUTOR  → ejecuta tools reales (LangChain tools).\n",
        "- CATCHER   → normaliza las salidas de tools a una lista de runs.\n",
        "- SUMMARIZER→ construye:\n",
        "    - respuesta final en modo usuario (user_answer),\n",
        "    - resumen técnico del pipeline (para vistas deep/dev).\n",
        "- VALIDATOR → revisa si la respuesta parece cubrir todo lo pedido.\n",
        "\n",
        "Notas:\n",
        "- Este módulo sigue usando TypedDict; todavía no está cableado\n",
        "  a los modelos Pydantic de `schemas.py`.\n",
        "- Ya integra memoria y kb_names en el planner, y deja\n",
        "  dev_out / deep_out / user_out en el estado.\n",
        "- Está pensado para casos donde el agente cruza:\n",
        "    * una tabla de atributos (input A, p.ej. filas de contratos),\n",
        "    * con tablas de contexto (input B, p.ej. parametrías y\n",
        "      diccionarios de abreviaturas/definiciones),\n",
        "    * y, opcionalmente, documentos (OCR de contratos) vía tools\n",
        "      como semantic_search_in_csv + rerank_qwen3.\n",
        "\"\"\"\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Optional, Callable, Tuple\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from typing_extensions import TypedDict, Annotated\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph.message import add_messages\n",
        "from langchain_core.messages import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    ToolMessage,\n",
        "    AnyMessage,\n",
        "    SystemMessage,\n",
        ")\n",
        "\n",
        "from .capabilities import PlannerConfig, build_planner_system_message\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Tipos de alto nivel para el \"program state\"\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "class AnalyzerResult(TypedDict, total=False):\n",
        "    input_payload: Dict[str, Any]\n",
        "    propositional_logic: str\n",
        "    subqueries: List[str]\n",
        "    subqueries_logic: List[str]\n",
        "\n",
        "\n",
        "class PlannerTrajectory(TypedDict, total=False):\n",
        "    subquery: str\n",
        "    description: str\n",
        "\n",
        "\n",
        "class ExecutorStep(TypedDict, total=False):\n",
        "    tool_call_id: str\n",
        "    tool_name: str\n",
        "    args: Dict[str, Any]\n",
        "\n",
        "\n",
        "class SummaryDict(TypedDict, total=False):\n",
        "    analyzer: str\n",
        "    planner: str\n",
        "    executor: str\n",
        "    catcher: str\n",
        "    summarizer: str\n",
        "    final_answer: str\n",
        "\n",
        "\n",
        "class ValidatorResult(TypedDict, total=False):\n",
        "    all_covered: bool\n",
        "    reasoning: str\n",
        "\n",
        "\n",
        "class State(TypedDict, total=False):\n",
        "    \"\"\"\n",
        "    Estado del grafo (versión 0.2):\n",
        "\n",
        "    - messages: historial de LangChain Messages.\n",
        "    - analyzer: resultado ligero del ANALYZER rule-based.\n",
        "    - planner_trajs: trazas de planificación del PLANNER.\n",
        "    - executor_steps: pasos efectivamente ejecutados (EXECUTOR).\n",
        "    - tool_runs: lista de runs normalizados (CATCHER).\n",
        "    - summary / pipeline_summary: SummaryDict de todo el pipeline.\n",
        "    - validator: ValidatorResult simple (cobertura / razonamiento).\n",
        "    - user_prompt / session_id / kb_names / memory_context:\n",
        "        metadatos que llegan desde Agent (o el llamador).\n",
        "    - dev_out / deep_out / user_out:\n",
        "        vistas finales que el Agent puede usar directamente.\n",
        "    - llm_raw_out / llm_clean_out:\n",
        "        invariantes para salida directa del modelo (sin tools),\n",
        "        donde llm_clean_out = llm_raw_out sin <think>...</think>.\n",
        "    \"\"\"\n",
        "    messages: Annotated[List[AnyMessage], add_messages]\n",
        "    analyzer: Optional[AnalyzerResult]\n",
        "    planner_trajs: List[PlannerTrajectory]\n",
        "    executor_steps: List[ExecutorStep]\n",
        "    tool_runs: List[Dict[str, Any]]\n",
        "    summary: Optional[SummaryDict]\n",
        "    pipeline_summary: Optional[SummaryDict]\n",
        "    validator: Optional[ValidatorResult]\n",
        "\n",
        "    # Metadatos / contexto\n",
        "    user_prompt: Optional[str]\n",
        "    session_id: Optional[str]\n",
        "    kb_names: List[str]\n",
        "    memory_context: Optional[Dict[str, Any]]\n",
        "\n",
        "    # Vistas finales (pueden ser rellenadas por SUMMARIZER)\n",
        "    dev_out: Optional[str]\n",
        "    deep_out: Optional[str]\n",
        "    user_out: Optional[str]\n",
        "\n",
        "    # Invariantes de salida (para modo sin tools)\n",
        "    llm_raw_out: Optional[str]\n",
        "    llm_clean_out: Optional[str]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Planner runtime helpers (tool_calls)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _coerce_content_str(content: Any) -> str:\n",
        "    if isinstance(content, str):\n",
        "        return content\n",
        "    if isinstance(content, list):\n",
        "        parts = []\n",
        "        for p in content:\n",
        "            if isinstance(p, dict):\n",
        "                parts.append(p.get(\"text\", \"\") or p.get(\"content\", \"\") or \"\")\n",
        "            else:\n",
        "                parts.append(str(p))\n",
        "        return \"\".join(parts)\n",
        "    return \"\" if content is None else str(content)\n",
        "\n",
        "\n",
        "def _parse_args_maybe_json(x: Any) -> dict:\n",
        "    if isinstance(x, dict):\n",
        "        return x\n",
        "    if isinstance(x, str):\n",
        "        try:\n",
        "            obj = json.loads(x)\n",
        "            return obj if isinstance(obj, dict) else {}\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "\n",
        "def _normalize_toolcalls_list(raw_calls: Any) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Normaliza múltiples formatos a:\n",
        "      [{\"id\": ..., \"name\": ..., \"args\": {...}}, ...]\n",
        "    \"\"\"\n",
        "    norm: List[Dict[str, Any]] = []\n",
        "    if not raw_calls:\n",
        "        return norm\n",
        "\n",
        "    # ✅ robustez: a veces viene dict o un objeto suelto\n",
        "    if isinstance(raw_calls, dict):\n",
        "        raw_calls = [raw_calls]\n",
        "    elif not isinstance(raw_calls, list):\n",
        "        raw_calls = [raw_calls]\n",
        "\n",
        "    for c in raw_calls:\n",
        "        if isinstance(c, dict):\n",
        "            fn = c.get(\"function\") or {}\n",
        "            name = c.get(\"name\") or fn.get(\"name\") or c.get(\"tool_name\")\n",
        "            if \"args\" in c:\n",
        "                args_raw = c.get(\"args\")\n",
        "            else:\n",
        "                args_raw = fn.get(\"arguments\") or c.get(\"arguments\") or c.get(\"parameters\")\n",
        "            id_ = c.get(\"id\") or c.get(\"tool_call_id\")\n",
        "        else:\n",
        "            fn = getattr(c, \"function\", None)\n",
        "            name = (\n",
        "                getattr(c, \"name\", None)\n",
        "                or (getattr(fn, \"name\", None) if fn else None)\n",
        "                or getattr(c, \"tool_name\", None)\n",
        "            )\n",
        "            args_raw = (\n",
        "                getattr(c, \"args\", None)\n",
        "                or (getattr(fn, \"arguments\", None) if fn else None)\n",
        "                or getattr(c, \"arguments\", None)\n",
        "                or getattr(c, \"parameters\", None)\n",
        "            )\n",
        "            id_ = getattr(c, \"id\", None) or getattr(c, \"tool_call_id\", None)\n",
        "\n",
        "        args = _parse_args_maybe_json(args_raw)\n",
        "        if name:\n",
        "            norm.append(\n",
        "                {\n",
        "                    \"id\": id_ or f\"call_{uuid.uuid4().hex}\",\n",
        "                    \"name\": name,\n",
        "                    \"args\": args,\n",
        "                }\n",
        "            )\n",
        "    return norm\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# ✅ XML fallback robusto (Qwen XML)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _scan_balanced_json(s: str, i: int) -> Tuple[Optional[str], int]:\n",
        "    \"\"\"\n",
        "    Escanea desde s[i] (debe ser '{') y devuelve (json_str, next_index)\n",
        "    contando llaves y respetando strings/escapes.\n",
        "    \"\"\"\n",
        "    if i < 0 or i >= len(s) or s[i] != \"{\":\n",
        "        return None, i\n",
        "\n",
        "    depth = 0\n",
        "    in_str = False\n",
        "    esc = False\n",
        "    start = i\n",
        "\n",
        "    while i < len(s):\n",
        "        c = s[i]\n",
        "        if in_str:\n",
        "            if esc:\n",
        "                esc = False\n",
        "            elif c == \"\\\\\":\n",
        "                esc = True\n",
        "            elif c == '\"':\n",
        "                in_str = False\n",
        "        else:\n",
        "            if c == '\"':\n",
        "                in_str = True\n",
        "            elif c == \"{\":\n",
        "                depth += 1\n",
        "            elif c == \"}\":\n",
        "                depth -= 1\n",
        "                if depth == 0:\n",
        "                    return s[start : i + 1], i + 1\n",
        "        i += 1\n",
        "\n",
        "    return None, i\n",
        "\n",
        "\n",
        "def _extract_tool_calls_via_etree(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Extrae <tool_call>...</tool_call> como dicts (JSON dentro) usando XML real.\n",
        "    \"\"\"\n",
        "    wrapped = f\"<root>{text}</root>\"\n",
        "    try:\n",
        "        root = ET.fromstring(wrapped)\n",
        "    except ET.ParseError:\n",
        "        return []\n",
        "\n",
        "    out: List[Dict[str, Any]] = []\n",
        "    for node in root.findall(\".//tool_call\"):\n",
        "        raw = \"\".join(node.itertext()).strip()\n",
        "        if not raw:\n",
        "            continue\n",
        "\n",
        "        # JSON directo\n",
        "        try:\n",
        "            obj = json.loads(raw)\n",
        "            if isinstance(obj, dict):\n",
        "                out.append(obj)\n",
        "            elif isinstance(obj, list):\n",
        "                out.extend([it for it in obj if isinstance(it, dict)])\n",
        "            continue\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        # brace-scan dentro del texto del tag\n",
        "        j = raw.find(\"{\")\n",
        "        if j != -1:\n",
        "            js, _ = _scan_balanced_json(raw, j)\n",
        "            if js:\n",
        "                try:\n",
        "                    obj2 = json.loads(js)\n",
        "                    if isinstance(obj2, dict):\n",
        "                        out.append(obj2)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _extract_tool_calls_via_xmlish_bracescan(text: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Cuando el XML viene malformado, buscamos bloques <tool_call>...</tool_call>\n",
        "    y adentro hacemos JSON parse o brace-scan.\n",
        "    \"\"\"\n",
        "    out: List[Dict[str, Any]] = []\n",
        "    tag_open = \"<tool_call>\"\n",
        "    tag_close = \"</tool_call>\"\n",
        "\n",
        "    pos = 0\n",
        "    while True:\n",
        "        a = text.find(tag_open, pos)\n",
        "        if a == -1:\n",
        "            break\n",
        "        b = text.find(tag_close, a)\n",
        "        if b == -1:\n",
        "            break\n",
        "\n",
        "        chunk = text[a + len(tag_open) : b].strip()\n",
        "\n",
        "        # JSON directo\n",
        "        try:\n",
        "            obj = json.loads(chunk)\n",
        "            if isinstance(obj, dict):\n",
        "                out.append(obj)\n",
        "            elif isinstance(obj, list):\n",
        "                out.extend([it for it in obj if isinstance(it, dict)])\n",
        "        except Exception:\n",
        "            # brace-scan\n",
        "            j = chunk.find(\"{\")\n",
        "            if j != -1:\n",
        "                js, _ = _scan_balanced_json(chunk, j)\n",
        "                if js:\n",
        "                    try:\n",
        "                        obj2 = json.loads(js)\n",
        "                        if isinstance(obj2, dict):\n",
        "                            out.append(obj2)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "        pos = b + len(tag_close)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def _extract_qwen_xml_calls(ai_msg: AIMessage) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Fallback robusto para Qwen3-XML:\n",
        "      1) intenta XML real (ElementTree) con wrapper <root>\n",
        "      2) si falla (XML roto), usa búsqueda xml-ish + brace-scan\n",
        "    Luego normaliza a {\"id\",\"name\",\"args\"} (misma forma que el resto).\n",
        "    \"\"\"\n",
        "    text = _coerce_content_str(getattr(ai_msg, \"content\", \"\"))\n",
        "    if \"<tool_call\" not in text:\n",
        "        return []\n",
        "\n",
        "    parsed = _extract_tool_calls_via_etree(text)\n",
        "    if not parsed:\n",
        "        parsed = _extract_tool_calls_via_xmlish_bracescan(text)\n",
        "\n",
        "    calls: List[Dict[str, Any]] = []\n",
        "    for obj in parsed:\n",
        "        if not isinstance(obj, dict):\n",
        "            continue\n",
        "        name = obj.get(\"name\") or obj.get(\"tool_name\")\n",
        "        args_raw = obj.get(\"arguments\") or obj.get(\"args\") or obj.get(\"parameters\") or {}\n",
        "        args = _parse_args_maybe_json(args_raw)\n",
        "        if name:\n",
        "            calls.append(\n",
        "                {\n",
        "                    \"id\": f\"call_{uuid.uuid4().hex}\",\n",
        "                    \"name\": name,\n",
        "                    \"args\": args,\n",
        "                }\n",
        "            )\n",
        "    return calls\n",
        "\n",
        "\n",
        "def extract_tool_calls(ai_msg: AIMessage) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    API robusta para obtener tool_calls de un AIMessage.\n",
        "    Compatible con:\n",
        "    - tool_calls nativos (OpenAI / qwen)\n",
        "    - additional_kwargs[\"tool_calls\"]\n",
        "    - XML qwen3-XML (<tool_call>{...}</tool_call>)\n",
        "    \"\"\"\n",
        "    if not isinstance(ai_msg, AIMessage):\n",
        "        return []\n",
        "\n",
        "    tc = getattr(ai_msg, \"tool_calls\", None)\n",
        "    norm = _normalize_toolcalls_list(tc)\n",
        "    if norm:\n",
        "        return norm\n",
        "\n",
        "    addkw = getattr(ai_msg, \"additional_kwargs\", {}) or {}\n",
        "    tc2 = addkw.get(\"tool_calls\")\n",
        "    norm2 = _normalize_toolcalls_list(tc2)\n",
        "    if norm2:\n",
        "        return norm2\n",
        "\n",
        "    return _extract_qwen_xml_calls(ai_msg)\n",
        "\n",
        "\n",
        "def call_planner_with_retry(\n",
        "    planner_llm,\n",
        "    system_message: SystemMessage,\n",
        "    user_or_history_messages: List[AnyMessage],\n",
        "    planner_config: PlannerConfig,\n",
        "    extra_system_messages: Optional[List[SystemMessage]] = None,\n",
        ") -> AIMessage:\n",
        "    \"\"\"\n",
        "    Llama al planner_llm con un SystemMessage fijo + historial (+ contextos extra).\n",
        "    Si no produce tool_calls, reintenta hasta max_retries veces.\n",
        "    \"\"\"\n",
        "    last_ai: AIMessage | None = None\n",
        "    extra = extra_system_messages or []\n",
        "    for _ in range(planner_config.max_retries + 1):\n",
        "        msgs = [system_message] + extra + list(user_or_history_messages)\n",
        "        ai_msg: AIMessage = planner_llm.invoke(msgs)\n",
        "        last_ai = ai_msg\n",
        "        if extract_tool_calls(ai_msg):\n",
        "            break\n",
        "    return last_ai  # type: ignore[return-value]\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Helpers JSON para serializar salidas de tools\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _json_default(obj: Any) -> Any:\n",
        "    \"\"\"\n",
        "    Fallback para tipos no JSON-serializables (np.int64, sets, etc.).\n",
        "    Mantiene estructura lo mejor posible en lugar de castear todo a str.\n",
        "    \"\"\"\n",
        "    # Numpy genéricos → .item()\n",
        "    try:\n",
        "        import numpy as _np  # import local para no romper si no hay numpy\n",
        "        if isinstance(obj, _np.generic):\n",
        "            return obj.item()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # Sets → lista\n",
        "    if isinstance(obj, (set, frozenset)):\n",
        "        return list(obj)\n",
        "\n",
        "    # Último recurso\n",
        "    return str(obj)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# 1) Utilidades: strip_think() + “último assistant real”\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "_THINK_RE = re.compile(r\"<think>.*?</think>\\s*\", flags=re.S | re.I)\n",
        "\n",
        "def strip_think(txt: str) -> str:\n",
        "    \"\"\"Elimina <think>...</think> si existe (Qwen / Hermes), y recorta.\"\"\"\n",
        "    if not isinstance(txt, str):\n",
        "        return \"\"\n",
        "    return _THINK_RE.sub(\"\", txt).strip()\n",
        "\n",
        "def _is_pipeline_internal_ai(m: AnyMessage) -> bool:\n",
        "    \"\"\"\n",
        "    Detecta mensajes internos del pipeline (summarizer/validator),\n",
        "    para NO confundirlos con la respuesta real del LLM.\n",
        "    \"\"\"\n",
        "    if not isinstance(m, AIMessage):\n",
        "        return False\n",
        "\n",
        "    addkw = getattr(m, \"additional_kwargs\", {}) or {}\n",
        "    if addkw.get(\"pipeline_internal\") is True:\n",
        "        return True\n",
        "\n",
        "    # Heurística por contenido (fallback defensivo)\n",
        "    txt = _coerce_content_str(getattr(m, \"content\", \"\")).lstrip()\n",
        "    if txt.startswith(\"## Resumen del pipeline\"):\n",
        "        return True\n",
        "    if txt.startswith(\"## Resumen deep del pipeline\"):\n",
        "        return True\n",
        "    if txt.startswith(\"### VALIDATOR\"):\n",
        "        return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def find_last_assistant_real(messages: List[AnyMessage]) -> Optional[AIMessage]:\n",
        "    \"\"\"\n",
        "    Devuelve el último AIMessage \"real\" (del LLM), ignorando mensajes internos del pipeline.\n",
        "    \"\"\"\n",
        "    for m in reversed(messages or []):\n",
        "        if isinstance(m, AIMessage) and not _is_pipeline_internal_ai(m):\n",
        "            txt = _coerce_content_str(getattr(m, \"content\", \"\")).strip()\n",
        "            if txt:\n",
        "                return m\n",
        "    return None\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Summarizer helpers\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _fmt_args(args: dict) -> str:\n",
        "    if not args:\n",
        "        return \"\"\n",
        "    return \", \".join(f\"{k}={repr(v)}\" for k, v in args.items())\n",
        "\n",
        "\n",
        "def _fmt_output(tool_name: str, v: Any) -> str:\n",
        "    if isinstance(v, bool):\n",
        "        return \"Sí\" if v else \"No\"\n",
        "\n",
        "    # Embeddings → preview compacto\n",
        "    if tool_name == \"embed_texts\":\n",
        "        try:\n",
        "            preview = []\n",
        "            if isinstance(v, list):\n",
        "                for idx, vec in enumerate(v):\n",
        "                    if isinstance(vec, list):\n",
        "                        preview.append(\n",
        "                            {\n",
        "                                \"index\": idx,\n",
        "                                \"dim\": len(vec),\n",
        "                                \"head_5\": vec[:5],\n",
        "                            }\n",
        "                        )\n",
        "            return json.dumps(preview, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(v)\n",
        "\n",
        "    # Reranker → lista JSON bonita\n",
        "    if tool_name == \"rerank_qwen3\":\n",
        "        try:\n",
        "            return json.dumps(v, ensure_ascii=False, indent=2)\n",
        "        except Exception:\n",
        "            return str(v)\n",
        "\n",
        "    # Tablas / dicts grandes → JSON bonito\n",
        "    if isinstance(v, (dict, list)):\n",
        "        try:\n",
        "            return json.dumps(v, ensure_ascii=False, indent=2, default=_json_default)\n",
        "        except Exception:\n",
        "            return str(v)\n",
        "\n",
        "    return str(v)\n",
        "\n",
        "\n",
        "def summarize_tool_runs(user_text: str, runs: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"\n",
        "    Resumen user-friendly basado SOLO en las salidas de herramientas.\n",
        "    Esto alimenta `summary.summarizer` y la sección dev \"SUMMARIZER (basado en herramientas)\".\n",
        "    \"\"\"\n",
        "    if not runs:\n",
        "        return (\n",
        "            \"No se invocó ninguna herramienta. \"\n",
        "            \"No puedo responder con garantías a la pregunta sólo con razonamiento interno.\"\n",
        "        )\n",
        "\n",
        "    partes = [\n",
        "        \"📌 **Resumen basado en herramientas (sin alucinaciones)**\",\n",
        "    ]\n",
        "\n",
        "    for r in runs:\n",
        "        arg_str = _fmt_args(r[\"args\"])\n",
        "        out_str = _fmt_output(r[\"name\"], r[\"output\"])\n",
        "\n",
        "        if r[\"name\"] in (\n",
        "            \"embed_texts\",\n",
        "            \"rerank_qwen3\",\n",
        "            \"embed_context_tables\",\n",
        "            \"semantic_search_in_csv\",\n",
        "            \"judge_row_with_context\",\n",
        "        ):\n",
        "            partes.append(\n",
        "                f\"- `{r['name']}({arg_str})`:\\n\\n```json\\n{out_str}\\n```\"\n",
        "            )\n",
        "        else:\n",
        "            partes.append(\n",
        "                f\"- `{r['name']}({arg_str})` → **{out_str}**\"\n",
        "            )\n",
        "\n",
        "    return \"\\n\".join(partes)\n",
        "\n",
        "\n",
        "def build_user_answer(user_text: str, runs: List[Dict[str, Any]]) -> str:\n",
        "    \"\"\"\n",
        "    Construye la respuesta 1:1 en lenguaje natural para el modo USER,\n",
        "    usando EXCLUSIVAMENTE lo que viene en `runs` (tool-first, sin alucinaciones).\n",
        "\n",
        "    Nota: es agnóstico, pero tiene atajos para algunas tools típicas\n",
        "    (to_upper, is_palindrome, word_count, embed_texts, rerank_qwen3,\n",
        "     y las tools de contexto: embed_context_tables,\n",
        "     semantic_search_in_csv, judge_row_with_context).\n",
        "\n",
        "    Si no reconoce nada, devolvemos \"\" y el caller\n",
        "    hará fallback al resumen de herramientas.\n",
        "    \"\"\"\n",
        "    if not runs:\n",
        "        return \"\"\n",
        "\n",
        "    sentences: List[str] = []\n",
        "\n",
        "    # 1) Reranker: priorizamos porque suele ser la \"respuesta clave\"\n",
        "    rr = next((r for r in runs if r.get(\"name\") == \"rerank_qwen3\"), None)\n",
        "    if rr is not None:\n",
        "        out = rr.get(\"output\")\n",
        "        args = rr.get(\"args\", {}) or {}\n",
        "        docs = args.get(\"documents\") or []\n",
        "\n",
        "        if isinstance(out, list) and out:\n",
        "            best = out[0]\n",
        "            idx = best.get(\"index\", 0)\n",
        "            doc = best.get(\"document\")\n",
        "            score = best.get(\"score\")\n",
        "\n",
        "            # Si el documento no viene embebido, lo recuperamos de args.documents\n",
        "            if doc is None and isinstance(docs, list) and isinstance(idx, int) and 0 <= idx < len(docs):\n",
        "                doc = docs[idx]\n",
        "\n",
        "            idx_h = idx + 1 if isinstance(idx, int) else idx\n",
        "\n",
        "            if doc is not None:\n",
        "                if isinstance(score, (int, float)):\n",
        "                    sentences.append(\n",
        "                        f\"El documento más relevante es el #{idx_h}: \\\"{doc}\\\" \"\n",
        "                        f\"(score ≈ {score:.3f}).\"\n",
        "                    )\n",
        "                else:\n",
        "                    sentences.append(\n",
        "                        f\"El documento más relevante es el #{idx_h}: \\\"{doc}\\\".\"\n",
        "                    )\n",
        "\n",
        "    # 2) to_upper\n",
        "    for r in runs:\n",
        "        if r.get(\"name\") == \"to_upper\":\n",
        "            txt = r.get(\"args\", {}).get(\"text\")\n",
        "            out = r.get(\"output\")\n",
        "            if txt is not None and out is not None:\n",
        "                sentences.append(f\"He convertido \\\"{txt}\\\" a mayúsculas: **{out}**.\")\n",
        "\n",
        "    # 3) is_palindrome\n",
        "    for r in runs:\n",
        "        if r.get(\"name\") == \"is_palindrome\":\n",
        "            txt = r.get(\"args\", {}).get(\"text\")\n",
        "            val = r.get(\"output\")\n",
        "            if txt:\n",
        "                if bool(val):\n",
        "                    sentences.append(f\"\\\"{txt}\\\" es un palíndromo.\")\n",
        "                else:\n",
        "                    sentences.append(f\"\\\"{txt}\\\" no es un palíndromo.\")\n",
        "\n",
        "    # 4) word_count\n",
        "    for r in runs:\n",
        "        if r.get(\"name\") == \"word_count\":\n",
        "            txt = r.get(\"args\", {}).get(\"text\")\n",
        "            n = r.get(\"output\")\n",
        "            if txt is not None and n is not None:\n",
        "                sentences.append(f\"El texto \\\"{txt}\\\" tiene {n} palabras.\")\n",
        "\n",
        "    # 5) embed_texts\n",
        "    emb = next((r for r in runs if r.get(\"name\") == \"embed_texts\"), None)\n",
        "    if emb is not None:\n",
        "        vecs = emb.get(\"output\")\n",
        "        n_vecs = len(vecs) if isinstance(vecs, list) else 0\n",
        "\n",
        "        dim: Optional[int] = None\n",
        "        if isinstance(vecs, list) and vecs:\n",
        "            v0 = vecs[0]\n",
        "            if isinstance(v0, list):  # lista cruda de floats\n",
        "                dim = len(v0)\n",
        "            elif isinstance(v0, dict) and \"dim\" in v0:\n",
        "                try:\n",
        "                    dim = int(v0[\"dim\"])\n",
        "                except Exception:\n",
        "                    dim = None\n",
        "\n",
        "        if n_vecs and dim:\n",
        "            sentences.append(\n",
        "                f\"He generado embeddings de dimensión {dim} para {n_vecs} texto(s).\"\n",
        "            )\n",
        "        elif n_vecs:\n",
        "            sentences.append(\n",
        "                f\"He generado embeddings para {n_vecs} texto(s).\"\n",
        "            )\n",
        "\n",
        "    # 6) embed_context_tables (tablas de parametrías / abreviaturas)\n",
        "    for r in runs:\n",
        "        if r.get(\"name\") == \"embed_context_tables\":\n",
        "            args = r.get(\"args\", {}) or {}\n",
        "            tables = args.get(\"table_paths\") or args.get(\"tables\") or []\n",
        "            out = r.get(\"output\")\n",
        "\n",
        "            n_tables = len(tables) if isinstance(tables, list) else 0\n",
        "\n",
        "            dim: Optional[int] = None\n",
        "            if isinstance(out, dict):\n",
        "                dim = out.get(\"embedding_dim\") or out.get(\"dim\")\n",
        "                try:\n",
        "                    if dim is not None:\n",
        "                        dim = int(dim)\n",
        "                except Exception:\n",
        "                    dim = None\n",
        "\n",
        "            if n_tables:\n",
        "                if dim:\n",
        "                    sentences.append(\n",
        "                        f\"He generado embeddings de dimensión {dim} para {n_tables} tabla(s) de contexto \"\n",
        "                        \"(parametrías, diccionarios o definiciones).\"\n",
        "                    )\n",
        "                else:\n",
        "                    sentences.append(\n",
        "                        f\"He generado embeddings para {n_tables} tabla(s) de contexto \"\n",
        "                        \"(parametrías, diccionarios o definiciones).\"\n",
        "                    )\n",
        "\n",
        "    # 7) semantic_search_in_csv (búsqueda semántica en tablas / OCR tabular)\n",
        "    for r in runs:\n",
        "        if r.get(\"name\") == \"semantic_search_in_csv\":\n",
        "            args = r.get(\"args\", {}) or {}\n",
        "            query = args.get(\"query\")\n",
        "            csv_path = args.get(\"csv_path\") or args.get(\"table_path\")\n",
        "            out = r.get(\"output\")\n",
        "\n",
        "            best_row = None\n",
        "            if isinstance(out, list) and out:\n",
        "                best_row = out[0]\n",
        "\n",
        "            if best_row is not None:\n",
        "                score = (\n",
        "                    best_row.get(\"score\")\n",
        "                    if isinstance(best_row, dict)\n",
        "                    else None\n",
        "                )\n",
        "                row_preview = best_row.get(\"row\") if isinstance(best_row, dict) else None\n",
        "                if isinstance(row_preview, dict):\n",
        "                    row_preview = json.dumps(row_preview, ensure_ascii=False)\n",
        "                if query and csv_path:\n",
        "                    if isinstance(score, (int, float)):\n",
        "                        sentences.append(\n",
        "                            f\"En la tabla `{csv_path}` he encontrado una fila muy relevante para la consulta \"\n",
        "                            f\"\\\"{query}\\\" (score ≈ {score:.3f}). Fila ejemplo: {row_preview}.\"\n",
        "                        )\n",
        "                    else:\n",
        "                        sentences.append(\n",
        "                            f\"En la tabla `{csv_path}` he encontrado una fila relevante para la consulta \"\n",
        "                            f\"\\\"{query}\\\". Fila ejemplo: {row_preview}.\"\n",
        "                        )\n",
        "\n",
        "    # 8) judge_row_with_context (juicio de una fila de atributos vs parametrías/diccionarios)\n",
        "    for r in runs:\n",
        "        if r.get(\"name\") == \"judge_row_with_context\":\n",
        "            args = r.get(\"args\", {}) or {}\n",
        "            out = r.get(\"output\") or {}\n",
        "\n",
        "            contract_id = (\n",
        "                out.get(\"contract_id\")\n",
        "                or out.get(\"row_id\")\n",
        "                or out.get(\"id\")\n",
        "                or args.get(\"contract_id\")\n",
        "                or args.get(\"row_id\")\n",
        "                or args.get(\"id\")\n",
        "            )\n",
        "            judgement = (\n",
        "                out.get(\"judgement\")\n",
        "                or out.get(\"verdict\")\n",
        "                or out.get(\"decision\")\n",
        "                or out.get(\"status\")\n",
        "            )\n",
        "            reasons = out.get(\"reasons\") or out.get(\"rule_hits\") or out.get(\"details\")\n",
        "\n",
        "            if isinstance(reasons, list):\n",
        "                reasons_str = \"; \".join(map(str, reasons[:3]))\n",
        "            else:\n",
        "                reasons_str = str(reasons) if reasons is not None else \"\"\n",
        "\n",
        "            if contract_id is not None and judgement is not None:\n",
        "                sent = (\n",
        "                    f\"He evaluado la fila/contrato '{contract_id}' aplicando las tablas de parametrías \"\n",
        "                    f\"y diccionarios de contexto. El juicio es: **{judgement}**.\"\n",
        "                )\n",
        "                if reasons_str:\n",
        "                    sent += f\" Motivos principales: {reasons_str}.\"\n",
        "                sentences.append(sent)\n",
        "            elif judgement is not None:\n",
        "                sent = (\n",
        "                    f\"He evaluado la fila de atributos con las tablas de contexto; \"\n",
        "                    f\"el juicio global es: **{judgement}**.\"\n",
        "                )\n",
        "                if reasons_str:\n",
        "                    sent += f\" Motivos principales: {reasons_str}.\"\n",
        "                sentences.append(sent)\n",
        "\n",
        "    if not sentences:\n",
        "        # Fallback: si por lo que sea no pudimos mapear nada,\n",
        "        # devolvemos cadena vacía y el caller decidirá usar el resumen de tools.\n",
        "        return \"\"\n",
        "\n",
        "    return \" \".join(sentences)\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Pequeños helpers de contexto (memoria / KB)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def _format_memory_context(mem: Any) -> str:\n",
        "    \"\"\"\n",
        "    Serializa el memory_context para pasarlo al planner como SystemMessage.\n",
        "\n",
        "    Pensado para cosas tipo:\n",
        "      - últimas N interacciones relevantes,\n",
        "      - notas de usuario,\n",
        "      - resúmenes de largo plazo.\n",
        "\n",
        "    Mantenerlo breve es trabajo de memory.py; aquí sólo lo volcamos.\n",
        "    \"\"\"\n",
        "    if not mem:\n",
        "        return \"\"\n",
        "    try:\n",
        "        return json.dumps(mem, ensure_ascii=False, indent=2)\n",
        "    except Exception:\n",
        "        return str(mem)\n",
        "\n",
        "\n",
        "def _format_kb_hint(kb_names: List[str]) -> str:\n",
        "    if not kb_names:\n",
        "        return \"\"\n",
        "    return (\n",
        "        \"KBs disponibles para esta sesión:\\n\"\n",
        "        + \"\\n\".join(f\"- {name}\" for name in kb_names)\n",
        "        + \"\\n\\nPuedes decidir llamar a herramientas que lean o crucen estas KBs \"\n",
        "          \"si es necesario (por ejemplo, comparar filas de una tabla con una tabla \"\n",
        "          \"de parámetros / reglas de calidad y emitir una tabla de juicios).\"\n",
        "    )\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Builder del grafo LangGraph\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "def build_graph_agent(\n",
        "    planner_llm,\n",
        "    tools: List[Any],\n",
        "    planner_config: PlannerConfig | None = None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Grafo:\n",
        "\n",
        "        START → ANALYZER → PLANNER\n",
        "                      ├─(tool_calls)→ EXECUTOR → CATCHER → SUMMARIZER → VALIDATOR → END\n",
        "                      └─────────────→ SUMMARIZER → VALIDATOR → END\n",
        "    \"\"\"\n",
        "    cfg = planner_config or PlannerConfig()\n",
        "    system_msg = build_planner_system_message(cfg)\n",
        "\n",
        "    # ANALYZER (rule-based inicial, pero ya guarda payload rico)\n",
        "    def analyzer_node(state: State) -> Dict[str, Any]:\n",
        "        messages = state.get(\"messages\", [])\n",
        "        user_messages = [m for m in messages if isinstance(m, HumanMessage)]\n",
        "        last_user = user_messages[-1] if user_messages else None\n",
        "        user_text = last_user.content if isinstance(last_user, HumanMessage) else \"\"\n",
        "\n",
        "        # Permitimos que el llamador ya haya rellenado user_prompt\n",
        "        user_prompt = state.get(\"user_prompt\") or user_text\n",
        "\n",
        "        # Input payload más rico: aquí podríamos meter referencias a tablas/JSON.\n",
        "        input_payload: Dict[str, Any] = {\n",
        "            \"user_prompt\": user_prompt,\n",
        "        }\n",
        "\n",
        "        # Split sencillo multi-sentencia: puntos y saltos de línea\n",
        "        raw = str(user_prompt).replace(\"\\n\", \" \")\n",
        "        subqueries: List[str] = []\n",
        "        for part in raw.split(\".\"):\n",
        "            part = part.strip()\n",
        "            if part:\n",
        "                subqueries.append(part)\n",
        "        if not subqueries and user_prompt:\n",
        "            subqueries = [user_prompt]\n",
        "\n",
        "        subqueries_logic = [f\"q{i+1}\" for i in range(len(subqueries))]\n",
        "        propositional_logic = \" ∧ \".join(subqueries_logic) if subqueries_logic else \"\"\n",
        "\n",
        "        analyzer: AnalyzerResult = {\n",
        "            \"input_payload\": input_payload,\n",
        "            \"propositional_logic\": propositional_logic,\n",
        "            \"subqueries\": subqueries,\n",
        "            \"subqueries_logic\": subqueries_logic,\n",
        "        }\n",
        "\n",
        "        return {\"analyzer\": analyzer}\n",
        "\n",
        "    # PLANNER (ve memoria y KB names como contextos)\n",
        "    def planner_node(state: State) -> Dict[str, Any]:\n",
        "        msgs: List[AnyMessage] = state[\"messages\"]\n",
        "\n",
        "        # Contextos adicionales\n",
        "        mem_ctx = state.get(\"memory_context\")\n",
        "        kb_names = state.get(\"kb_names\") or []\n",
        "\n",
        "        mem_str = _format_memory_context(mem_ctx)\n",
        "        kb_str = _format_kb_hint(kb_names)\n",
        "\n",
        "        extra_system_messages: List[SystemMessage] = []\n",
        "        if mem_str:\n",
        "            extra_system_messages.append(\n",
        "                SystemMessage(\n",
        "                    content=(\n",
        "                        \"Contexto de memoria para esta sesión \"\n",
        "                        \"(puede contener preferencias, interacciones previas o notas):\\n\"\n",
        "                        f\"{mem_str}\"\n",
        "                    )\n",
        "                )\n",
        "            )\n",
        "        if kb_str:\n",
        "            extra_system_messages.append(SystemMessage(content=kb_str))\n",
        "\n",
        "        ai_msg: AIMessage = call_planner_with_retry(\n",
        "            planner_llm=planner_llm,\n",
        "            system_message=system_msg,\n",
        "            user_or_history_messages=msgs,\n",
        "            planner_config=cfg,\n",
        "            extra_system_messages=extra_system_messages,\n",
        "        )\n",
        "\n",
        "        # 5) Invariante: guardamos raw/clean del LLM (sirve para modo sin tools)\n",
        "        llm_raw_out = _coerce_content_str(getattr(ai_msg, \"content\", \"\"))\n",
        "        llm_clean_out = strip_think(llm_raw_out)\n",
        "\n",
        "        tool_calls = extract_tool_calls(ai_msg)\n",
        "        analyzer = state.get(\"analyzer\") or {}\n",
        "        subqs: List[str] = analyzer.get(\"subqueries\") or []\n",
        "\n",
        "        if not subqs:\n",
        "            user_messages = [m for m in msgs if isinstance(m, HumanMessage)]\n",
        "            last_user = user_messages[-1] if user_messages else None\n",
        "            if isinstance(last_user, HumanMessage):\n",
        "                subqs = [last_user.content]\n",
        "\n",
        "        plan_trajs: List[PlannerTrajectory] = []\n",
        "        if subqs:\n",
        "            desc_lines: List[str] = []\n",
        "            if not tool_calls:\n",
        "                desc_lines.append(\n",
        "                    \"No se planificó ninguna llamada a herramientas; \"\n",
        "                    \"el agente responderá directamente.\"\n",
        "                )\n",
        "            else:\n",
        "                for idx, tc in enumerate(tool_calls, start=1):\n",
        "                    desc_lines.append(\n",
        "                        f\"Paso {idx}: llamar a la herramienta `{tc['name']}` \"\n",
        "                        f\"con args={tc.get('args', {})}.\"\n",
        "                    )\n",
        "            plan_trajs.append(\n",
        "                PlannerTrajectory(\n",
        "                    subquery=subqs[0],\n",
        "                    description=\"\\n\".join(desc_lines),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            \"messages\": [ai_msg],\n",
        "            \"planner_trajs\": plan_trajs,\n",
        "            \"llm_raw_out\": llm_raw_out,\n",
        "            \"llm_clean_out\": llm_clean_out,\n",
        "        }\n",
        "\n",
        "    # EXECUTOR\n",
        "    def executor_node(state: State) -> Dict[str, Any]:\n",
        "        messages = state[\"messages\"]\n",
        "        ai_msgs = [m for m in messages if isinstance(m, AIMessage)]\n",
        "        if not ai_msgs:\n",
        "            return {\"messages\": [], \"executor_steps\": []}\n",
        "\n",
        "        ai_plan = ai_msgs[-1]\n",
        "        tool_calls = extract_tool_calls(ai_plan)\n",
        "        if not tool_calls:\n",
        "            return {\"messages\": [], \"executor_steps\": []}\n",
        "\n",
        "        tool_msgs: List[ToolMessage] = []\n",
        "        exec_steps: List[ExecutorStep] = []\n",
        "\n",
        "        for tc in tool_calls:\n",
        "            name = tc[\"name\"]\n",
        "            args = tc.get(\"args\", {}) or {}\n",
        "\n",
        "            try:\n",
        "                tool_obj = next(t for t in tools if t.name == name)\n",
        "                observation = tool_obj.invoke(args)\n",
        "            except StopIteration:\n",
        "                observation = {\"error\": f\"Tool '{name}' no encontrada.\"}\n",
        "            except Exception as e:\n",
        "                observation = {\"error\": f\"Excepción ejecutando tool '{name}': {e!r}\"}\n",
        "\n",
        "            try:\n",
        "                payload = json.dumps(\n",
        "                    {\"value\": observation},\n",
        "                    ensure_ascii=False,\n",
        "                    default=_json_default,\n",
        "                )\n",
        "            except TypeError:\n",
        "                payload = json.dumps(\n",
        "                    {\"value\": str(observation)},\n",
        "                    ensure_ascii=False,\n",
        "                )\n",
        "\n",
        "            tool_call_id = tc[\"id\"]\n",
        "\n",
        "            tool_msgs.append(\n",
        "                ToolMessage(\n",
        "                    content=payload,\n",
        "                    tool_call_id=tool_call_id,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            exec_steps.append(\n",
        "                ExecutorStep(\n",
        "                    tool_call_id=tool_call_id,\n",
        "                    tool_name=name,\n",
        "                    args=args,\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return {\n",
        "            \"messages\": tool_msgs,\n",
        "            \"executor_steps\": exec_steps,\n",
        "        }\n",
        "\n",
        "    # CATCHER\n",
        "    def catcher_node(state: State) -> Dict[str, Any]:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "        ai_msgs = [m for m in messages if isinstance(m, AIMessage)]\n",
        "        ai_plan = next(\n",
        "            (m for m in reversed(ai_msgs) if extract_tool_calls(m)),\n",
        "            None,\n",
        "        )\n",
        "        tool_calls = extract_tool_calls(ai_plan) if ai_plan else []\n",
        "\n",
        "        tmsgs: List[ToolMessage] = [m for m in messages if isinstance(m, ToolMessage)]\n",
        "\n",
        "        runs: List[Dict[str, Any]] = []\n",
        "        for tc in tool_calls:\n",
        "            tm = next((t for t in tmsgs if t.tool_call_id == tc[\"id\"]), None)\n",
        "            if tm is None:\n",
        "                continue\n",
        "            raw = tm.content\n",
        "            try:\n",
        "                decoded = json.loads(raw)\n",
        "                output = decoded.get(\"value\", decoded)\n",
        "            except Exception:\n",
        "                output = raw\n",
        "            runs.append(\n",
        "                {\n",
        "                    \"id\": tc[\"id\"],\n",
        "                    \"name\": tc[\"name\"],\n",
        "                    \"args\": tc.get(\"args\", {}) or {},\n",
        "                    \"output\": output,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        return {\"tool_runs\": runs}\n",
        "\n",
        "    # SUMMARIZER\n",
        "    def summarizer_node(state: State) -> Dict[str, Any]:\n",
        "        messages = state[\"messages\"]\n",
        "        user_messages = [m for m in messages if isinstance(m, HumanMessage)]\n",
        "        last_user = user_messages[-1] if user_messages else None\n",
        "        user_text = last_user.content if isinstance(last_user, HumanMessage) else \"\"\n",
        "        user_prompt = state.get(\"user_prompt\") or user_text\n",
        "\n",
        "        runs = state.get(\"tool_runs\", []) or []\n",
        "\n",
        "        # 2) Parche SUMMARIZER (regla de oro):\n",
        "        # Si NO hay tools (runs vacío) y el último AI NO tiene tool_calls,\n",
        "        # user_out debe ser la salida directa del LLM (limpia de <think>).\n",
        "        if not runs:\n",
        "            last_ai = find_last_assistant_real(messages)\n",
        "            last_ai_has_tools = bool(extract_tool_calls(last_ai)) if last_ai else False\n",
        "\n",
        "            llm_raw = state.get(\"llm_raw_out\") or (_coerce_content_str(getattr(last_ai, \"content\", \"\")) if last_ai else \"\")\n",
        "            llm_clean = state.get(\"llm_clean_out\") or strip_think(llm_raw)\n",
        "\n",
        "            if last_ai_has_tools:\n",
        "                # Caso raro: se planearon tools pero no hay runs (falló executor/catcher).\n",
        "                user_answer = (\n",
        "                    \"Se planificaron llamadas a herramientas, pero no se obtuvo ninguna salida. \"\n",
        "                    \"Revisa EXECUTOR/CATCHER o el registro de tools.\"\n",
        "                )\n",
        "            else:\n",
        "                user_answer = llm_clean or \"¿Qué te gustaría hacer?\"\n",
        "\n",
        "            tools_summary_text = summarize_tool_runs(user_prompt, runs)\n",
        "\n",
        "            analyzer = state.get(\"analyzer\") or {}\n",
        "            subqs = analyzer.get(\"subqueries\") or []\n",
        "            logic = analyzer.get(\"propositional_logic\") or \"\"\n",
        "            input_payload = analyzer.get(\"input_payload\") or {}\n",
        "\n",
        "            if analyzer:\n",
        "                analyzer_text_lines = [\n",
        "                    f\"Input payload: {input_payload!r}\",\n",
        "                    f\"Lógica proposicional: {logic or '(no construida)'}\",\n",
        "                    f\"Subconsultas ({len(subqs)}):\",\n",
        "                ]\n",
        "                for s in subqs:\n",
        "                    analyzer_text_lines.append(f\"- {s}\")\n",
        "                analyzer_text = \"\\n\".join(analyzer_text_lines)\n",
        "            else:\n",
        "                analyzer_text = \"No se ejecutó ANALYZER o no dejó estado.\"\n",
        "\n",
        "            planner_trajs = state.get(\"planner_trajs\", []) or []\n",
        "            if planner_trajs:\n",
        "                pl_lines: List[str] = []\n",
        "                for i, tr in enumerate(planner_trajs, start=1):\n",
        "                    pl_lines.append(f\"Subquery {i}: {tr.get('subquery', '')}\")\n",
        "                    desc = tr.get(\"description\")\n",
        "                    if desc:\n",
        "                        pl_lines.append(desc)\n",
        "                planner_text = \"\\n\".join(pl_lines)\n",
        "            else:\n",
        "                planner_text = (\n",
        "                    \"No se construyó un plan de herramientas; probablemente se respondió \"\n",
        "                    \"directamente (o no hubo tool_calls).\"\n",
        "                )\n",
        "\n",
        "            executor_steps = state.get(\"executor_steps\", []) or []\n",
        "            if executor_steps:\n",
        "                ex_lines: List[str] = [\n",
        "                    f\"Se ejecutaron {len(executor_steps)} llamadas a herramientas:\"\n",
        "                ]\n",
        "                for step in executor_steps:\n",
        "                    ex_lines.append(\n",
        "                        f\"- tool_call_id={step['tool_call_id']}, \"\n",
        "                        f\"name={step['tool_name']}, args={step['args']!r}\"\n",
        "                    )\n",
        "                executor_text = \"\\n\".join(ex_lines)\n",
        "            else:\n",
        "                executor_text = \"No se ejecutó ninguna herramienta para esta consulta.\"\n",
        "\n",
        "            catcher_text = \"Catcher no encontró resultados de tools (runs vacío).\"\n",
        "\n",
        "            # summarizer_text para DEV/DEEP: en modo sin tools, dejamos constancia\n",
        "            summarizer_text = \"No se invocaron herramientas. Respuesta directa del modelo (passthrough).\"\n",
        "\n",
        "            summary_dict: SummaryDict = SummaryDict(\n",
        "                analyzer=analyzer_text,\n",
        "                planner=planner_text,\n",
        "                executor=executor_text,\n",
        "                catcher=catcher_text,\n",
        "                summarizer=summarizer_text,\n",
        "                final_answer=user_answer,\n",
        "            )\n",
        "\n",
        "            sections = [\n",
        "                \"## Resumen del pipeline\",\n",
        "                \"### ANALYZER\",\n",
        "                analyzer_text,\n",
        "                \"### PLANNER\",\n",
        "                planner_text,\n",
        "                \"### EXECUTOR\",\n",
        "                executor_text,\n",
        "                \"### CATCHER\",\n",
        "                catcher_text,\n",
        "                \"### SUMMARIZER (basado en herramientas)\",\n",
        "                tools_summary_text,\n",
        "                \"### RESPUESTA FINAL (modo usuario)\",\n",
        "                user_answer,\n",
        "            ]\n",
        "            answer_markdown = \"\\n\\n\".join(sections)\n",
        "\n",
        "            final_ai = AIMessage(\n",
        "                content=answer_markdown,\n",
        "                additional_kwargs={\"pipeline_internal\": True, \"node\": \"summarizer\"},\n",
        "            )\n",
        "\n",
        "            dev_out = answer_markdown\n",
        "            deep_out = \"\\n\\n\".join([\n",
        "                \"## Resumen deep del pipeline\",\n",
        "                \"### ANALYZER\",\n",
        "                analyzer_text,\n",
        "                \"### PLANNER\",\n",
        "                planner_text,\n",
        "                \"### EXECUTOR\",\n",
        "                executor_text,\n",
        "                \"### CATCHER\",\n",
        "                catcher_text,\n",
        "                \"### SUMMARIZER\",\n",
        "                summarizer_text,\n",
        "                \"### RESPUESTA FINAL\",\n",
        "                user_answer,\n",
        "            ])\n",
        "            user_out = user_answer\n",
        "\n",
        "            return {\n",
        "                \"messages\": [final_ai],\n",
        "                \"summary\": summary_dict,\n",
        "                \"pipeline_summary\": summary_dict,\n",
        "                \"dev_out\": dev_out,\n",
        "                \"deep_out\": deep_out,\n",
        "                \"user_out\": user_out,\n",
        "            }\n",
        "\n",
        "        tools_summary_text = summarize_tool_runs(user_prompt, runs)\n",
        "\n",
        "        analyzer = state.get(\"analyzer\") or {}\n",
        "        subqs = analyzer.get(\"subqueries\") or []\n",
        "        logic = analyzer.get(\"propositional_logic\") or \"\"\n",
        "        input_payload = analyzer.get(\"input_payload\") or {}\n",
        "\n",
        "        if analyzer:\n",
        "            analyzer_text_lines = [\n",
        "                f\"Input payload: {input_payload!r}\",\n",
        "                f\"Lógica proposicional: {logic or '(no construida)'}\",\n",
        "                f\"Subconsultas ({len(subqs)}):\",\n",
        "            ]\n",
        "            for s in subqs:\n",
        "                analyzer_text_lines.append(f\"- {s}\")\n",
        "            analyzer_text = \"\\n\".join(analyzer_text_lines)\n",
        "        else:\n",
        "            analyzer_text = \"No se ejecutó ANALYZER o no dejó estado.\"\n",
        "\n",
        "        planner_trajs = state.get(\"planner_trajs\", []) or []\n",
        "        if planner_trajs:\n",
        "            pl_lines: List[str] = []\n",
        "            for i, tr in enumerate(planner_trajs, start=1):\n",
        "                pl_lines.append(f\"Subquery {i}: {tr.get('subquery', '')}\")\n",
        "                desc = tr.get(\"description\")\n",
        "                if desc:\n",
        "                    pl_lines.append(desc)\n",
        "            planner_text = \"\\n\".join(pl_lines)\n",
        "        else:\n",
        "            planner_text = (\n",
        "                \"No se construyó un plan de herramientas; probablemente se respondió \"\n",
        "                \"directamente (o no hubo tool_calls).\"\n",
        "            )\n",
        "\n",
        "        executor_steps = state.get(\"executor_steps\", []) or []\n",
        "        if executor_steps:\n",
        "            ex_lines: List[str] = [\n",
        "                f\"Se ejecutaron {len(executor_steps)} llamadas a herramientas:\"\n",
        "            ]\n",
        "            for step in executor_steps:\n",
        "                ex_lines.append(\n",
        "                    f\"- tool_call_id={step['tool_call_id']}, \"\n",
        "                    f\"name={step['tool_name']}, args={step['args']!r}\"\n",
        "                )\n",
        "            executor_text = \"\\n\".join(ex_lines)\n",
        "        else:\n",
        "            executor_text = \"No se ejecutó ninguna herramienta para esta consulta.\"\n",
        "\n",
        "        if runs:\n",
        "            ca_lines: List[str] = [\n",
        "                f\"Catcher recopiló {len(runs)} resultados de tools.\"\n",
        "            ]\n",
        "            for r in runs:\n",
        "                ca_lines.append(\n",
        "                    f\"- {r['name']}({r['args']!r}) → output tipo {type(r['output']).__name__}\"\n",
        "                )\n",
        "            catcher_text = \"\\n\".join(ca_lines)\n",
        "        else:\n",
        "            catcher_text = \"Catcher no encontró resultados de tools (runs vacío).\"\n",
        "\n",
        "        # Resumen tool-based (para DEV/DEEP)\n",
        "        summarizer_text = tools_summary_text\n",
        "\n",
        "        # Respuesta 1:1 en lenguaje natural para USER (tool-first)\n",
        "        user_answer = build_user_answer(user_prompt, runs)\n",
        "        if not user_answer:\n",
        "            # Fallback conservador: si por lo que sea no pudimos mapear nada,\n",
        "            # devolvemos el resumen de herramientas como antes.\n",
        "            user_answer = tools_summary_text\n",
        "\n",
        "        summary_dict: SummaryDict = SummaryDict(\n",
        "            analyzer=analyzer_text,\n",
        "            planner=planner_text,\n",
        "            executor=executor_text,\n",
        "            catcher=catcher_text,\n",
        "            summarizer=summarizer_text,\n",
        "            final_answer=user_answer,\n",
        "        )\n",
        "\n",
        "        # Esta respuesta (answer_markdown) es la vista \"dev\" con todo el pipeline.\n",
        "        sections = [\n",
        "            \"## Resumen del pipeline\",\n",
        "            \"### ANALYZER\",\n",
        "            analyzer_text,\n",
        "            \"### PLANNER\",\n",
        "            planner_text,\n",
        "            \"### EXECUTOR\",\n",
        "            executor_text,\n",
        "            \"### CATCHER\",\n",
        "            catcher_text,\n",
        "            \"### SUMMARIZER (basado en herramientas)\",\n",
        "            summarizer_text,\n",
        "            \"### RESPUESTA FINAL (modo usuario)\",\n",
        "            user_answer,\n",
        "        ]\n",
        "        answer_markdown = \"\\n\\n\".join(sections)\n",
        "\n",
        "        final_ai = AIMessage(\n",
        "            content=answer_markdown,\n",
        "            additional_kwargs={\"pipeline_internal\": True, \"node\": \"summarizer\"},\n",
        "        )\n",
        "\n",
        "        # Además rellenamos dev_out / deep_out / user_out:\n",
        "        dev_out = answer_markdown\n",
        "        deep_out = \"\\n\\n\".join([\n",
        "            \"## Resumen deep del pipeline\",\n",
        "            \"### ANALYZER\",\n",
        "            analyzer_text,\n",
        "            \"### PLANNER\",\n",
        "            planner_text,\n",
        "            \"### EXECUTOR\",\n",
        "            executor_text,\n",
        "            \"### CATCHER\",\n",
        "            catcher_text,\n",
        "            \"### SUMMARIZER\",\n",
        "            summarizer_text,\n",
        "            \"### RESPUESTA FINAL\",\n",
        "            user_answer,\n",
        "        ])\n",
        "        user_out = user_answer\n",
        "\n",
        "        return {\n",
        "            \"messages\": [final_ai],\n",
        "            \"summary\": summary_dict,\n",
        "            \"pipeline_summary\": summary_dict,\n",
        "            \"dev_out\": dev_out,\n",
        "            \"deep_out\": deep_out,\n",
        "            \"user_out\": user_out,\n",
        "        }\n",
        "\n",
        "    # VALIDATOR (heurística simple, preparada para LLM en el futuro)\n",
        "    def validator_node(state: State) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Pequeño validador que mira:\n",
        "          - si hubo tools,\n",
        "          - si el Summarizer dijo \"no se ejecutó ninguna herramienta\",\n",
        "          - si el final_answer está vacío,\n",
        "          - y heurísticas ligeras sobre prompts tabulares/contratos.\n",
        "\n",
        "        Marca all_covered=False en casos sospechosos.\n",
        "        Más adelante se puede reemplazar por un LLM que reciba:\n",
        "          (user_prompt, tool_runs, final_answer) y devuelva ValidatorResult.\n",
        "        \"\"\"\n",
        "        user_prompt = state.get(\"user_prompt\") or \"\"\n",
        "        summary = state.get(\"pipeline_summary\") or state.get(\"summary\") or {}\n",
        "        final_answer = summary.get(\"final_answer\") or \"\"\n",
        "        summarizer_text = summary.get(\"summarizer\") or \"\"\n",
        "        runs = state.get(\"tool_runs\", []) or []\n",
        "\n",
        "        # 3) Guardrail en VALIDATOR: auto-reparación (modo sin tools)\n",
        "        bad_templates = (\n",
        "            \"no se invocó ninguna herramienta\",\n",
        "            \"no puedo responder con garantías\",\n",
        "            \"sin herramientas no puedo\",\n",
        "        )\n",
        "        if runs == [] and any(t in final_answer.strip().lower() for t in bad_templates):\n",
        "            last_ai = find_last_assistant_real(state.get(\"messages\", []) or [])\n",
        "            raw = state.get(\"llm_raw_out\") or (_coerce_content_str(getattr(last_ai, \"content\", \"\")) if last_ai else \"\")\n",
        "            direct = state.get(\"llm_clean_out\") or strip_think(raw)\n",
        "\n",
        "            if direct:\n",
        "                final_answer = direct\n",
        "                try:\n",
        "                    summary[\"final_answer\"] = direct\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                # también reparamos user_out si estaba “apagado”\n",
        "                state[\"user_out\"] = direct\n",
        "\n",
        "        all_covered = True\n",
        "        reasons: List[str] = []\n",
        "\n",
        "        if not final_answer.strip():\n",
        "            all_covered = False\n",
        "            reasons.append(\"La respuesta final está vacía.\")\n",
        "\n",
        "        if \"No se invocó ninguna herramienta\" in summarizer_text and runs:\n",
        "            all_covered = False\n",
        "            reasons.append(\n",
        "                \"Inconsistencia: el SUMMARIZER dice que no hubo tools, \"\n",
        "                \"pero tool_runs no está vacío.\"\n",
        "            )\n",
        "\n",
        "        # Heurística: prompt tabular sin tools\n",
        "        if runs == [] and user_prompt:\n",
        "            if any(tok in user_prompt.lower() for tok in [\"tabla\", \"table\", \"fila\", \"row\", \"columna\", \"column\"]):\n",
        "                all_covered = False\n",
        "                reasons.append(\n",
        "                    \"El usuario menciona estructuras tabulares pero no se invocaron herramientas; \"\n",
        "                    \"puede faltar cálculo determinista sobre tablas/diccionarios.\"\n",
        "                )\n",
        "\n",
        "        # Heurística adicional: caso de contratos sin juicio explícito\n",
        "        if \"contrato\" in user_prompt.lower() or \"contract\" in user_prompt.lower():\n",
        "            has_judge = any(r.get(\"name\") == \"judge_row_with_context\" for r in runs)\n",
        "            if not has_judge:\n",
        "                all_covered = False\n",
        "                reasons.append(\n",
        "                    \"El usuario menciona contratos pero no se detectó ninguna ejecución \"\n",
        "                    \"de `judge_row_with_context`; podría faltar el juicio fila+contexto.\"\n",
        "                )\n",
        "\n",
        "        if not reasons and all_covered:\n",
        "            reasons.append(\"No se detectaron problemas obvios de cobertura.\")\n",
        "\n",
        "        validator: ValidatorResult = {\n",
        "            \"all_covered\": all_covered,\n",
        "            \"reasoning\": \"\\n\".join(reasons),\n",
        "        }\n",
        "\n",
        "        # Mensaje para la traza dev\n",
        "        validator_msg = AIMessage(\n",
        "            content=(\n",
        "                \"### VALIDATOR\\n\\n\"\n",
        "                f\"- all_covered: {all_covered}\\n\"\n",
        "                f\"- reasoning:\\n{validator['reasoning']}\"\n",
        "            ),\n",
        "            additional_kwargs={\"pipeline_internal\": True, \"node\": \"validator\"},\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"validator\": validator,\n",
        "            \"messages\": [validator_msg],\n",
        "            \"pipeline_summary\": summary,\n",
        "            \"summary\": summary,\n",
        "            \"user_out\": final_answer if isinstance(final_answer, str) and final_answer.strip() else state.get(\"user_out\"),\n",
        "        }\n",
        "\n",
        "    # Router\n",
        "    def route_from_planner(state: State) -> str:\n",
        "        messages = state[\"messages\"]\n",
        "        ai_msgs = [m for m in messages if isinstance(m, AIMessage)]\n",
        "        if not ai_msgs:\n",
        "            return \"summarizer\"\n",
        "\n",
        "        last_ai = ai_msgs[-1]\n",
        "        if extract_tool_calls(last_ai):\n",
        "            return \"executor\"\n",
        "        return \"summarizer\"\n",
        "\n",
        "    # Build graph\n",
        "    builder = StateGraph(State)\n",
        "\n",
        "    builder.add_node(\"analyzer\", analyzer_node)\n",
        "    builder.add_node(\"planner\", planner_node)\n",
        "    builder.add_node(\"executor\", executor_node)\n",
        "    builder.add_node(\"catcher\", catcher_node)\n",
        "    builder.add_node(\"summarizer\", summarizer_node)\n",
        "    builder.add_node(\"validator\", validator_node)\n",
        "\n",
        "    builder.add_edge(START, \"analyzer\")\n",
        "    builder.add_edge(\"analyzer\", \"planner\")\n",
        "    builder.add_conditional_edges(\n",
        "        \"planner\",\n",
        "        route_from_planner,\n",
        "        [\"executor\", \"summarizer\"],\n",
        "    )\n",
        "    builder.add_edge(\"executor\", \"catcher\")\n",
        "    builder.add_edge(\"catcher\", \"summarizer\")\n",
        "    builder.add_edge(\"summarizer\", \"validator\")\n",
        "    builder.add_edge(\"validator\", END)\n",
        "\n",
        "    graph_app = builder.compile()\n",
        "    return graph_app\n",
        "\n",
        "\n",
        "# ─────────────────────────────────────────────\n",
        "# Logic loader (registro de grafos)\n",
        "# ─────────────────────────────────────────────\n",
        "\n",
        "@dataclass\n",
        "class LogicConfig:\n",
        "    module: str = \"agnostic_agent.logic\"\n",
        "    builder_fn: str = \"build_graph_agent\"\n",
        "\n",
        "\n",
        "def load_logic(\n",
        "    planner_llm: Any,\n",
        "    tools: List[Any],\n",
        "    planner_config: Optional[PlannerConfig] = None,\n",
        "    logic_config: Optional[LogicConfig] = None,\n",
        ") -> Any:\n",
        "    \"\"\"\n",
        "    Carga y ejecuta la función builder que construye el grafo del agente.\n",
        "\n",
        "    Por defecto usa este mismo módulo:\n",
        "        agnostic_agent.logic.build_graph_agent\n",
        "    \"\"\"\n",
        "    cfg = logic_config or LogicConfig()\n",
        "\n",
        "    if cfg.module == \"agnostic_agent.logic\":\n",
        "        builder: Callable[..., Any] = globals().get(cfg.builder_fn)  # type: ignore[assignment]\n",
        "        if builder is None or not callable(builder):\n",
        "            raise AttributeError(\n",
        "                f\"No se encontró función builder '{cfg.builder_fn}' en agnostic_agent.logic.\"\n",
        "            )\n",
        "        return builder(planner_llm, tools, planner_config)\n",
        "\n",
        "    import importlib\n",
        "\n",
        "    try:\n",
        "        mod = importlib.import_module(cfg.module)\n",
        "    except ModuleNotFoundError as e:\n",
        "        raise ImportError(\n",
        "            f\"No se pudo importar el módulo de lógica '{cfg.module}'.\"\n",
        "        ) from e\n",
        "\n",
        "    builder = getattr(mod, cfg.builder_fn, None)\n",
        "    if builder is None or not callable(builder):\n",
        "        raise AttributeError(\n",
        "            f\"El módulo '{cfg.module}' no tiene una función callable '{cfg.builder_fn}'.\"\n",
        "        )\n",
        "\n",
        "    return builder(planner_llm, tools, planner_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-deTBqf-L0uq",
        "outputId": "a5351291-7066-4935-bcaf-a0d63e62278d"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/agent.py\n",
        "%%writefile agnostic_agent/agent.py\n",
        "from __future__ import annotations\n",
        "\n",
        "from typing import Any, Dict, List, Optional, Union, Tuple\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import yaml\n",
        "from langchain_core.messages import (\n",
        "    HumanMessage,\n",
        "    AIMessage,\n",
        "    ToolMessage,\n",
        "    AnyMessage,\n",
        ")\n",
        "\n",
        "from .capabilities import PlannerConfig, build_planner_llm\n",
        "from .logic import load_logic, State\n",
        "from .communication import (\n",
        "    AgentInput,\n",
        "    AgentOutput,\n",
        "    AgentView,\n",
        "    AgentSummary,\n",
        "    ToolRun,\n",
        ")\n",
        "from .tools import get_default_tools  # ✅ catálogo global de tools\n",
        "from .memory import read_memory, write_memory  # ✅ memoria multi-nivel (in-memory)\n",
        "from .context import (  # ✅ KBs externas/tabulares\n",
        "    KnowledgeBase,\n",
        "    get_default_context,\n",
        "    get_kb_by_names,\n",
        "    build_kb_from_setup,\n",
        ")\n",
        "\n",
        "\n",
        "class Agent:\n",
        "    \"\"\"\n",
        "    Agente agnóstico sobre LangGraph + Qwen3.\n",
        "\n",
        "    Patrones de inicialización:\n",
        "\n",
        "        # 1) Totalmente por defecto (sin setup.yaml)\n",
        "        agent = Agent.init()\n",
        "\n",
        "        # 2) Pasando path a setup.yaml (panel de control middleware)\n",
        "        agent = Agent.init(\"setup.yaml\")\n",
        "        # o bien:\n",
        "        agent = Agent.init(setup_path=\"setup.yaml\")\n",
        "\n",
        "        # 3) Pasando un PlannerConfig explícito (ignora planner de setup.yaml)\n",
        "        agent = Agent.init(PlannerConfig(temperature=0.0))\n",
        "\n",
        "        # 4) Pasando tablas de contexto (parametrías / abreviaturas, etc.)\n",
        "        agent = Agent.init(\n",
        "            \"setup.yaml\",\n",
        "            context_tables=[\n",
        "                \"/content/parametrias.csv\",\n",
        "                \"/content/diccionario_abreviaturas.csv\",\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    - run_turn(...) SIEMPRE devuelve un dict:\n",
        "\n",
        "        {\n",
        "          \"dev_out\":  {...},  # traza completa (pipeline + raw_state)\n",
        "          \"deep_out\": {...},  # resumen por sección (ANALYZER/PLANNER/...).\n",
        "          \"user_out\": {...},  # respuesta 1:1 basada en herramientas.\n",
        "        }\n",
        "\n",
        "    El grafo deja en el estado:\n",
        "      - tool_runs\n",
        "      - summary / pipeline_summary (SummaryDict)\n",
        "      - user_out / deep_out / dev_out (strings opcionales)\n",
        "    y este wrapper los empaqueta en AgentOutput.\n",
        "\n",
        "    Además:\n",
        "      - Resuelve session_id / kb_names a partir de AgentInput.\n",
        "      - Inyecta memory_context (read_memory) en el estado.\n",
        "      - Inyecta knowledge_bases y context_tables (parametrías, abreviaturas, etc.).\n",
        "      - Al final de cada turno actualiza la memoria con write_memory.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        graph_app: Any,\n",
        "        planner_config: PlannerConfig,\n",
        "        tools: List[Any],\n",
        "        *,\n",
        "        setup_path: Optional[str] = None,\n",
        "        setup_config: Optional[Dict[str, Any]] = None,\n",
        "        memory_cfg: Optional[Dict[str, Any]] = None,\n",
        "        knowledge_bases: Optional[List[KnowledgeBase]] = None,\n",
        "        context_tables: Optional[List[str]] = None,\n",
        "        context_cfg: Optional[Dict[str, Any]] = None,\n",
        "    ) -> None:\n",
        "        self.graph_app = graph_app\n",
        "        self.planner_config = planner_config\n",
        "        self.tools = tools\n",
        "\n",
        "        # Panel de control cargado desde setup.yaml (si existe)\n",
        "        self.setup_path: Optional[str] = setup_path\n",
        "        self.setup_config: Dict[str, Any] = setup_config or {}\n",
        "\n",
        "        # Config de memoria (session / short_term / long_term)\n",
        "        self.memory_cfg: Dict[str, Any] = memory_cfg or {}\n",
        "\n",
        "        # KBs registradas (tabulares, vectores, APIs, etc.)\n",
        "        self.knowledge_bases: List[KnowledgeBase] = (\n",
        "            knowledge_bases if knowledge_bases is not None else get_default_context()\n",
        "        )\n",
        "\n",
        "        # Tablas CSV de contexto (parametrías, abreviaturas/definiciones, etc.)\n",
        "        self.context_tables: List[str] = context_tables or []\n",
        "\n",
        "        # Config de contexto crudo desde setup.yaml (opcional)\n",
        "        self.context_cfg: Dict[str, Any] = context_cfg or {}\n",
        "\n",
        "        # Estado de conversación multi-turn (historial reducido)\n",
        "        self._state: Optional[Dict[str, Any]] = None\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Helpers de setup.yaml\n",
        "    # ------------------------------------------------------------------\n",
        "    @staticmethod\n",
        "    def _load_setup_config(\n",
        "        setup_path: Optional[Union[str, Path]],\n",
        "    ) -> Tuple[Optional[Path], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Intenta cargar setup.yaml (o el path que se pase).\n",
        "\n",
        "        Orden de resolución:\n",
        "          1) setup_path explícito (argumento).\n",
        "          2) AGENT_SETUP_PATH en variables de entorno.\n",
        "        \"\"\"\n",
        "        cfg: Dict[str, Any] = {}\n",
        "\n",
        "        path_obj: Optional[Path] = None\n",
        "        if isinstance(setup_path, (str, Path)):\n",
        "            path_obj = Path(setup_path)\n",
        "        else:\n",
        "            env_path = os.getenv(\"AGENT_SETUP_PATH\")\n",
        "            if env_path:\n",
        "                path_obj = Path(env_path)\n",
        "\n",
        "        if path_obj is None:\n",
        "            return None, cfg\n",
        "\n",
        "        if not path_obj.is_file():\n",
        "            # No levantamos excepción para no romper en Colab si no está el archivo\n",
        "            print(f\"[Agent] ⚠️ setup.yaml no encontrado en: {path_obj}. Se usarán defaults.\")\n",
        "            return path_obj, cfg\n",
        "\n",
        "        try:\n",
        "            with path_obj.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                data = yaml.safe_load(f) or {}\n",
        "            if not isinstance(data, dict):\n",
        "                print(f\"[Agent] ⚠️ setup.yaml no tiene formato dict en {path_obj}. Ignorando.\")\n",
        "                return path_obj, {}\n",
        "            cfg = data\n",
        "        except Exception as e:\n",
        "            print(f\"[Agent] ⚠️ Error leyendo setup.yaml ({path_obj}): {e!r}\")\n",
        "            cfg = {}\n",
        "\n",
        "        return path_obj, cfg\n",
        "\n",
        "    @staticmethod\n",
        "    def _apply_model_env_from_setup(setup_cfg: Dict[str, Any]) -> None:\n",
        "        \"\"\"\n",
        "        Opcional: aplica variables de entorno para modelos / endpoints\n",
        "        a partir de setup.yaml, sin pisar lo que ya esté definido.\n",
        "        \"\"\"\n",
        "        models_cfg = setup_cfg.get(\"models\") or {}\n",
        "\n",
        "        # LLM\n",
        "        llm_cfg = models_cfg.get(\"llm\") or {}\n",
        "        llm_api_base = llm_cfg.get(\"api_base\")\n",
        "        llm_served_name = llm_cfg.get(\"served_name\")\n",
        "\n",
        "        if llm_api_base and \"VLLM_API_BASE\" not in os.environ:\n",
        "            os.environ[\"VLLM_API_BASE\"] = str(llm_api_base)\n",
        "        if llm_api_base and \"VLLM_LLM_API_BASE\" not in os.environ:\n",
        "            os.environ[\"VLLM_LLM_API_BASE\"] = str(llm_api_base)\n",
        "        if llm_served_name and \"LLM_SERVED_NAME\" not in os.environ:\n",
        "            os.environ[\"LLM_SERVED_NAME\"] = str(llm_served_name)\n",
        "\n",
        "        # Embeddings\n",
        "        emb_cfg = models_cfg.get(\"emb\") or {}\n",
        "        emb_api_base = emb_cfg.get(\"api_base\")\n",
        "        emb_served_name = emb_cfg.get(\"served_name\")\n",
        "\n",
        "        if emb_api_base and \"VLLM_EMB_API_BASE\" not in os.environ:\n",
        "            os.environ[\"VLLM_EMB_API_BASE\"] = str(emb_api_base)\n",
        "        if emb_served_name and \"EMB_SERVED_NAME\" not in os.environ:\n",
        "            os.environ[\"EMB_SERVED_NAME\"] = str(emb_served_name)\n",
        "\n",
        "        # Reranker\n",
        "        rerank_cfg = models_cfg.get(\"rerank\") or {}\n",
        "        rerank_api_base = rerank_cfg.get(\"api_base\")\n",
        "        rerank_served_name = rerank_cfg.get(\"served_name\")\n",
        "\n",
        "        if rerank_api_base and \"VLLM_RERANK_API_BASE\" not in os.environ:\n",
        "            os.environ[\"VLLM_RERANK_API_BASE\"] = str(rerank_api_base)\n",
        "        if rerank_served_name and \"RERANK_SERVED_NAME\" not in os.environ:\n",
        "            os.environ[\"RERANK_SERVED_NAME\"] = str(rerank_served_name)\n",
        "\n",
        "        # Clave dummy para OpenAI-compatible (vLLM la ignora pero la requiere)\n",
        "        if \"OPENAI_API_KEY\" not in os.environ:\n",
        "            os.environ[\"OPENAI_API_KEY\"] = \"EMPTY\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_kb_from_setup(setup_cfg: Dict[str, Any]) -> List[KnowledgeBase]:\n",
        "        \"\"\"\n",
        "        Construye la lista de KnowledgeBase a partir de setup.yaml usando\n",
        "        el helper genérico de context.build_kb_from_setup.\n",
        "\n",
        "        Si no hay nada en el YAML, cae en get_default_context().\n",
        "        \"\"\"\n",
        "        kb_list = build_kb_from_setup(setup_cfg)\n",
        "        if not kb_list:\n",
        "            kb_list = get_default_context()\n",
        "        return kb_list\n",
        "\n",
        "    @staticmethod\n",
        "    def _resolve_context_tables(\n",
        "        setup_cfg: Dict[str, Any],\n",
        "        explicit_context_tables: Optional[List[str]],\n",
        "    ) -> Tuple[List[str], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Resuelve tablas de contexto (CSV) y config de contexto.\n",
        "\n",
        "        Prioridad:\n",
        "          1) context_tables explícito pasado a Agent.init(...)\n",
        "          2) setup.yaml:\n",
        "               context:\n",
        "                 tables: [...]\n",
        "               # o bien, compat:\n",
        "               context_tables: [...]\n",
        "        \"\"\"\n",
        "        context_cfg: Dict[str, Any] = setup_cfg.get(\"context\") or {}\n",
        "\n",
        "        yaml_tables = context_cfg.get(\"tables\") or setup_cfg.get(\"context_tables\") or []\n",
        "        if isinstance(yaml_tables, str):\n",
        "            yaml_tables = [yaml_tables]\n",
        "        yaml_tables = [str(p) for p in yaml_tables] if isinstance(yaml_tables, list) else []\n",
        "\n",
        "        if explicit_context_tables is not None:\n",
        "            final_tables = list(explicit_context_tables)\n",
        "        else:\n",
        "            final_tables = yaml_tables\n",
        "\n",
        "        return final_tables, context_cfg\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Factory\n",
        "    # ------------------------------------------------------------------\n",
        "    @classmethod\n",
        "    def init(\n",
        "        cls,\n",
        "        config_or_setup: Optional[Union[PlannerConfig, str, os.PathLike]] = None,\n",
        "        tools: Optional[List[Any]] = None,\n",
        "        *,\n",
        "        context_tables: Optional[List[str]] = None,\n",
        "    ) -> \"Agent\":\n",
        "        \"\"\"\n",
        "        Construye un Agent listo para usar.\n",
        "\n",
        "        Patrones soportados:\n",
        "\n",
        "            agent = Agent.init()\n",
        "            agent = Agent.init(\"setup.yaml\")\n",
        "            agent = Agent.init(setup_path)  # PathLike\n",
        "            agent = Agent.init(PlannerConfig(...))\n",
        "            agent = Agent.init(\"setup.yaml\", context_tables=[...])\n",
        "\n",
        "        NOTA:\n",
        "        - Si pasas PlannerConfig → se ignora el bloque `planner` de setup.yaml.\n",
        "        - Si quieres combinar ambas cosas, construye tú el PlannerConfig\n",
        "          leyendo el YAML y pásalo aquí.\n",
        "        \"\"\"\n",
        "        # 1) Resolver si el primer parámetro es un PlannerConfig o un path\n",
        "        setup_path: Optional[Union[str, Path]] = None\n",
        "        planner_cfg: Optional[PlannerConfig] = None\n",
        "\n",
        "        if isinstance(config_or_setup, PlannerConfig):\n",
        "            planner_cfg = config_or_setup\n",
        "        elif isinstance(config_or_setup, (str, os.PathLike)):\n",
        "            setup_path = config_or_setup\n",
        "\n",
        "        # 2) Cargar setup.yaml (si existe)\n",
        "        setup_path_resolved, setup_cfg = cls._load_setup_config(setup_path)\n",
        "\n",
        "        # 3) Aplicar envs de modelos (si vienen en setup.yaml)\n",
        "        if setup_cfg:\n",
        "            cls._apply_model_env_from_setup(setup_cfg)\n",
        "\n",
        "        # 4) PlannerConfig: o bien el explícito, o bien override desde YAML\n",
        "        if planner_cfg is not None:\n",
        "            cfg = planner_cfg\n",
        "        else:\n",
        "            cfg = PlannerConfig()\n",
        "            planner_section = setup_cfg.get(\"planner\") or {}\n",
        "            # Sobrescribimos sólo campos conocidos (tolerante)\n",
        "            for key, value in planner_section.items():\n",
        "                if hasattr(cfg, key):\n",
        "                    setattr(cfg, key, value)\n",
        "\n",
        "        # 5) Tools: prioridad al parámetro explícito; si no, leer de setup.yaml\n",
        "        if tools is not None:\n",
        "            tools_list = tools\n",
        "        else:\n",
        "            tools_section = setup_cfg.get(\"tools\") or {}\n",
        "            enabled_names = tools_section.get(\"enabled\")\n",
        "            tools_list = get_default_tools(enabled_names=enabled_names)\n",
        "\n",
        "        # 6) LLM planner bindeado a las tools\n",
        "        planner_llm = build_planner_llm(cfg)\n",
        "        planner_llm = planner_llm.bind_tools(tools_list)\n",
        "\n",
        "        # 7) Construir grafo principal\n",
        "        graph_app = load_logic(\n",
        "            planner_llm=planner_llm,\n",
        "            tools=tools_list,\n",
        "            planner_config=cfg,\n",
        "        )\n",
        "\n",
        "        # 8) Config de memoria desde setup.yaml (si existe)\n",
        "        memory_cfg = setup_cfg.get(\"memory\") or {}\n",
        "\n",
        "        # 9) Knowledge Bases (tabulares / vectores / APIs) desde setup.yaml\n",
        "        kb_list = cls._build_kb_from_setup(setup_cfg)\n",
        "\n",
        "        # 10) Tablas de contexto (parametrías, abreviaturas, etc.)\n",
        "        final_context_tables, context_cfg = cls._resolve_context_tables(\n",
        "            setup_cfg=setup_cfg,\n",
        "            explicit_context_tables=context_tables,\n",
        "        )\n",
        "\n",
        "        return cls(\n",
        "            graph_app=graph_app,\n",
        "            planner_config=cfg,\n",
        "            tools=tools_list,\n",
        "            setup_path=str(setup_path_resolved) if setup_path_resolved else None,\n",
        "            setup_config=setup_cfg,\n",
        "            memory_cfg=memory_cfg,\n",
        "            knowledge_bases=kb_list,\n",
        "            context_tables=final_context_tables,\n",
        "            context_cfg=context_cfg,\n",
        "        )\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Helpers internos\n",
        "    # ------------------------------------------------------------------\n",
        "    def _coerce_input(\n",
        "        self,\n",
        "        user_input: Union[str, Dict[str, Any], AgentInput],\n",
        "    ) -> AgentInput:\n",
        "        \"\"\"\n",
        "        Normaliza la entrada a AgentInput:\n",
        "\n",
        "        - str  -> AgentInput(user_prompt=...)\n",
        "        - dict -> AgentInput(**dict)\n",
        "        - AgentInput -> se respeta tal cual\n",
        "        \"\"\"\n",
        "        if isinstance(user_input, AgentInput):\n",
        "            return user_input\n",
        "        if isinstance(user_input, dict):\n",
        "            return AgentInput(**user_input)\n",
        "        return AgentInput(user_prompt=str(user_input))\n",
        "\n",
        "    def _clean_prev_messages(self) -> List[AnyMessage]:\n",
        "        \"\"\"\n",
        "        Limpia el historial previo para evitar ToolMessages gigantes en turnos futuros.\n",
        "        \"\"\"\n",
        "        msgs: List[AnyMessage] = []\n",
        "        if self._state is None:\n",
        "            return msgs\n",
        "\n",
        "        for m in self._state.get(\"messages\", []):\n",
        "            # ❌ No arrastramos ToolMessages con JSONs enormes\n",
        "            if isinstance(m, ToolMessage):\n",
        "                continue\n",
        "            msgs.append(m)\n",
        "        return msgs\n",
        "\n",
        "    def _build_deep_text(\n",
        "        self,\n",
        "        summary_obj: Optional[AgentSummary],\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Construye la vista 'deep' (resumen por sección) a partir de AgentSummary.\n",
        "\n",
        "        - No incluye el raw_state completo.\n",
        "        - Es más compacta que la traza dev_out, pero sigue seccionada.\n",
        "        \"\"\"\n",
        "        if summary_obj is None:\n",
        "            # Si por alguna razón no hay summary, devolvemos cadena vacía;\n",
        "            # el caller puede hacer fallback.\n",
        "            return \"\"\n",
        "\n",
        "        parts: List[str] = [\"## Resumen deep del pipeline\"]\n",
        "\n",
        "        if summary_obj.analyzer:\n",
        "            parts.append(\"### ANALYZER\\n\" + summary_obj.analyzer)\n",
        "        if summary_obj.planner:\n",
        "            parts.append(\"### PLANNER\\n\" + summary_obj.planner)\n",
        "        if summary_obj.executor:\n",
        "            parts.append(\"### EXECUTOR\\n\" + summary_obj.executor)\n",
        "        if summary_obj.catcher:\n",
        "            parts.append(\"### CATCHER\\n\" + summary_obj.catcher)\n",
        "        if summary_obj.summarizer:\n",
        "            parts.append(\"### SUMMARIZER (basado en herramientas)\\n\" + summary_obj.summarizer)\n",
        "        if summary_obj.final_answer:\n",
        "            parts.append(\"### RESPUESTA FINAL\\n\" + summary_obj.final_answer)\n",
        "\n",
        "        return \"\\n\\n\".join(parts)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # API público\n",
        "    # ------------------------------------------------------------------\n",
        "    def run_turn(\n",
        "        self,\n",
        "        user_input: Union[str, Dict[str, Any], AgentInput],\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Ejecuta un turno de conversación.\n",
        "\n",
        "        Devuelve SIEMPRE un dict:\n",
        "\n",
        "            {\n",
        "              \"dev_out\": {\n",
        "                  \"final_answer\": \"...pipeline markdown (traza completa)...\",\n",
        "                  \"summary\": {...},\n",
        "                  \"tool_runs\": [...],\n",
        "                  \"raw_state\": {...}   # estado crudo del grafo\n",
        "              },\n",
        "              \"deep_out\": {\n",
        "                  \"final_answer\": \"...resumen por sección...\",\n",
        "                  \"summary\": {...},\n",
        "                  \"tool_runs\": [...],\n",
        "                  \"raw_state\": {}      # vista ligera\n",
        "              },\n",
        "              \"user_out\": {\n",
        "                  \"final_answer\": \"...respuesta para usuario (1:1)...\",\n",
        "                  \"summary\": {...},\n",
        "                  \"tool_runs\": [...],\n",
        "                  \"raw_state\": {}\n",
        "              }\n",
        "            }\n",
        "        \"\"\"\n",
        "        agent_in = self._coerce_input(user_input)\n",
        "\n",
        "        # Texto canónico del prompt (user_prompt > user_text como fallback)\n",
        "        prompt_text = (\n",
        "            getattr(agent_in, \"user_prompt\", None)\n",
        "            or getattr(agent_in, \"user_text\", None)\n",
        "            or \"\"\n",
        "        )\n",
        "\n",
        "        # -------------------------\n",
        "        # 0) Resolver session_id / user_id / kb_names\n",
        "        # -------------------------\n",
        "        session_id = agent_in.session_id or \"default\"\n",
        "        user_id = None\n",
        "        if agent_in.metadata:\n",
        "            user_id = agent_in.metadata.get(\"user_id\")\n",
        "\n",
        "        kb_names = agent_in.kb_names or []\n",
        "\n",
        "        # Seleccionar KBs activas para este turno (si kb_names está vacío, usamos todas)\n",
        "        kb_selected = get_kb_by_names(kb_names, self.knowledge_bases)\n",
        "\n",
        "        # -------------------------\n",
        "        # 1) Leer memoria para esta sesión\n",
        "        # -------------------------\n",
        "        memory_context = read_memory(session_id=session_id)\n",
        "\n",
        "        # 2) Construir estado de entrada al grafo\n",
        "        prev_messages = self._clean_prev_messages()\n",
        "        state_in: State = {\n",
        "            \"messages\": prev_messages + [HumanMessage(content=prompt_text)],\n",
        "            \"analyzer\": None,\n",
        "            \"planner_trajs\": [],\n",
        "            \"executor_steps\": [],\n",
        "            \"tool_runs\": [],\n",
        "            \"summary\": None,\n",
        "            \"pipeline_summary\": None,\n",
        "            # Campos extra (no tipados en State, pero soportados por TypedDict total=False)\n",
        "            \"user_prompt\": prompt_text,\n",
        "            \"session_id\": session_id,\n",
        "            \"user_id\": user_id,\n",
        "            \"setup_path\": self.setup_path or \"\",\n",
        "            \"setup_config\": self.setup_config,  # 👈 YAML completo (por si lo necesita el grafo)\n",
        "            \"kb_names\": kb_names,\n",
        "            \"kb_all\": [kb.__dict__ for kb in self.knowledge_bases],\n",
        "            \"kb_selected\": [kb.__dict__ for kb in kb_selected],\n",
        "            \"memory_context\": memory_context,\n",
        "            # Tablas de contexto (para semantic_search_in_csv en el grafo)\n",
        "            \"context_tables\": list(self.context_tables),\n",
        "            \"context_cfg\": self.context_cfg,\n",
        "        }\n",
        "\n",
        "        # 3) Invocar grafo\n",
        "        out_state: State = self.graph_app.invoke(state_in)\n",
        "        self._state = out_state  # guardamos para multi-turn\n",
        "\n",
        "        # 4) Extraer último AIMessage (por si no vienen campos dev_out/deep_out en state)\n",
        "        ai_messages = [\n",
        "            m for m in out_state.get(\"messages\", []) if isinstance(m, AIMessage)\n",
        "        ]\n",
        "        last_ai = ai_messages[-1] if ai_messages else None\n",
        "        last_ai_text = last_ai.content if last_ai is not None else \"\"\n",
        "\n",
        "        # 5) Campos de texto que el grafo ya pudo haber dejado en el estado\n",
        "        dev_text_state = out_state.get(\"dev_out\")  # type: ignore[assignment]\n",
        "        deep_text_state = out_state.get(\"deep_out\")  # type: ignore[assignment]\n",
        "        user_text_state = out_state.get(\"user_out\")  # type: ignore[assignment]\n",
        "\n",
        "        # 6) Summary estructurado (con final_answer \"para usuario\")\n",
        "        raw_summary: Dict[str, Any] = (\n",
        "            out_state.get(\"pipeline_summary\")  # type: ignore[arg-type]\n",
        "            or out_state.get(\"summary\")  # type: ignore[arg-type]\n",
        "            or {}\n",
        "        )\n",
        "        if raw_summary:\n",
        "            summary_obj = AgentSummary(**raw_summary)\n",
        "            summary_user_answer = summary_obj.final_answer or \"\"\n",
        "        else:\n",
        "            summary_obj = None\n",
        "            summary_user_answer = \"\"\n",
        "\n",
        "        # 7) Tool runs normalizados (para las tres vistas)\n",
        "        raw_runs = out_state.get(\"tool_runs\", []) or []\n",
        "        tool_runs: List[ToolRun] = []\n",
        "        for r in raw_runs:\n",
        "            tool_runs.append(\n",
        "                ToolRun(\n",
        "                    id=str(r.get(\"id\", \"\")),\n",
        "                    name=str(r.get(\"name\", \"\")),\n",
        "                    args=r.get(\"args\", {}),\n",
        "                    output=r.get(\"output\"),\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # 8) Resolver textos finales por vista con prioridades claras\n",
        "        # USER:\n",
        "        #   1) user_text_state (si el grafo lo puso)\n",
        "        #   2) summary_user_answer (si existe en AgentSummary)\n",
        "        #   3) last_ai_text (último mensaje de la traza)\n",
        "        final_user = (\n",
        "            (user_text_state or \"\").strip()\n",
        "            or summary_user_answer.strip()\n",
        "            or last_ai_text.strip()\n",
        "        )\n",
        "\n",
        "        # DEEP:\n",
        "        #   1) deep_text_state (del grafo)\n",
        "        #   2) _build_deep_text(summary_obj)\n",
        "        #   3) summary_user_answer\n",
        "        final_deep = (\n",
        "            (deep_text_state or \"\").strip()\n",
        "            or self._build_deep_text(summary_obj).strip()\n",
        "            or summary_user_answer.strip()\n",
        "            or last_ai_text.strip()\n",
        "        )\n",
        "\n",
        "        # DEV:\n",
        "        #   1) dev_text_state (del grafo: traza markdown completa)\n",
        "        #   2) last_ai_text\n",
        "        #   3) deep_text (como fallback)\n",
        "        final_dev = (\n",
        "            (dev_text_state or \"\").strip()\n",
        "            or last_ai_text.strip()\n",
        "            or final_deep\n",
        "        )\n",
        "\n",
        "        # 9) Construir vistas\n",
        "        dev_view = AgentView(\n",
        "            final_answer=final_dev,\n",
        "            summary=summary_obj,\n",
        "            tool_runs=tool_runs,\n",
        "            raw_state=out_state,  # vista completa\n",
        "        )\n",
        "\n",
        "        deep_view = AgentView(\n",
        "            final_answer=final_deep,\n",
        "            summary=summary_obj,\n",
        "            tool_runs=tool_runs,\n",
        "            raw_state={},  # light view\n",
        "        )\n",
        "\n",
        "        user_view = AgentView(\n",
        "            final_answer=final_user,\n",
        "            summary=summary_obj,\n",
        "            tool_runs=tool_runs,\n",
        "            raw_state={},  # usuario nunca necesita estado crudo\n",
        "        )\n",
        "\n",
        "        # 10) Actualizar memoria de largo/corto plazo\n",
        "        try:\n",
        "            write_memory(\n",
        "                session_id=session_id,\n",
        "                user_prompt=prompt_text,\n",
        "                user_out=final_user,\n",
        "                user_id=user_id,\n",
        "                memory_cfg=self.memory_cfg,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            # No queremos que un fallo en memoria rompa el turno\n",
        "            print(f\"[Agent] ⚠️ Error escribiendo memoria: {e!r}\")\n",
        "\n",
        "        # 11) Empaquetar en AgentOutput y devolver como dict puro\n",
        "        output = AgentOutput(\n",
        "            dev_out=dev_view,\n",
        "            deep_out=deep_view,\n",
        "            user_out=user_view,\n",
        "        )\n",
        "        return output.to_dict()\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Extras útiles para debugging\n",
        "    # ------------------------------------------------------------------\n",
        "    @property\n",
        "    def last_state(self) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Devuelve el último estado crudo del grafo (solo lectura).\"\"\"\n",
        "        return self._state\n",
        "\n",
        "    def reset_session(self) -> None:\n",
        "        \"\"\"Resetea el estado interno de conversación (no borra memoria global).\"\"\"\n",
        "        self._state = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Zp_ogq0KAbFw",
        "outputId": "fcfcac27-51bc-411f-cd3f-e59963ddfa31"
      },
      "outputs": [],
      "source": [
        "#@title agnostic_agent/__init__.py\n",
        "%%writefile agnostic_agent/__init__.py\n",
        "from __future__ import annotations\n",
        "\n",
        "\"\"\"\n",
        "Agnostic Deep Agent 2026 – paquete principal.\n",
        "\n",
        "Arquitectura por módulos:\n",
        "\n",
        "- setup.yaml        → configuración declarativa (modelos, memoria, planner, KBs, etc.).\n",
        "- schemas.py        → contratos de datos internos (AnalyzerIntent, PlannerPlan, ToolRun…).\n",
        "- capabilities.py   → capacidades de exploración de entornos:\n",
        "                      · gestión de modelos / backends (vLLM, OpenAI, etc.)\n",
        "                      · lanzamiento de servidores Qwen3+vLLM\n",
        "                      · configuración del planner de herramientas.\n",
        "- tools.py          → catálogo y registro de tools (toy, matemáticas, embeddings,\n",
        "                      reranker, tools de contexto/tablas…).\n",
        "- memory.py         → memoria de sesión / corto / largo plazo.\n",
        "- prompts.py        → prompts por rol (analyzer, summarizer, validator, memory_write…).\n",
        "- logic.py          → grafo maestro + sub-grafos\n",
        "                      (ANALYZER, PLANNER, EXECUTOR, CATCHER, SUMMARIZER, VALIDATOR).\n",
        "- agent.py          → clase Agent de alto nivel (init, run_turn).\n",
        "- communication.py  → normalización de I/O (AgentInput, AgentOutput, vistas user/deep/dev).\n",
        "- context.py        → definición de Knowledge Bases y conectores externos\n",
        "                      (por ejemplo:\n",
        "                       · BD tabular en SQLite\n",
        "                       · VDB en sqlite-vec\n",
        "                       · otras fuentes RAG, APIs, SQL, etc.).\n",
        "\"\"\"\n",
        "\n",
        "from .agent import Agent\n",
        "from .communication import AgentInput, AgentOutput\n",
        "from .capabilities import (\n",
        "    PlannerConfig,\n",
        "    QwenModelPaths,\n",
        "    VllmConfig,\n",
        "    VllmServers,\n",
        "    VllmEndpoints,\n",
        "    prepare_qwen_models,\n",
        "    start_qwen_vllm_servers,\n",
        ")\n",
        "from .tools import get_default_tools\n",
        "from .context import KnowledgeBase, get_default_context\n",
        "\n",
        "__version__ = \"0.2.0\"\n",
        "\n",
        "# Alias de compatibilidad con versiones anteriores (legacy)\n",
        "AgentSession = Agent  # type: ignore\n",
        "\n",
        "# API pública principal\n",
        "__all__ = [\n",
        "    # Núcleo del agente\n",
        "    \"Agent\",\n",
        "    \"AgentSession\",\n",
        "    \"AgentInput\",\n",
        "    \"AgentOutput\",\n",
        "    # Configuración / modelos\n",
        "    \"PlannerConfig\",\n",
        "    \"QwenModelPaths\",\n",
        "    \"VllmConfig\",\n",
        "    \"VllmServers\",\n",
        "    \"VllmEndpoints\",\n",
        "    \"prepare_qwen_models\",\n",
        "    \"start_qwen_vllm_servers\",\n",
        "    # Contexto / tools de alto nivel\n",
        "    \"KnowledgeBase\",\n",
        "    \"get_default_tools\",\n",
        "    \"get_default_context\",\n",
        "    # Meta\n",
        "    \"__version__\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpKY9D-_A1e0"
      },
      "source": [
        "# DEFINE vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "70ea88c22d4f4b5bba014abb3f349591",
            "530a8ebc39f44d068713c14806ca235f",
            "1d66561f7a504a038d16f2c1b27e2198",
            "f25e85518d144a7cb3cd4ff520999f30",
            "99b11010c8e948c59306857eb91c6eb7",
            "d0e54c64b1c54cb185b3a2fce0bfd3a5",
            "a30827a944074bf492ceda605fde31de",
            "fdc05ca4bf1e4a809457dac153eda61b",
            "67321bbe29544fbbb280f57c70661bcf",
            "49d5e0bf3e714140b47de7e26e70524a",
            "a9b1a7770a894986ab02e71ca54ccc8f",
            "903d102f8fd046888b325c2bd30d723b",
            "03bdbb86346f47f6a861955e71aec295",
            "b347abe7783041b39c00c9713f0515c3",
            "94f3efccfa8041d4ad14ed288783883f",
            "28bf413b76644b108b28d50aecaf6f87",
            "49714a558a7b4d3185806b88fcb3ce5b",
            "5a1462f7a41941079e1cc03c8639695e",
            "cc334d03910b4fa19af1f86e7339c3f1",
            "82db5b96e80849d59bb1e6bcaac9ebf4",
            "3bedc8b346ec499db047c563ebc5a42a",
            "03f1dee8ad8848318261057f66159549",
            "4b10f9de7b9a4e89b103bbb2b9a8e30e",
            "634195a2bf13459ead87450c6a8c634b",
            "867f0c587dd5494aa4d2b0d42cb6f6c9",
            "568c99c13aea4d85bc9ac278820d9206",
            "d09fa840319943898a93e989f3d40e05",
            "7679be21c9fb41b9bea0fc0f1e0591f2",
            "ee7b09fa8da64c76b6bbe3e44768ae63",
            "ab2cc26d2a494019b7da48ee2896c89c"
          ]
        },
        "id": "By1R6kDWA0kt",
        "outputId": "4496cb87-bc03-4af5-d21b-2f05dd39d1eb"
      },
      "outputs": [],
      "source": [
        "# TRY\n",
        "\n",
        "#@title DOWNLOAD MODELS – LLM, Embedding, Reranker (Qwen3)\n",
        "from agnostic_agent import prepare_qwen_models\n",
        "\n",
        "# IDs de modelos (puedes ajustarlos)\n",
        "LLM_MODEL_ID = \"Qwen/Qwen3-0.6B\"  #@param [\"Qwen/Qwen3-0.6B\", \"Qwen/Qwen3-0.6B-Base\", \"Qwen/Qwen3-4B\", \"Qwen/Qwen3-4B-Instruct-2507\", \"Qwen/Qwen3-4B-Thinking-2507\"]\n",
        "\n",
        "EMB_MODEL_ID = \"Qwen/Qwen3-Embedding-0.6B\"  #@param [\"Qwen/Qwen3-Embedding-0.6B\"]\n",
        "\n",
        "RERANK_MODEL_ID = \"Qwen/Qwen3-Reranker-0.6B\"  #@param [\"Qwen/Qwen3-Reranker-0.6B\"]\n",
        "\n",
        "print(\"⬇️ Descargando / preparando modelos Qwen3 (prepare_qwen_models)...\")\n",
        "model_paths = prepare_qwen_models(\n",
        "    llm_model_id=LLM_MODEL_ID,\n",
        "    emb_model_id=EMB_MODEL_ID,\n",
        "    rerank_model_id=RERANK_MODEL_ID,\n",
        "    base_dir=\"LM_MODEL\",\n",
        ")\n",
        "print(\"\\n✅ Modelos listos en disco:\")\n",
        "print(\"  LLM   :\", model_paths.llm_dir)\n",
        "print(\"  EMB   :\", model_paths.emb_dir)\n",
        "print(\"  RERANK:\", model_paths.rerank_dir)\n",
        "\n",
        "import os\n",
        "os.environ[\"LLM_MODEL_ID\"]   = LLM_MODEL_ID\n",
        "os.environ[\"EMB_MODEL_ID\"]   = EMB_MODEL_ID\n",
        "os.environ[\"RERANK_MODEL_ID\"] = RERANK_MODEL_ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "eu9RrulDBD0N",
        "outputId": "f078bce8-eef4-4744-d0bd-bec17a83c402"
      },
      "outputs": [],
      "source": [
        "#@title LIMPIEZA LIGERA DE GPU (opcional)\n",
        "import gc\n",
        "try:\n",
        "    import torch\n",
        "    has_torch = True\n",
        "except ImportError:\n",
        "    has_torch = False\n",
        "\n",
        "print(\"🧹 Limpiando referencias de Python...\")\n",
        "gc.collect()\n",
        "if has_torch and torch.cuda.is_available():\n",
        "    print(\"🧠 Vaciando caché CUDA...\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\"ℹ️ Torch CUDA no disponible o sin GPU visible.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9V8pxH2FnJ-",
        "outputId": "c8de88c3-a73e-41e3-ef9d-b839ae9ea233"
      },
      "outputs": [],
      "source": [
        "#@title DEPLOY vLLM SERVER – SOLO LLM (chat) · L4/T4-friendly\n",
        "import transformers\n",
        "from agnostic_agent import VllmConfig, start_qwen_vllm_servers\n",
        "\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "\n",
        "VLLM_HOST = os.getenv(\"VLLM_HOST\", \"127.0.0.1\")\n",
        "LLM_PORT = 8000\n",
        "EMB_PORT = 8001\n",
        "RERANK_PORT = 8002\n",
        "VLLM_MODE = \"POWER\"  # \"FAST\", \"MEDIUM\", \"POWER\", \"LIMIT\"\n",
        "\n",
        "print(f\"\\n🚀 Lanzando servidor vLLM (start_qwen_vllm_servers)...\")\n",
        "print(f\"🔧 VLLM_MODE = {VLLM_MODE}\")\n",
        "\n",
        "if VLLM_MODE == \"FAST\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.40,\n",
        "        llm_max_len=2048,\n",
        "        llm_max_num_seqs=4,\n",
        "        emb_gpu_util=0.30,\n",
        "        emb_max_len=512,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.30,\n",
        "        rerank_max_len=512,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "elif VLLM_MODE == \"MEDIUM\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.55,\n",
        "        llm_max_len=4096,\n",
        "        llm_max_num_seqs=6,\n",
        "        emb_gpu_util=0.30,\n",
        "        emb_max_len=512,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.30,\n",
        "        rerank_max_len=512,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "elif VLLM_MODE == \"POWER\":\n",
        "    # Modo POWER “cargado” para L4 + Qwen3-0.6B:\n",
        "    # - Más contexto (16k tokens)\n",
        "    # - Menos concurrencia (4 secuencias)\n",
        "    # - GPU util ~0.8 para exprimir la L4 sin matarla\n",
        "    vllm_cfg = VllmConfig(\n",
        "        # config base\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        # language model\n",
        "        llm_gpu_util=0.90,   # antes 0.70\n",
        "        llm_max_len=32768,   # antes 8192  ← clave\n",
        "        llm_max_num_seqs=1,  # antes 8     ← menos batch, más contexto\n",
        "        # embedding model\n",
        "        emb_gpu_util=0.05,\n",
        "        emb_max_len=1024,\n",
        "        emb_max_num_seqs=1,\n",
        "        # reranker model\n",
        "        rerank_gpu_util=0.05,\n",
        "        rerank_max_len=1024,\n",
        "        rerank_max_num_seqs=1,\n",
        "        # config extra\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "\n",
        "elif \"LIMIT\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.85,    # un poco más agresivo\n",
        "        llm_max_len=32768,    # 🔥 32k tokens de contexto\n",
        "        llm_max_num_seqs=2,   # menos batch, más contexto por secuencia\n",
        "        emb_gpu_util=0.35,\n",
        "        emb_max_len=1024,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.35,\n",
        "        rerank_max_len=1024,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True, # True | False\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",    # \"qwen3\" | none\n",
        "        start_emb_server=True,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "\n",
        "endpoints, servers = start_qwen_vllm_servers(\n",
        "    model_paths=model_paths,\n",
        "    config=vllm_cfg,\n",
        "    set_env=True,\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Servidor vLLM (LLM) listo.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3FxXSB_qyR5"
      },
      "source": [
        "# RUN EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCjT0JZm9zu3",
        "outputId": "ec5c25f2-1ef6-434a-ffe3-43859625992f"
      },
      "outputs": [],
      "source": [
        "#@title streamlit_app.py\n",
        "%%writefile streamlit_app.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import html\n",
        "from typing import Any, Dict, Optional, List, Tuple\n",
        "\n",
        "import streamlit as st\n",
        "from agnostic_agent.agent import Agent\n",
        "\n",
        "# -----------------------------\n",
        "# Page\n",
        "# -----------------------------\n",
        "st.set_page_config(\n",
        "    page_title=\"Agnostic Agent · Chat Studio (Inspector)\",\n",
        "    page_icon=\"🧪\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Hide Streamlit chrome (dark cintillo)\n",
        "# -----------------------------\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "[data-testid=\"stHeader\"] { display: none !important; }\n",
        "[data-testid=\"stToolbar\"] { display: none !important; }\n",
        "[data-testid=\"stDecoration\"] { display: none !important; }\n",
        "#MainMenu { visibility: hidden !important; }\n",
        "footer { visibility: hidden !important; }\n",
        ".block-container { padding-top: 1rem !important; }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# -----------------------------\n",
        "# CSS (Studio + Inspector layout)\n",
        "# -----------------------------\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "<style>\n",
        ":root{\n",
        "  --bg: #0b1020;\n",
        "  --panel: rgba(255,255,255,.06);\n",
        "  --panel2: rgba(255,255,255,.08);\n",
        "  --border: rgba(255,255,255,.10);\n",
        "  --text: rgba(255,255,255,.92);\n",
        "  --muted: rgba(255,255,255,.65);\n",
        "  --accent: #7c5cff;\n",
        "  --good: #2dd4bf;\n",
        "\n",
        "  --r: 18px;\n",
        "  --r2: 14px;\n",
        "  --shadow: 0 12px 35px rgba(0,0,0,.35);\n",
        "  --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", \"Courier New\", monospace;\n",
        "}\n",
        "\n",
        ".stApp{\n",
        "  background:\n",
        "    radial-gradient(1200px 500px at 10% -10%, rgba(124,92,255,.35), transparent 60%),\n",
        "    radial-gradient(900px 500px at 90% 0%, rgba(45,212,191,.18), transparent 60%),\n",
        "    linear-gradient(180deg, var(--bg), #070a14 60%, #050712);\n",
        "  color: var(--text);\n",
        "}\n",
        "\n",
        ".block-container{ padding-top: 1.0rem; padding-bottom: 1.6rem; }\n",
        "\n",
        "section[data-testid=\"stSidebar\"]{\n",
        "  background: rgba(0,0,0,.18);\n",
        "  border-right: 1px solid rgba(255,255,255,.06);\n",
        "}\n",
        "\n",
        ".topbar{\n",
        "  display:flex; align-items:center; justify-content:space-between;\n",
        "  gap:12px;\n",
        "  padding: 12px 14px;\n",
        "  border-radius: var(--r);\n",
        "  background: linear-gradient(180deg, rgba(255,255,255,.08), rgba(255,255,255,.05));\n",
        "  border: 1px solid var(--border);\n",
        "  box-shadow: var(--shadow);\n",
        "  margin-bottom: 10px;\n",
        "}\n",
        ".brand{display:flex; align-items:center; gap:10px;}\n",
        ".logo{\n",
        "  width: 38px; height: 38px; border-radius: 12px;\n",
        "  display:flex; align-items:center; justify-content:center;\n",
        "  background: linear-gradient(135deg, rgba(124,92,255,.9), rgba(45,212,191,.6));\n",
        "  box-shadow: 0 10px 25px rgba(124,92,255,.22);\n",
        "  font-size: 18px;\n",
        "}\n",
        ".title{font-size: 15px; font-weight: 800; line-height: 1.1;}\n",
        ".subtitle{font-size: 12px; color: var(--muted);}\n",
        "\n",
        ".badges{display:flex; flex-wrap:wrap; gap:8px; justify-content:flex-end;}\n",
        ".badge{\n",
        "  font-size: 12px;\n",
        "  padding: 6px 10px;\n",
        "  border-radius: 999px;\n",
        "  border: 1px solid var(--border);\n",
        "  background: rgba(255,255,255,.06);\n",
        "  color: var(--text);\n",
        "}\n",
        ".badge.accent{ border-color: rgba(124,92,255,.45); }\n",
        ".badge.good{ border-color: rgba(45,212,191,.45); }\n",
        "\n",
        ".card{\n",
        "  border-radius: var(--r);\n",
        "  border: 1px solid var(--border);\n",
        "  background: rgba(255,255,255,.06);\n",
        "  box-shadow: 0 10px 30px rgba(0,0,0,.28);\n",
        "  overflow: hidden;\n",
        "  margin-top: 6px;\n",
        "}\n",
        ".card .card-h{\n",
        "  display:flex; align-items:center; justify-content:space-between;\n",
        "  gap:10px;\n",
        "  padding: 10px 12px;\n",
        "  background: rgba(255,255,255,.05);\n",
        "  border-bottom: 1px solid var(--border);\n",
        "  font-weight: 800;\n",
        "  font-size: 13px;\n",
        "}\n",
        ".card .card-h .hint{\n",
        "  font-weight: 500; font-size: 11px; color: var(--muted);\n",
        "}\n",
        ".card .card-b{\n",
        "  padding: 12px 12px 10px 12px;\n",
        "  font-size: 14px;\n",
        "  color: var(--text);\n",
        "}\n",
        "\n",
        "/* Code-like block inside cards (for Thinking) */\n",
        ".codebox{\n",
        "  margin-top: 8px;\n",
        "  padding: 10px 12px;\n",
        "  border-radius: 14px;\n",
        "  border: 1px solid rgba(255,255,255,.10);\n",
        "  background: rgba(0,0,0,.28);\n",
        "  font-family: var(--mono);\n",
        "  font-size: 12px;\n",
        "  line-height: 1.45;\n",
        "  white-space: pre-wrap;\n",
        "  word-break: break-word;\n",
        "  color: rgba(255,255,255,.92);\n",
        "}\n",
        "\n",
        "/* User bubble */\n",
        ".bubble-user{\n",
        "  padding: 10px 12px;\n",
        "  border-radius: 16px;\n",
        "  border: 1px solid rgba(124,92,255,.35);\n",
        "  background: linear-gradient(180deg, rgba(124,92,255,.22), rgba(255,255,255,.05));\n",
        "  box-shadow: 0 8px 24px rgba(0,0,0,.25);\n",
        "}\n",
        "\n",
        "/* Inspector wrapper */\n",
        ".inspector{\n",
        "  border-radius: var(--r);\n",
        "  border: 1px solid var(--border);\n",
        "  background: rgba(255,255,255,.05);\n",
        "  box-shadow: var(--shadow);\n",
        "  padding: 12px;\n",
        "}\n",
        ".inspector h3{ margin: 0 0 6px 0; }\n",
        "\n",
        "/* Expanders */\n",
        "[data-testid=\"stExpander\"]{\n",
        "  border-radius: var(--r);\n",
        "  border: 1px solid var(--border);\n",
        "  background: rgba(255,255,255,.04);\n",
        "  overflow:hidden;\n",
        "}\n",
        "\n",
        "/* Chat spacing */\n",
        "[data-testid=\"stChatMessage\"]{\n",
        "  padding-top: 0.25rem;\n",
        "  padding-bottom: 0.25rem;\n",
        "}\n",
        "</style>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Session state\n",
        "# -----------------------------\n",
        "if \"agent\" not in st.session_state:\n",
        "    st.session_state.agent = None\n",
        "if \"agent_mode\" not in st.session_state:\n",
        "    st.session_state.agent_mode = \"tools_strict\"\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"msg_counter\" not in st.session_state:\n",
        "    st.session_state.msg_counter = 0\n",
        "if \"selected_msg_id\" not in st.session_state:\n",
        "    st.session_state.selected_msg_id = None\n",
        "if \"export_json\" not in st.session_state:\n",
        "    st.session_state.export_json = None\n",
        "\n",
        "# -----------------------------\n",
        "# Sidebar controls\n",
        "# -----------------------------\n",
        "with st.sidebar:\n",
        "    st.markdown(\"## 🧪 Chat Studio\")\n",
        "    agent_mode = st.selectbox(\n",
        "        \"Policy mode\",\n",
        "        [\"tools_strict\", \"free_policies\"],\n",
        "        index=0 if st.session_state.agent_mode == \"tools_strict\" else 1,\n",
        "    )\n",
        "\n",
        "    st.markdown(\"### 🧭 Inspector\")\n",
        "    show_thinking_tab = st.checkbox(\"🧠 Thinking\", value=True)\n",
        "    show_deep_tab = st.checkbox(\"🧠 Deep\", value=True)\n",
        "    show_dev_tab = st.checkbox(\"🔍 Dev\", value=True)\n",
        "\n",
        "    st.markdown(\"### 🧹 Acciones\")\n",
        "    cA, cB = st.columns(2)\n",
        "    with cA:\n",
        "        if st.button(\"🗑️ Limpiar\", use_container_width=True):\n",
        "            st.session_state.messages = []\n",
        "            st.session_state.agent = None\n",
        "            st.session_state.selected_msg_id = None\n",
        "            st.toast(\"Chat reiniciado.\", icon=\"🧹\")\n",
        "            st.rerun()\n",
        "    with cB:\n",
        "        if st.button(\"⬇️ Export\", use_container_width=True):\n",
        "            export = {\"agent_mode\": st.session_state.agent_mode, \"messages\": st.session_state.messages}\n",
        "            st.session_state.export_json = json.dumps(export, ensure_ascii=False, indent=2)\n",
        "            st.toast(\"Transcript listo.\", icon=\"⬇️\")\n",
        "\n",
        "    if isinstance(st.session_state.export_json, str):\n",
        "        st.download_button(\n",
        "            \"Descargar transcript.json\",\n",
        "            data=st.session_state.export_json,\n",
        "            file_name=\"transcript.json\",\n",
        "            mime=\"application/json\",\n",
        "            use_container_width=True,\n",
        "        )\n",
        "\n",
        "# Mode change => reset agent (history stays)\n",
        "if st.session_state.agent_mode != agent_mode:\n",
        "    st.session_state.agent_mode = agent_mode\n",
        "    st.session_state.agent = None\n",
        "    st.toast(f\"Modo: {agent_mode}\", icon=\"🧭\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def next_id() -> int:\n",
        "    st.session_state.msg_counter += 1\n",
        "    return st.session_state.msg_counter\n",
        "\n",
        "def get_or_init_agent(mode: str) -> Agent:\n",
        "    if st.session_state.agent is None:\n",
        "        os.environ[\"PLANNER_POLICY_MODE\"] = mode\n",
        "        os.environ[\"AGENT_POLICY_MODE\"] = mode\n",
        "        with st.spinner(f\"Inicializando agente (mode={mode})…\"):\n",
        "            try:\n",
        "                st.session_state.agent = Agent.init(policy_mode=mode)\n",
        "            except TypeError:\n",
        "                st.session_state.agent = Agent.init()\n",
        "    return st.session_state.agent\n",
        "\n",
        "def normalize_output(raw: Any) -> Dict[str, Any]:\n",
        "    if raw is None:\n",
        "        return {}\n",
        "    if hasattr(raw, \"model_dump\"):\n",
        "        try:\n",
        "            return raw.model_dump()\n",
        "        except TypeError:\n",
        "            pass\n",
        "    if isinstance(raw, dict):\n",
        "        return raw\n",
        "    return {\"user_out\": str(raw)}\n",
        "\n",
        "def as_text(v: Any) -> str:\n",
        "    if v is None:\n",
        "        return \"\"\n",
        "    if isinstance(v, str):\n",
        "        return v.strip()\n",
        "    if isinstance(v, dict):\n",
        "        for k in (\"final_answer\", \"text\", \"content\", \"answer\", \"user_out\"):\n",
        "            vv = v.get(k)\n",
        "            if isinstance(vv, str) and vv.strip():\n",
        "                return vv.strip()\n",
        "        return \"\"\n",
        "    return str(v).strip()\n",
        "\n",
        "def strip_user_prefix(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    t = text.strip()\n",
        "    prefixes = [\n",
        "        \"Respuesta final (modo usuario):\",\n",
        "        \"**Respuesta final (modo usuario):**\",\n",
        "        \"RESPUESTA FINAL (modo usuario):\",\n",
        "    ]\n",
        "    for p in prefixes:\n",
        "        if t.startswith(p):\n",
        "            t = t[len(p):].strip()\n",
        "    return t\n",
        "\n",
        "def get_raw_state(out: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    if not isinstance(out, dict):\n",
        "        return None\n",
        "    if isinstance(out.get(\"messages\"), list):\n",
        "        return out\n",
        "    rs = out.get(\"raw_state\")\n",
        "    if isinstance(rs, dict):\n",
        "        return rs\n",
        "    dev = out.get(\"dev_out\")\n",
        "    if isinstance(dev, dict) and isinstance(dev.get(\"raw_state\"), dict):\n",
        "        return dev[\"raw_state\"]\n",
        "    return None\n",
        "\n",
        "def extract_thinking(raw_state: Optional[Dict[str, Any]]) -> str:\n",
        "    if not isinstance(raw_state, dict):\n",
        "        return \"\"\n",
        "    msgs = raw_state.get(\"messages\")\n",
        "    if not isinstance(msgs, list):\n",
        "        return \"\"\n",
        "    for m in reversed(msgs):\n",
        "        if not isinstance(m, dict):\n",
        "            continue\n",
        "        if m.get(\"type\") != \"ai\":\n",
        "            continue\n",
        "        ak = m.get(\"additional_kwargs\") or {}\n",
        "        if isinstance(ak, dict) and ak.get(\"pipeline_internal\"):\n",
        "            continue\n",
        "        thinking = ak.get(\"reasoning_content\") or ak.get(\"reasoning\") or ak.get(\"thoughts\") or \"\"\n",
        "        return thinking.strip() if isinstance(thinking, str) else \"\"\n",
        "    return \"\"\n",
        "\n",
        "def extract_summary_deep(raw_state: Optional[Dict[str, Any]], deep_out_text: str) -> str:\n",
        "    if deep_out_text:\n",
        "        return deep_out_text\n",
        "    if not isinstance(raw_state, dict):\n",
        "        return \"\"\n",
        "    summary = raw_state.get(\"summary\") or raw_state.get(\"pipeline_summary\")\n",
        "    if not isinstance(summary, dict):\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for k in [\"analyzer\", \"planner\", \"executor\", \"catcher\", \"summarizer\", \"final_answer\"]:\n",
        "        v = summary.get(k, \"\")\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            parts.append(f\"**{k.upper()}**\\n\\n{v.strip()}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts) if parts else \"\"\n",
        "\n",
        "def extract_tool_runs(out: Dict[str, Any], raw_state: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    if isinstance(raw_state, dict):\n",
        "        tr = raw_state.get(\"tool_runs\")\n",
        "        if isinstance(tr, list):\n",
        "            return tr\n",
        "    tr2 = out.get(\"tool_runs\")\n",
        "    if isinstance(tr2, list):\n",
        "        return tr2\n",
        "    dev = out.get(\"dev_out\")\n",
        "    if isinstance(dev, dict) and isinstance(dev.get(\"tool_runs\"), list):\n",
        "        return dev[\"tool_runs\"]\n",
        "    return []\n",
        "\n",
        "def assistant_messages() -> List[Dict[str, Any]]:\n",
        "    return [m for m in st.session_state.messages if m.get(\"role\") == \"assistant\"]\n",
        "\n",
        "def find_message_by_id(msg_id: Optional[int]) -> Optional[Dict[str, Any]]:\n",
        "    if msg_id is None:\n",
        "        return None\n",
        "    for m in st.session_state.messages:\n",
        "        if m.get(\"id\") == msg_id:\n",
        "            return m\n",
        "    return None\n",
        "\n",
        "def default_selected_id() -> Optional[int]:\n",
        "    a = assistant_messages()\n",
        "    return a[-1][\"id\"] if a else None\n",
        "\n",
        "def card_md(title: str, body_md: str, icon: str = \"⬛\", hint: str = \"\") -> None:\n",
        "    body_md = body_md or \"_(vacío)_\"\n",
        "    hint_html = f'<span class=\"hint\">{html.escape(hint)}</span>' if hint else \"\"\n",
        "    # NOTE: body_md here is treated as plain HTML content; for Deep this is OK.\n",
        "    # For Thinking we use code-card below so it looks like \"markdown blocks\".\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "<div class=\"card\">\n",
        "  <div class=\"card-h\">\n",
        "    <div>{icon} {html.escape(title)}</div>\n",
        "    {hint_html}\n",
        "  </div>\n",
        "  <div class=\"card-b\">{body_md}</div>\n",
        "</div>\n",
        "\"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "def card_code(title: str, code_text: str, icon: str = \"🧠\", hint: str = \"reasoning_content\") -> None:\n",
        "    safe = html.escape(code_text or \"\")\n",
        "    hint_html = f'<span class=\"hint\">{html.escape(hint)}</span>' if hint else \"\"\n",
        "    content = safe if safe.strip() else html.escape(\"_(no viene thinking en este turno)_\")\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "<div class=\"card\">\n",
        "  <div class=\"card-h\">\n",
        "    <div>{icon} {html.escape(title)}</div>\n",
        "    {hint_html}\n",
        "  </div>\n",
        "  <div class=\"card-b\">\n",
        "    <div class=\"codebox\">{content}</div>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "def render_tool_runs(tool_runs: List[Dict[str, Any]]) -> None:\n",
        "    if not tool_runs:\n",
        "        st.markdown(\"_(No se ejecutaron tools en este turno.)_\")\n",
        "        return\n",
        "    st.markdown(\"#### 🛠 Tools ejecutadas\")\n",
        "    for i, tr in enumerate(tool_runs, start=1):\n",
        "        if not isinstance(tr, dict):\n",
        "            st.markdown(f\"**{i}. tool_{i}**\")\n",
        "            st.code(str(tr))\n",
        "            continue\n",
        "        name = tr.get(\"name\", f\"tool_{i}\")\n",
        "        args = tr.get(\"args\", {})\n",
        "        output = tr.get(\"output\", \"\")\n",
        "        st.markdown(f\"**{i}. {name}**\")\n",
        "        if args:\n",
        "            st.code(args, language=\"json\")\n",
        "        if output != \"\":\n",
        "            st.markdown(\"**Salida:**\")\n",
        "            st.code(str(output))\n",
        "\n",
        "# If nothing selected yet, default to last assistant\n",
        "if st.session_state.selected_msg_id is None:\n",
        "    st.session_state.selected_msg_id = default_selected_id()\n",
        "\n",
        "# -----------------------------\n",
        "# Top bar\n",
        "# -----------------------------\n",
        "mode_badge = \"tools_strict\" if st.session_state.agent_mode == \"tools_strict\" else \"free_policies\"\n",
        "mode_class = \"accent\" if st.session_state.agent_mode == \"tools_strict\" else \"good\"\n",
        "\n",
        "st.markdown(\n",
        "    f\"\"\"\n",
        "<div class=\"topbar\">\n",
        "  <div class=\"brand\">\n",
        "    <div class=\"logo\">🧪</div>\n",
        "    <div>\n",
        "      <div class=\"title\">Agnostic Agent · Chat Studio</div>\n",
        "      <div class=\"subtitle\">Feed limpio + Inspector dinámico (thinking/deep/dev)</div>\n",
        "    </div>\n",
        "  </div>\n",
        "  <div class=\"badges\">\n",
        "    <span class=\"badge {mode_class}\">🧭 {mode_badge}</span>\n",
        "    <span class=\"badge\">🔎 inspector: on</span>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Main layout: Feed | Inspector\n",
        "# -----------------------------\n",
        "feed_col, insp_col = st.columns([2.2, 1.0], gap=\"large\")\n",
        "\n",
        "# -------- FEED (left) --------\n",
        "with feed_col:\n",
        "    for msg in st.session_state.messages:\n",
        "        role = msg.get(\"role\", \"user\")\n",
        "\n",
        "        if role == \"user\":\n",
        "            with st.chat_message(\"user\"):\n",
        "                st.markdown(f'<div class=\"bubble-user\">{html.escape(msg.get(\"content\",\"\"))}</div>', unsafe_allow_html=True)\n",
        "\n",
        "        elif role == \"assistant\":\n",
        "            out = msg.get(\"out\") or {}\n",
        "            answer = strip_user_prefix(as_text(out.get(\"user_out\")))\n",
        "            raw_state = get_raw_state(out)\n",
        "            tool_runs = extract_tool_runs(out, raw_state)\n",
        "\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                # Pretty answer only\n",
        "                card_md(\n",
        "                    title=\"Respuesta\",\n",
        "                    body_md=html.escape(answer or \"_(sin respuesta)_\").replace(\"\\n\", \"<br>\"),\n",
        "                    icon=\"👤\",\n",
        "                    hint=f\"id={msg.get('id')}\",\n",
        "                )\n",
        "\n",
        "                c1, c2, c3 = st.columns([1.2, 1.0, 0.8])\n",
        "                with c1:\n",
        "                    st.caption(f\"🛠 tools: {len(tool_runs)}\")\n",
        "                with c2:\n",
        "                    st.caption(\"📎 Inspector →\")\n",
        "                with c3:\n",
        "                    if st.button(\"🔎 Inspect\", key=f\"inspect_{msg.get('id')}\", use_container_width=True):\n",
        "                        st.session_state.selected_msg_id = msg.get(\"id\")\n",
        "                        st.toast(f\"Inspector → id={msg.get('id')}\", icon=\"🔎\")\n",
        "                        st.rerun()\n",
        "\n",
        "# -------- INSPECTOR (right) --------\n",
        "with insp_col:\n",
        "    st.markdown('<div class=\"inspector\">', unsafe_allow_html=True)\n",
        "    st.markdown(\"### 🔎 Inspector\")\n",
        "\n",
        "    a_msgs = assistant_messages()\n",
        "    if not a_msgs:\n",
        "        st.info(\"Aún no hay respuestas del agente. Escribe algo para empezar.\")\n",
        "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "    else:\n",
        "        ids = [m[\"id\"] for m in a_msgs]\n",
        "\n",
        "        def label(mid: int) -> str:\n",
        "            m = find_message_by_id(mid) or {}\n",
        "            out = m.get(\"out\") or {}\n",
        "            text = strip_user_prefix(as_text(out.get(\"user_out\"))).replace(\"\\n\", \" \").strip()\n",
        "            text = (text[:60] + \"…\") if len(text) > 60 else text\n",
        "            return f\"id={mid} · {text or '(sin texto)'}\"\n",
        "\n",
        "        if st.session_state.selected_msg_id not in ids:\n",
        "            st.session_state.selected_msg_id = ids[-1]\n",
        "\n",
        "        sel = st.selectbox(\n",
        "            \"Mensaje seleccionado\",\n",
        "            options=ids,\n",
        "            index=ids.index(st.session_state.selected_msg_id),\n",
        "            format_func=label,\n",
        "            key=\"inspector_selectbox\",\n",
        "        )\n",
        "        st.session_state.selected_msg_id = sel\n",
        "\n",
        "        m = find_message_by_id(st.session_state.selected_msg_id) or {}\n",
        "        out = m.get(\"out\") or {}\n",
        "        raw_state = get_raw_state(out)\n",
        "\n",
        "        thinking = extract_thinking(raw_state)\n",
        "        deep_txt = extract_summary_deep(raw_state, as_text(out.get(\"deep_out\")))\n",
        "        tool_runs = extract_tool_runs(out, raw_state)\n",
        "\n",
        "        tab_specs: List[Tuple[str, str]] = []\n",
        "        if show_thinking_tab:\n",
        "            tab_specs.append((\"🧠 Thinking\", \"thinking\"))\n",
        "        if show_deep_tab:\n",
        "            tab_specs.append((\"🧠 Deep\", \"deep\"))\n",
        "        if show_dev_tab:\n",
        "            tab_specs.append((\"🔍 Dev\", \"dev\"))\n",
        "\n",
        "        tabs = st.tabs([t[0] for t in tab_specs])\n",
        "\n",
        "        for (tab_title, tab_key), tab in zip(tab_specs, tabs):\n",
        "            with tab:\n",
        "                if tab_key == \"thinking\":\n",
        "                    # ✅ Now it looks like a proper \"markdown code block\" card (monospace, wrapped, bordered)\n",
        "                    card_code(\"Pensamiento (thinking)\", thinking, icon=\"🧠\", hint=\"reasoning_content\")\n",
        "\n",
        "                elif tab_key == \"deep\":\n",
        "                    # Keep as-is (you said you like it)\n",
        "                    # We keep newlines readable and let emphasis render in markdown-ish style\n",
        "                    body = deep_txt or \"_(vacío)_\"\n",
        "                    # Render markdown emphasis by replacing to HTML-ish layout; deep usually ok as plain text too\n",
        "                    body_html = html.escape(body).replace(\"\\n\", \"<br>\")\n",
        "                    card_md(\"Vista profunda (deep_out / summary)\", body_html, icon=\"🧠\", hint=\"pipeline\")\n",
        "\n",
        "                elif tab_key == \"dev\":\n",
        "                    # ✅ Dev: tools like before + raw_state collapsed\n",
        "                    render_tool_runs(tool_runs)\n",
        "\n",
        "                    with st.expander(\"🧬 raw_state (debug)\", expanded=False):\n",
        "                        if isinstance(raw_state, dict) and raw_state:\n",
        "                            st.json(raw_state)\n",
        "                        else:\n",
        "                            st.markdown(\"_(sin raw_state)_\")\n",
        "\n",
        "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Chat input (bottom)\n",
        "# -----------------------------\n",
        "prompt = st.chat_input(\"Escribe tu mensaje…\")\n",
        "\n",
        "if prompt:\n",
        "    uid = next_id()\n",
        "    st.session_state.messages.append({\"id\": uid, \"role\": \"user\", \"content\": prompt, \"out\": None})\n",
        "\n",
        "    agent = get_or_init_agent(st.session_state.agent_mode)\n",
        "    try:\n",
        "        raw_out = agent.run_turn(prompt, policy_mode=st.session_state.agent_mode)\n",
        "    except TypeError:\n",
        "        raw_out = agent.run_turn(prompt)\n",
        "\n",
        "    out = normalize_output(raw_out)\n",
        "    aid = next_id()\n",
        "    st.session_state.messages.append(\n",
        "        {\"id\": aid, \"role\": \"assistant\", \"content\": strip_user_prefix(as_text(out.get(\"user_out\"))), \"out\": out}\n",
        "    )\n",
        "\n",
        "    st.session_state.selected_msg_id = aid\n",
        "    st.rerun()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0TKvBKKbtti"
      },
      "outputs": [],
      "source": [
        "#@title .streamlit/config.toml\n",
        "!mkdir -p .streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Lctc9Zb5sd",
        "outputId": "61a2ab98-c2a5-468b-dc77-1725b046a6a0"
      },
      "outputs": [],
      "source": [
        "%%writefile .streamlit/config.toml\n",
        "[theme]\n",
        "base=\"dark\"\n",
        "primaryColor=\"#7c5cff\"\n",
        "backgroundColor=\"#070a14\"\n",
        "secondaryBackgroundColor=\"#10162b\"\n",
        "textColor=\"#e9e9f2\"\n",
        "font=\"sans serif\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "bx-fpi8XBHZt",
        "outputId": "dd69dbda-7858-4756-d9d6-ee19adb65acc"
      },
      "outputs": [],
      "source": [
        "#@title Lanzar servicio Streamlit en Colab (window o iframe)\n",
        "import subprocess, time, os\n",
        "\n",
        "# ---- PARAMS (Colab UI) ----\n",
        "PORT = 8501  #@param {type:\"integer\"}\n",
        "MODE = \"iframe\"  #@param [\"window\", \"iframe\"]\n",
        "IFRAME_HEIGHT = 800  #@param {type:\"integer\"}\n",
        "IFRAME_WIDTH = \"100%\"  #@param {type:\"string\"}\n",
        "\n",
        "# ---- Colab output (solo si existe) ----\n",
        "try:\n",
        "    from google.colab import output\n",
        "    _IN_COLAB = True\n",
        "except Exception:\n",
        "    output = None\n",
        "    _IN_COLAB = False\n",
        "\n",
        "# ---- Mata procesos previos (opcional) ----\n",
        "os.system('pkill -f \"streamlit run streamlit_app.py\" || true')\n",
        "os.system('pkill -f \"streamlit\" || true')\n",
        "\n",
        "# ---- Lanza Streamlit ----\n",
        "cmd = [\n",
        "    \"streamlit\", \"run\", \"streamlit_app.py\",\n",
        "    \"--server.address\", \"0.0.0.0\",\n",
        "    \"--server.port\", str(PORT),\n",
        "    \"--server.headless\", \"true\",\n",
        "    \"--server.enableCORS\", \"false\",\n",
        "    \"--server.enableXsrfProtection\", \"false\",\n",
        "]\n",
        "\n",
        "streamlit_process = subprocess.Popen(cmd)\n",
        "time.sleep(3)\n",
        "\n",
        "# ---- Exponer ----\n",
        "if _IN_COLAB and output is not None:\n",
        "    if MODE == \"iframe\":\n",
        "        output.serve_kernel_port_as_iframe(\n",
        "            PORT,\n",
        "            width=IFRAME_WIDTH,\n",
        "            height=IFRAME_HEIGHT,\n",
        "        )\n",
        "    else:\n",
        "        output.serve_kernel_port_as_window(\n",
        "            PORT,\n",
        "            anchor_text=\"🔗 Abrir app Streamlit en una nueva pestaña\"\n",
        "        )\n",
        "else:\n",
        "    print(f\"Streamlit corriendo en http://127.0.0.1:{PORT} (no-Colab).\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RFqZU7tnBLHH",
        "kpKY9D-_A1e0"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
