{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqZU7tnBLHH"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zbpyIXulE3sS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.4/948.4 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m509.2/509.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Building wheel for langchain-qwq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m584.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hâœ… Dependencias base instaladas:\n",
            "   - vLLM / Qwen wrapper / LangChain / LangGraph\n",
            "   - Pydantic 2.x + PyYAML (setup.yaml como panel de control)\n",
            "   - Streamlit\n",
            "   - sqlite-vec + pandas + numpy para KB tabular/vectorial sobre SQLite\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# @title INSTALL â€“ Agnostic Agent (GitHub) + Infra Qwen3\n",
        "# ============================================================\n",
        "import os, sys, subprocess, importlib\n",
        "\n",
        "def sh(cmd: str):\n",
        "    return subprocess.run(cmd, shell=True, text=True, capture_output=True)\n",
        "\n",
        "def install_if_missing(package_name, import_name=None):\n",
        "    import_name = import_name or package_name.split('>=')[0].split('==')[0].replace('-', '_')\n",
        "    try: \n",
        "        importlib.import_module(import_name)\n",
        "    except ImportError:\n",
        "        print(f\"ğŸ“¦ Installing {package_name}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package_name])\n",
        "\n",
        "# 1. SUPER CLEANUP & UPDATE\n",
        "if os.path.exists(\"repo_agnostic\"):\n",
        "    print(\"ğŸ”„ Repo exists. Pulling latest changes...\")\n",
        "    os.system(\"cd repo_agnostic && git pull\")\n",
        "else:\n",
        "    print(\"ğŸ§¹ Cleaning up shadowing folders...\")\n",
        "    os.system(\"rm -rf agnostic_agent repo_agnostic\")\n",
        "    print(\"ğŸ“¥ Cloning repo...\")\n",
        "    os.system(\"git clone https://github.com/JacoboGGLeon/agnostic_agent.git repo_agnostic\")\n",
        "\n",
        "# Always reinstall editable to be sure (fast/idempotent usually)\n",
        "print(\"ğŸ›  Updating package install...\")\n",
        "os.system(\"pip install -q -e repo_agnostic\")\n",
        "\n",
        "# 2. FIX PATH\n",
        "pkg_path = os.path.abspath(\"repo_agnostic\")\n",
        "if pkg_path not in sys.path: sys.path.insert(0, pkg_path)\n",
        "\n",
        "# 3. OTHER DEPENDENCIES (Idempotent)\n",
        "deps = [\n",
        "    (\"vllm>=0.9.0\", \"vllm\"),\n",
        "    (\"huggingface_hub>=0.23.0\", \"huggingface_hub\"),\n",
        "    (\"openai==1.108.2\", \"openai\"),\n",
        "    (\"langchain\", \"langchain\"),\n",
        "    (\"langchain-core\", \"langchain_core\"),\n",
        "    (\"langchain-openai\", \"langchain_openai\"),\n",
        "    (\"langgraph\", \"langgraph\"),\n",
        "    (\"pydantic>=2.7.0\", \"pydantic\"),\n",
        "    (\"pyyaml>=6.0\", \"yaml\"),\n",
        "    (\"streamlit\", \"streamlit\"),\n",
        "    (\"sqlite-vec\", \"sqlite_vec\"),\n",
        "    (\"pandas\", \"pandas\"),\n",
        "    (\"numpy\", \"numpy\"),\n",
        "]\n",
        "\n",
        "for pkg, imp in deps:\n",
        "    install_if_missing(pkg, imp)\n",
        "\n",
        "try:\n",
        "    importlib.import_module(\"langchain_qwq_vllm\")\n",
        "except ImportError:\n",
        "    print(\"ğŸ“¦ Installing langchain-qwq-vllm...\")\n",
        "    os.system(\"pip install -q git+https://github.com/whynpc9/langchain-qwq-vllm.git\")\n",
        "\n",
        "print(\"âœ… Agnostic Agent INSTALLED and ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:41 .\n",
            "drwxr-xr-x 1 root root 4096 Feb  6 19:38 ..\n",
            "drwxr-xr-x 4 root root 4096 Dec  9 14:41 .config\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:42 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpKY9D-_A1e0"
      },
      "source": [
        "# DEFINE vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "70ea88c22d4f4b5bba014abb3f349591",
            "530a8ebc39f44d068713c14806ca235f",
            "1d66561f7a504a038d16f2c1b27e2198",
            "f25e85518d144a7cb3cd4ff520999f30",
            "99b11010c8e948c59306857eb91c6eb7",
            "d0e54c64b1c54cb185b3a2fce0bfd3a5",
            "a30827a944074bf492ceda605fde31de",
            "fdc05ca4bf1e4a809457dac153eda61b",
            "67321bbe29544fbbb280f57c70661bcf",
            "49d5e0bf3e714140b47de7e26e70524a",
            "a9b1a7770a894986ab02e71ca54ccc8f",
            "903d102f8fd046888b325c2bd30d723b",
            "03bdbb86346f47f6a861955e71aec295",
            "b347abe7783041b39c00c9713f0515c3",
            "94f3efccfa8041d4ad14ed288783883f",
            "28bf413b76644b108b28d50aecaf6f87",
            "49714a558a7b4d3185806b88fcb3ce5b",
            "5a1462f7a41941079e1cc03c8639695e",
            "cc334d03910b4fa19af1f86e7339c3f1",
            "82db5b96e80849d59bb1e6bcaac9ebf4",
            "3bedc8b346ec499db047c563ebc5a42a",
            "03f1dee8ad8848318261057f66159549",
            "4b10f9de7b9a4e89b103bbb2b9a8e30e",
            "634195a2bf13459ead87450c6a8c634b",
            "867f0c587dd5494aa4d2b0d42cb6f6c9",
            "568c99c13aea4d85bc9ac278820d9206",
            "d09fa840319943898a93e989f3d40e05",
            "7679be21c9fb41b9bea0fc0f1e0591f2",
            "ee7b09fa8da64c76b6bbe3e44768ae63",
            "ab2cc26d2a494019b7da48ee2896c89c"
          ]
        },
        "id": "By1R6kDWA0kt",
        "outputId": "4496cb87-bc03-4af5-d21b-2f05dd39d1eb"
      },
      "outputs": [],
      "source": [
        "# TRY\n",
        "\n",
        "#@title DOWNLOAD MODELS â€“ LLM, Embedding, Reranker (Qwen3)\n",
        "from agnostic_agent import prepare_qwen_models\n",
        "\n",
        "# IDs de modelos (puedes ajustarlos)\n",
        "LLM_MODEL_ID = \"Qwen/Qwen3-0.6B\"  #@param [\"Qwen/Qwen3-0.6B\", \"Qwen/Qwen3-0.6B-Base\", \"Qwen/Qwen3-4B\", \"Qwen/Qwen3-4B-Instruct-2507\", \"Qwen/Qwen3-4B-Thinking-2507\"]\n",
        "\n",
        "EMB_MODEL_ID = \"Qwen/Qwen3-Embedding-0.6B\"  #@param [\"Qwen/Qwen3-Embedding-0.6B\"]\n",
        "\n",
        "RERANK_MODEL_ID = \"Qwen/Qwen3-Reranker-0.6B\"  #@param [\"Qwen/Qwen3-Reranker-0.6B\"]\n",
        "\n",
        "print(\"â¬‡ï¸ Descargando / preparando modelos Qwen3 (prepare_qwen_models)...\")\n",
        "model_paths = prepare_qwen_models(\n",
        "    llm_model_id=LLM_MODEL_ID,\n",
        "    emb_model_id=EMB_MODEL_ID,\n",
        "    rerank_model_id=RERANK_MODEL_ID,\n",
        "    base_dir=\"LM_MODEL\",\n",
        ")\n",
        "print(\"\\nâœ… Modelos listos en disco:\")\n",
        "print(\"  LLM   :\", model_paths.llm_dir)\n",
        "print(\"  EMB   :\", model_paths.emb_dir)\n",
        "print(\"  RERANK:\", model_paths.rerank_dir)\n",
        "\n",
        "import os\n",
        "os.environ[\"LLM_MODEL_ID\"]   = LLM_MODEL_ID\n",
        "os.environ[\"EMB_MODEL_ID\"]   = EMB_MODEL_ID\n",
        "os.environ[\"RERANK_MODEL_ID\"] = RERANK_MODEL_ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "eu9RrulDBD0N",
        "outputId": "f078bce8-eef4-4744-d0bd-bec17a83c402"
      },
      "outputs": [],
      "source": [
        "#@title LIMPIEZA LIGERA DE GPU (opcional)\n",
        "import gc\n",
        "try:\n",
        "    import torch\n",
        "    has_torch = True\n",
        "except ImportError:\n",
        "    has_torch = False\n",
        "\n",
        "print(\"ğŸ§¹ Limpiando referencias de Python...\")\n",
        "gc.collect()\n",
        "if has_torch and torch.cuda.is_available():\n",
        "    print(\"ğŸ§  Vaciando cachÃ© CUDA...\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\"â„¹ï¸ Torch CUDA no disponible o sin GPU visible.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9V8pxH2FnJ-",
        "outputId": "c8de88c3-a73e-41e3-ef9d-b839ae9ea233"
      },
      "outputs": [],
      "source": [
        "#@title DEPLOY vLLM SERVER â€“ SOLO LLM (chat) Â· L4/T4-friendly\n",
        "import transformers\n",
        "from agnostic_agent import VllmConfig, start_qwen_vllm_servers\n",
        "\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "\n",
        "VLLM_HOST = os.getenv(\"VLLM_HOST\", \"127.0.0.1\")\n",
        "LLM_PORT = 8000\n",
        "EMB_PORT = 8001\n",
        "RERANK_PORT = 8002\n",
        "VLLM_MODE = \"POWER\"  # \"FAST\", \"MEDIUM\", \"POWER\", \"LIMIT\"\n",
        "\n",
        "print(f\"\\nğŸš€ Lanzando servidor vLLM (start_qwen_vllm_servers)...\")\n",
        "print(f\"ğŸ”§ VLLM_MODE = {VLLM_MODE}\")\n",
        "\n",
        "if VLLM_MODE == \"FAST\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.40,\n",
        "        llm_max_len=2048,\n",
        "        llm_max_num_seqs=4,\n",
        "        emb_gpu_util=0.30,\n",
        "        emb_max_len=512,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.30,\n",
        "        rerank_max_len=512,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "elif VLLM_MODE == \"MEDIUM\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.55,\n",
        "        llm_max_len=4096,\n",
        "        llm_max_num_seqs=6,\n",
        "        emb_gpu_util=0.30,\n",
        "        emb_max_len=512,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.30,\n",
        "        rerank_max_len=512,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "elif VLLM_MODE == \"POWER\":\n",
        "    # Modo POWER â€œcargadoâ€ para L4 + Qwen3-0.6B:\n",
        "    # - MÃ¡s contexto (16k tokens)\n",
        "    # - Menos concurrencia (4 secuencias)\n",
        "    # - GPU util ~0.8 para exprimir la L4 sin matarla\n",
        "    vllm_cfg = VllmConfig(\n",
        "        # config base\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        # language model\n",
        "        llm_gpu_util=0.90,   # antes 0.70\n",
        "        llm_max_len=32768,   # antes 8192  â† clave\n",
        "        llm_max_num_seqs=1,  # antes 8     â† menos batch, mÃ¡s contexto\n",
        "        # embedding model\n",
        "        emb_gpu_util=0.05,\n",
        "        emb_max_len=1024,\n",
        "        emb_max_num_seqs=1,\n",
        "        # reranker model\n",
        "        rerank_gpu_util=0.05,\n",
        "        rerank_max_len=1024,\n",
        "        rerank_max_num_seqs=1,\n",
        "        # config extra\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "\n",
        "elif \"LIMIT\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.85,    # un poco mÃ¡s agresivo\n",
        "        llm_max_len=32768,    # ğŸ”¥ 32k tokens de contexto\n",
        "        llm_max_num_seqs=2,   # menos batch, mÃ¡s contexto por secuencia\n",
        "        emb_gpu_util=0.35,\n",
        "        emb_max_len=1024,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.35,\n",
        "        rerank_max_len=1024,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True, # True | False\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",    # \"qwen3\" | none\n",
        "        start_emb_server=True,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "\n",
        "endpoints, servers = start_qwen_vllm_servers(\n",
        "    model_paths=model_paths,\n",
        "    config=vllm_cfg,\n",
        "    set_env=True,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Servidor vLLM (LLM) listo.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3FxXSB_qyR5"
      },
      "source": [
        "# RUN EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCjT0JZm9zu3",
        "outputId": "ec5c25f2-1ef6-434a-ffe3-43859625992f"
      },
      "outputs": [],
      "source": [
        "#@title streamlit_app.py (From Git)\n",
        "import os, shutil\n",
        "\n",
        "if os.path.exists(\"repo_agnostic/streamlit_app.py\"):\n",
        "    shutil.copy(\"repo_agnostic/streamlit_app.py\", \".\")\n",
        "    print(\"âœ… streamlit_app.py copied from repo_agnostic\")\n",
        "else:\n",
        "    print(\"âŒ ERROR: streamlit_app.py not found in repo_agnostic\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0TKvBKKbtti"
      },
      "outputs": [],
      "source": [
        "#@title .streamlit/config.toml (From Git)\n",
        "import os, shutil\n",
        "\n",
        "!mkdir -p .streamlit\n",
        "if os.path.exists(\"repo_agnostic/.streamlit/config.toml\"):\n",
        "    shutil.copy(\"repo_agnostic/.streamlit/config.toml\", \".streamlit/config.toml\")\n",
        "    print(\"âœ… .streamlit/config.toml copied from repo_agnostic\")\n",
        "else:\n",
        "    print(\"âŒ ERROR: .streamlit/config.toml not found in repo_agnostic\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Lctc9Zb5sd",
        "outputId": "61a2ab98-c2a5-468b-dc77-1725b046a6a0"
      },
      "outputs": [],
      "source": [
        "#@title .streamlit/config.toml (From Git)\n",
        "import os, shutil\n",
        "\n",
        "!mkdir -p .streamlit\n",
        "if os.path.exists(\"repo_agnostic/.streamlit/config.toml\"):\n",
        "    shutil.copy(\"repo_agnostic/.streamlit/config.toml\", \".streamlit/config.toml\")\n",
        "    print(\"âœ… .streamlit/config.toml copied from repo_agnostic\")\n",
        "else:\n",
        "    print(\"âŒ ERROR: .streamlit/config.toml not found in repo_agnostic\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "bx-fpi8XBHZt",
        "outputId": "dd69dbda-7858-4756-d9d6-ee19adb65acc"
      },
      "outputs": [],
      "source": [
        "#@title Lanzar servicio Streamlit en Colab (window o iframe)\n",
        "import subprocess, time, os\n",
        "\n",
        "# ---- PARAMS (Colab UI) ----\n",
        "PORT = 8501  #@param {type:\"integer\"}\n",
        "MODE = \"iframe\"  #@param [\"window\", \"iframe\"]\n",
        "IFRAME_HEIGHT = 800  #@param {type:\"integer\"}\n",
        "IFRAME_WIDTH = \"100%\"  #@param {type:\"string\"}\n",
        "\n",
        "# ---- Colab output (solo si existe) ----\n",
        "try:\n",
        "    from google.colab import output\n",
        "    _IN_COLAB = True\n",
        "except Exception:\n",
        "    output = None\n",
        "    _IN_COLAB = False\n",
        "\n",
        "# ---- Mata procesos previos (opcional) ----\n",
        "os.system('pkill -f \"streamlit run streamlit_app.py\" || true')\n",
        "os.system('pkill -f \"streamlit\" || true')\n",
        "\n",
        "# ---- Lanza Streamlit ----\n",
        "cmd = [\n",
        "    \"streamlit\", \"run\", \"streamlit_app.py\",\n",
        "    \"--server.address\", \"0.0.0.0\",\n",
        "    \"--server.port\", str(PORT),\n",
        "    \"--server.headless\", \"true\",\n",
        "    \"--server.enableCORS\", \"false\",\n",
        "    \"--server.enableXsrfProtection\", \"false\",\n",
        "]\n",
        "\n",
        "streamlit_process = subprocess.Popen(cmd)\n",
        "time.sleep(3)\n",
        "\n",
        "# ---- Exponer ----\n",
        "if _IN_COLAB and output is not None:\n",
        "    if MODE == \"iframe\":\n",
        "        output.serve_kernel_port_as_iframe(\n",
        "            PORT,\n",
        "            width=IFRAME_WIDTH,\n",
        "            height=IFRAME_HEIGHT,\n",
        "        )\n",
        "    else:\n",
        "        output.serve_kernel_port_as_window(\n",
        "            PORT,\n",
        "            anchor_text=\"ğŸ”— Abrir app Streamlit en una nueva pestaÃ±a\"\n",
        "        )\n",
        "else:\n",
        "    print(f\"Streamlit corriendo en http://127.0.0.1:{PORT} (no-Colab).\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RFqZU7tnBLHH",
        "kpKY9D-_A1e0"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}