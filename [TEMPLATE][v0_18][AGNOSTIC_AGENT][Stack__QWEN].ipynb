{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqZU7tnBLHH"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zbpyIXulE3sS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m87.9/87.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.4/948.4 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m509.2/509.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m99.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m405.5/405.5 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m323.5/323.5 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.4/72.4 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m47.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.0/388.0 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m285.4/285.4 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m149.0/149.0 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.9/224.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m180.7/180.7 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m105.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.6/58.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m821.6/821.6 kB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m959.8/959.8 kB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.5 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.5 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Building wheel for langchain-qwq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m120.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m151.6/151.6 kB\u001b[0m \u001b[31m584.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hâœ… Dependencias base instaladas:\n",
            "   - vLLM / Qwen wrapper / LangChain / LangGraph\n",
            "   - Pydantic 2.x + PyYAML (setup.yaml como panel de control)\n",
            "   - Streamlit\n",
            "   - sqlite-vec + pandas + numpy para KB tabular/vectorial sobre SQLite\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# @title INSTALL â€“ Agnostic Agent (GitHub) + Infra Qwen3\n",
        "# ============================================================\n",
        "# 1. SUPER CLEANUP (Delete conflicting folders)\n",
        "!rm -rf agnostic_agent  # <--- Deletes the lingering folder that causes ImportError\n",
        "!rm -rf repo_agnostic   # <--- Clean start\n",
        "\n",
        "# 2. CLONE REPO (Into 'repo_agnostic' to avoid any persistent name clash)\n",
        "!git clone https://github.com/JacoboGGLeon/agnostic_agent.git repo_agnostic\n",
        "!pip install -e repo_agnostic\n",
        "\n",
        "# 3. FIX PATH (Tell Python where the package is)\n",
        "import sys, os\n",
        "pkg_path = os.path.abspath(\"repo_agnostic\")\n",
        "if pkg_path not in sys.path: sys.path.insert(0, pkg_path)\n",
        "\n",
        "# 4. OTHER DEPENDENCIES\n",
        "!pip -q install \"vllm>=0.9.0\" \"huggingface_hub>=0.23.0\" \"openai==1.108.2\"\n",
        "!pip -q install langchain langchain-core langchain-openai langgraph\n",
        "!pip -q install \"git+https://github.com/whynpc9/langchain-qwq-vllm.git\"\n",
        "!pip -q install \"pydantic>=2.7.0\" \"pyyaml>=6.0\" streamlit sqlite-vec pandas numpy\n",
        "\n",
        "print(\"âœ… Agnostic Agent INSTALLED from GitHub!\")\n",
        "print(\"ğŸ“‚ Repo Path: repo_agnostic\")\n",
        "print(\"âš ï¸ IMPORTANT: If imports fail, go to Runtime -> Restart session, then run DOWNLOAD MODELS directly.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 16\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:41 .\n",
            "drwxr-xr-x 1 root root 4096 Feb  6 19:38 ..\n",
            "drwxr-xr-x 4 root root 4096 Dec  9 14:41 .config\n",
            "drwxr-xr-x 1 root root 4096 Dec  9 14:42 sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls -la"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpKY9D-_A1e0"
      },
      "source": [
        "# DEFINE vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "70ea88c22d4f4b5bba014abb3f349591",
            "530a8ebc39f44d068713c14806ca235f",
            "1d66561f7a504a038d16f2c1b27e2198",
            "f25e85518d144a7cb3cd4ff520999f30",
            "99b11010c8e948c59306857eb91c6eb7",
            "d0e54c64b1c54cb185b3a2fce0bfd3a5",
            "a30827a944074bf492ceda605fde31de",
            "fdc05ca4bf1e4a809457dac153eda61b",
            "67321bbe29544fbbb280f57c70661bcf",
            "49d5e0bf3e714140b47de7e26e70524a",
            "a9b1a7770a894986ab02e71ca54ccc8f",
            "903d102f8fd046888b325c2bd30d723b",
            "03bdbb86346f47f6a861955e71aec295",
            "b347abe7783041b39c00c9713f0515c3",
            "94f3efccfa8041d4ad14ed288783883f",
            "28bf413b76644b108b28d50aecaf6f87",
            "49714a558a7b4d3185806b88fcb3ce5b",
            "5a1462f7a41941079e1cc03c8639695e",
            "cc334d03910b4fa19af1f86e7339c3f1",
            "82db5b96e80849d59bb1e6bcaac9ebf4",
            "3bedc8b346ec499db047c563ebc5a42a",
            "03f1dee8ad8848318261057f66159549",
            "4b10f9de7b9a4e89b103bbb2b9a8e30e",
            "634195a2bf13459ead87450c6a8c634b",
            "867f0c587dd5494aa4d2b0d42cb6f6c9",
            "568c99c13aea4d85bc9ac278820d9206",
            "d09fa840319943898a93e989f3d40e05",
            "7679be21c9fb41b9bea0fc0f1e0591f2",
            "ee7b09fa8da64c76b6bbe3e44768ae63",
            "ab2cc26d2a494019b7da48ee2896c89c"
          ]
        },
        "id": "By1R6kDWA0kt",
        "outputId": "4496cb87-bc03-4af5-d21b-2f05dd39d1eb"
      },
      "outputs": [],
      "source": [
        "# TRY\n",
        "\n",
        "#@title DOWNLOAD MODELS â€“ LLM, Embedding, Reranker (Qwen3)\n",
        "from agnostic_agent import prepare_qwen_models\n",
        "\n",
        "# IDs de modelos (puedes ajustarlos)\n",
        "LLM_MODEL_ID = \"Qwen/Qwen3-0.6B\"  #@param [\"Qwen/Qwen3-0.6B\", \"Qwen/Qwen3-0.6B-Base\", \"Qwen/Qwen3-4B\", \"Qwen/Qwen3-4B-Instruct-2507\", \"Qwen/Qwen3-4B-Thinking-2507\"]\n",
        "\n",
        "EMB_MODEL_ID = \"Qwen/Qwen3-Embedding-0.6B\"  #@param [\"Qwen/Qwen3-Embedding-0.6B\"]\n",
        "\n",
        "RERANK_MODEL_ID = \"Qwen/Qwen3-Reranker-0.6B\"  #@param [\"Qwen/Qwen3-Reranker-0.6B\"]\n",
        "\n",
        "print(\"â¬‡ï¸ Descargando / preparando modelos Qwen3 (prepare_qwen_models)...\")\n",
        "model_paths = prepare_qwen_models(\n",
        "    llm_model_id=LLM_MODEL_ID,\n",
        "    emb_model_id=EMB_MODEL_ID,\n",
        "    rerank_model_id=RERANK_MODEL_ID,\n",
        "    base_dir=\"LM_MODEL\",\n",
        ")\n",
        "print(\"\\nâœ… Modelos listos en disco:\")\n",
        "print(\"  LLM   :\", model_paths.llm_dir)\n",
        "print(\"  EMB   :\", model_paths.emb_dir)\n",
        "print(\"  RERANK:\", model_paths.rerank_dir)\n",
        "\n",
        "import os\n",
        "os.environ[\"LLM_MODEL_ID\"]   = LLM_MODEL_ID\n",
        "os.environ[\"EMB_MODEL_ID\"]   = EMB_MODEL_ID\n",
        "os.environ[\"RERANK_MODEL_ID\"] = RERANK_MODEL_ID\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "eu9RrulDBD0N",
        "outputId": "f078bce8-eef4-4744-d0bd-bec17a83c402"
      },
      "outputs": [],
      "source": [
        "#@title LIMPIEZA LIGERA DE GPU (opcional)\n",
        "import gc\n",
        "try:\n",
        "    import torch\n",
        "    has_torch = True\n",
        "except ImportError:\n",
        "    has_torch = False\n",
        "\n",
        "print(\"ğŸ§¹ Limpiando referencias de Python...\")\n",
        "gc.collect()\n",
        "if has_torch and torch.cuda.is_available():\n",
        "    print(\"ğŸ§  Vaciando cachÃ© CUDA...\")\n",
        "    torch.cuda.empty_cache()\n",
        "else:\n",
        "    print(\"â„¹ï¸ Torch CUDA no disponible o sin GPU visible.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9V8pxH2FnJ-",
        "outputId": "c8de88c3-a73e-41e3-ef9d-b839ae9ea233"
      },
      "outputs": [],
      "source": [
        "#@title DEPLOY vLLM SERVER â€“ SOLO LLM (chat) Â· L4/T4-friendly\n",
        "import transformers\n",
        "from agnostic_agent import VllmConfig, start_qwen_vllm_servers\n",
        "\n",
        "print(\"Transformers version:\", transformers.__version__)\n",
        "\n",
        "VLLM_HOST = os.getenv(\"VLLM_HOST\", \"127.0.0.1\")\n",
        "LLM_PORT = 8000\n",
        "EMB_PORT = 8001\n",
        "RERANK_PORT = 8002\n",
        "VLLM_MODE = \"POWER\"  # \"FAST\", \"MEDIUM\", \"POWER\", \"LIMIT\"\n",
        "\n",
        "print(f\"\\nğŸš€ Lanzando servidor vLLM (start_qwen_vllm_servers)...\")\n",
        "print(f\"ğŸ”§ VLLM_MODE = {VLLM_MODE}\")\n",
        "\n",
        "if VLLM_MODE == \"FAST\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.40,\n",
        "        llm_max_len=2048,\n",
        "        llm_max_num_seqs=4,\n",
        "        emb_gpu_util=0.30,\n",
        "        emb_max_len=512,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.30,\n",
        "        rerank_max_len=512,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "elif VLLM_MODE == \"MEDIUM\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.55,\n",
        "        llm_max_len=4096,\n",
        "        llm_max_num_seqs=6,\n",
        "        emb_gpu_util=0.30,\n",
        "        emb_max_len=512,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.30,\n",
        "        rerank_max_len=512,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "elif VLLM_MODE == \"POWER\":\n",
        "    # Modo POWER â€œcargadoâ€ para L4 + Qwen3-0.6B:\n",
        "    # - MÃ¡s contexto (16k tokens)\n",
        "    # - Menos concurrencia (4 secuencias)\n",
        "    # - GPU util ~0.8 para exprimir la L4 sin matarla\n",
        "    vllm_cfg = VllmConfig(\n",
        "        # config base\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        # language model\n",
        "        llm_gpu_util=0.90,   # antes 0.70\n",
        "        llm_max_len=32768,   # antes 8192  â† clave\n",
        "        llm_max_num_seqs=1,  # antes 8     â† menos batch, mÃ¡s contexto\n",
        "        # embedding model\n",
        "        emb_gpu_util=0.05,\n",
        "        emb_max_len=1024,\n",
        "        emb_max_num_seqs=1,\n",
        "        # reranker model\n",
        "        rerank_gpu_util=0.05,\n",
        "        rerank_max_len=1024,\n",
        "        rerank_max_num_seqs=1,\n",
        "        # config extra\n",
        "        enable_reasoning=True,\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",\n",
        "        start_emb_server=False,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "\n",
        "elif \"LIMIT\":\n",
        "    vllm_cfg = VllmConfig(\n",
        "        host=VLLM_HOST,\n",
        "        llm_port=LLM_PORT,\n",
        "        emb_port=EMB_PORT,\n",
        "        rerank_port=RERANK_PORT,\n",
        "        llm_gpu_util=0.85,    # un poco mÃ¡s agresivo\n",
        "        llm_max_len=32768,    # ğŸ”¥ 32k tokens de contexto\n",
        "        llm_max_num_seqs=2,   # menos batch, mÃ¡s contexto por secuencia\n",
        "        emb_gpu_util=0.35,\n",
        "        emb_max_len=1024,\n",
        "        emb_max_num_seqs=4,\n",
        "        rerank_gpu_util=0.35,\n",
        "        rerank_max_len=1024,\n",
        "        rerank_max_num_seqs=4,\n",
        "        enable_reasoning=True, # True | False\n",
        "        tool_call_parser=\"hermes\",\n",
        "        reasoning_parser=\"qwen3\",    # \"qwen3\" | none\n",
        "        start_emb_server=True,\n",
        "        start_rerank_server=False,\n",
        "    )\n",
        "\n",
        "endpoints, servers = start_qwen_vllm_servers(\n",
        "    model_paths=model_paths,\n",
        "    config=vllm_cfg,\n",
        "    set_env=True,\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… Servidor vLLM (LLM) listo.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3FxXSB_qyR5"
      },
      "source": [
        "# RUN EXPERIMENT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCjT0JZm9zu3",
        "outputId": "ec5c25f2-1ef6-434a-ffe3-43859625992f"
      },
      "outputs": [],
      "source": [
        "#@title streamlit_app.py\n",
        "%%writefile streamlit_app.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "import html\n",
        "from typing import Any, Dict, Optional, List, Tuple\n",
        "\n",
        "import streamlit as st\n",
        "from agnostic_agent.agent import Agent\n",
        "\n",
        "# -----------------------------\n",
        "# Page\n",
        "# -----------------------------\n",
        "st.set_page_config(\n",
        "    page_title=\"Agnostic Agent Â· Chat Studio (Inspector)\",\n",
        "    page_icon=\"ğŸ§ª\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Hide Streamlit chrome (dark cintillo)\n",
        "# -----------------------------\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        "[data-testid=\"stHeader\"] { display: none !important; }\n",
        "[data-testid=\"stToolbar\"] { display: none !important; }\n",
        "[data-testid=\"stDecoration\"] { display: none !important; }\n",
        "#MainMenu { visibility: hidden !important; }\n",
        "footer { visibility: hidden !important; }\n",
        ".block-container { padding-top: 1rem !important; }\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# -----------------------------\n",
        "# CSS (Studio + Inspector layout)\n",
        "# -----------------------------\n",
        "st.markdown(\n",
        "    \"\"\"\n",
        "<style>\n",
        ":root{\n",
        "  --bg: #0b1020;\n",
        "  --panel: rgba(255,255,255,.06);\n",
        "  --panel2: rgba(255,255,255,.08);\n",
        "  --border: rgba(255,255,255,.10);\n",
        "  --text: rgba(255,255,255,.92);\n",
        "  --muted: rgba(255,255,255,.65);\n",
        "  --accent: #7c5cff;\n",
        "  --good: #2dd4bf;\n",
        "\n",
        "  --r: 18px;\n",
        "  --r2: 14px;\n",
        "  --shadow: 0 12px 35px rgba(0,0,0,.35);\n",
        "  --mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", \"Courier New\", monospace;\n",
        "}\n",
        "\n",
        ".stApp{\n",
        "  background:\n",
        "    radial-gradient(1200px 500px at 10% -10%, rgba(124,92,255,.35), transparent 60%),\n",
        "    radial-gradient(900px 500px at 90% 0%, rgba(45,212,191,.18), transparent 60%),\n",
        "    linear-gradient(180deg, var(--bg), #070a14 60%, #050712);\n",
        "  color: var(--text);\n",
        "}\n",
        "\n",
        ".block-container{ padding-top: 1.0rem; padding-bottom: 1.6rem; }\n",
        "\n",
        "section[data-testid=\"stSidebar\"]{\n",
        "  background: rgba(0,0,0,.18);\n",
        "  border-right: 1px solid rgba(255,255,255,.06);\n",
        "}\n",
        "\n",
        ".topbar{\n",
        "  display:flex; align-items:center; justify-content:space-between;\n",
        "  gap:12px;\n",
        "  padding: 12px 14px;\n",
        "  border-radius: var(--r);\n",
        "  background: linear-gradient(180deg, rgba(255,255,255,.08), rgba(255,255,255,.05));\n",
        "  border: 1px solid var(--border);\n",
        "  box-shadow: var(--shadow);\n",
        "  margin-bottom: 10px;\n",
        "}\n",
        ".brand{display:flex; align-items:center; gap:10px;}\n",
        ".logo{\n",
        "  width: 38px; height: 38px; border-radius: 12px;\n",
        "  display:flex; align-items:center; justify-content:center;\n",
        "  background: linear-gradient(135deg, rgba(124,92,255,.9), rgba(45,212,191,.6));\n",
        "  box-shadow: 0 10px 25px rgba(124,92,255,.22);\n",
        "  font-size: 18px;\n",
        "}\n",
        ".title{font-size: 15px; font-weight: 800; line-height: 1.1;}\n",
        ".subtitle{font-size: 12px; color: var(--muted);}\n",
        "\n",
        ".badges{display:flex; flex-wrap:wrap; gap:8px; justify-content:flex-end;}\n",
        ".badge{\n",
        "  font-size: 12px;\n",
        "  padding: 6px 10px;\n",
        "  border-radius: 999px;\n",
        "  border: 1px solid var(--border);\n",
        "  background: rgba(255,255,255,.06);\n",
        "  color: var(--text);\n",
        "}\n",
        ".badge.accent{ border-color: rgba(124,92,255,.45); }\n",
        ".badge.good{ border-color: rgba(45,212,191,.45); }\n",
        "\n",
        ".card{\n",
        "  border-radius: var(--r);\n",
        "  border: 1px solid var(--border);\n",
        "  background: rgba(255,255,255,.06);\n",
        "  box-shadow: 0 10px 30px rgba(0,0,0,.28);\n",
        "  overflow: hidden;\n",
        "  margin-top: 6px;\n",
        "}\n",
        ".card .card-h{\n",
        "  display:flex; align-items:center; justify-content:space-between;\n",
        "  gap:10px;\n",
        "  padding: 10px 12px;\n",
        "  background: rgba(255,255,255,.05);\n",
        "  border-bottom: 1px solid var(--border);\n",
        "  font-weight: 800;\n",
        "  font-size: 13px;\n",
        "}\n",
        ".card .card-h .hint{\n",
        "  font-weight: 500; font-size: 11px; color: var(--muted);\n",
        "}\n",
        ".card .card-b{\n",
        "  padding: 12px 12px 10px 12px;\n",
        "  font-size: 14px;\n",
        "  color: var(--text);\n",
        "}\n",
        "\n",
        "/* Code-like block inside cards (for Thinking) */\n",
        ".codebox{\n",
        "  margin-top: 8px;\n",
        "  padding: 10px 12px;\n",
        "  border-radius: 14px;\n",
        "  border: 1px solid rgba(255,255,255,.10);\n",
        "  background: rgba(0,0,0,.28);\n",
        "  font-family: var(--mono);\n",
        "  font-size: 12px;\n",
        "  line-height: 1.45;\n",
        "  white-space: pre-wrap;\n",
        "  word-break: break-word;\n",
        "  color: rgba(255,255,255,.92);\n",
        "}\n",
        "\n",
        "/* User bubble */\n",
        ".bubble-user{\n",
        "  padding: 10px 12px;\n",
        "  border-radius: 16px;\n",
        "  border: 1px solid rgba(124,92,255,.35);\n",
        "  background: linear-gradient(180deg, rgba(124,92,255,.22), rgba(255,255,255,.05));\n",
        "  box-shadow: 0 8px 24px rgba(0,0,0,.25);\n",
        "}\n",
        "\n",
        "/* Inspector wrapper */\n",
        ".inspector{\n",
        "  border-radius: var(--r);\n",
        "  border: 1px solid var(--border);\n",
        "  background: rgba(255,255,255,.05);\n",
        "  box-shadow: var(--shadow);\n",
        "  padding: 12px;\n",
        "}\n",
        ".inspector h3{ margin: 0 0 6px 0; }\n",
        "\n",
        "/* Expanders */\n",
        "[data-testid=\"stExpander\"]{\n",
        "  border-radius: var(--r);\n",
        "  border: 1px solid var(--border);\n",
        "  background: rgba(255,255,255,.04);\n",
        "  overflow:hidden;\n",
        "}\n",
        "\n",
        "/* Chat spacing */\n",
        "[data-testid=\"stChatMessage\"]{\n",
        "  padding-top: 0.25rem;\n",
        "  padding-bottom: 0.25rem;\n",
        "}\n",
        "</style>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Session state\n",
        "# -----------------------------\n",
        "if \"agent\" not in st.session_state:\n",
        "    st.session_state.agent = None\n",
        "if \"agent_mode\" not in st.session_state:\n",
        "    st.session_state.agent_mode = \"tools_strict\"\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = []\n",
        "if \"msg_counter\" not in st.session_state:\n",
        "    st.session_state.msg_counter = 0\n",
        "if \"selected_msg_id\" not in st.session_state:\n",
        "    st.session_state.selected_msg_id = None\n",
        "if \"export_json\" not in st.session_state:\n",
        "    st.session_state.export_json = None\n",
        "\n",
        "# -----------------------------\n",
        "# Sidebar controls\n",
        "# -----------------------------\n",
        "with st.sidebar:\n",
        "    st.markdown(\"## ğŸ§ª Chat Studio\")\n",
        "    agent_mode = st.selectbox(\n",
        "        \"Policy mode\",\n",
        "        [\"tools_strict\", \"free_policies\"],\n",
        "        index=0 if st.session_state.agent_mode == \"tools_strict\" else 1,\n",
        "    )\n",
        "\n",
        "    st.markdown(\"### ğŸ§­ Inspector\")\n",
        "    show_thinking_tab = st.checkbox(\"ğŸ§  Thinking\", value=True)\n",
        "    show_deep_tab = st.checkbox(\"ğŸ§  Deep\", value=True)\n",
        "    show_dev_tab = st.checkbox(\"ğŸ” Dev\", value=True)\n",
        "\n",
        "    st.markdown(\"### ğŸ§¹ Acciones\")\n",
        "    cA, cB = st.columns(2)\n",
        "    with cA:\n",
        "        if st.button(\"ğŸ—‘ï¸ Limpiar\", use_container_width=True):\n",
        "            st.session_state.messages = []\n",
        "            st.session_state.agent = None\n",
        "            st.session_state.selected_msg_id = None\n",
        "            st.toast(\"Chat reiniciado.\", icon=\"ğŸ§¹\")\n",
        "            st.rerun()\n",
        "    with cB:\n",
        "        if st.button(\"â¬‡ï¸ Export\", use_container_width=True):\n",
        "            export = {\"agent_mode\": st.session_state.agent_mode, \"messages\": st.session_state.messages}\n",
        "            st.session_state.export_json = json.dumps(export, ensure_ascii=False, indent=2)\n",
        "            st.toast(\"Transcript listo.\", icon=\"â¬‡ï¸\")\n",
        "\n",
        "    if isinstance(st.session_state.export_json, str):\n",
        "        st.download_button(\n",
        "            \"Descargar transcript.json\",\n",
        "            data=st.session_state.export_json,\n",
        "            file_name=\"transcript.json\",\n",
        "            mime=\"application/json\",\n",
        "            use_container_width=True,\n",
        "        )\n",
        "\n",
        "# Mode change => reset agent (history stays)\n",
        "if st.session_state.agent_mode != agent_mode:\n",
        "    st.session_state.agent_mode = agent_mode\n",
        "    st.session_state.agent = None\n",
        "    st.toast(f\"Modo: {agent_mode}\", icon=\"ğŸ§­\")\n",
        "\n",
        "# -----------------------------\n",
        "# Helpers\n",
        "# -----------------------------\n",
        "def next_id() -> int:\n",
        "    st.session_state.msg_counter += 1\n",
        "    return st.session_state.msg_counter\n",
        "\n",
        "def get_or_init_agent(mode: str) -> Agent:\n",
        "    if st.session_state.agent is None:\n",
        "        os.environ[\"PLANNER_POLICY_MODE\"] = mode\n",
        "        os.environ[\"AGENT_POLICY_MODE\"] = mode\n",
        "        with st.spinner(f\"Inicializando agente (mode={mode})â€¦\"):\n",
        "            try:\n",
        "                st.session_state.agent = Agent.init(policy_mode=mode)\n",
        "            except TypeError:\n",
        "                st.session_state.agent = Agent.init()\n",
        "    return st.session_state.agent\n",
        "\n",
        "def normalize_output(raw: Any) -> Dict[str, Any]:\n",
        "    if raw is None:\n",
        "        return {}\n",
        "    if hasattr(raw, \"model_dump\"):\n",
        "        try:\n",
        "            return raw.model_dump()\n",
        "        except TypeError:\n",
        "            pass\n",
        "    if isinstance(raw, dict):\n",
        "        return raw\n",
        "    return {\"user_out\": str(raw)}\n",
        "\n",
        "def as_text(v: Any) -> str:\n",
        "    if v is None:\n",
        "        return \"\"\n",
        "    if isinstance(v, str):\n",
        "        return v.strip()\n",
        "    if isinstance(v, dict):\n",
        "        for k in (\"final_answer\", \"text\", \"content\", \"answer\", \"user_out\"):\n",
        "            vv = v.get(k)\n",
        "            if isinstance(vv, str) and vv.strip():\n",
        "                return vv.strip()\n",
        "        return \"\"\n",
        "    return str(v).strip()\n",
        "\n",
        "def strip_user_prefix(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    t = text.strip()\n",
        "    prefixes = [\n",
        "        \"Respuesta final (modo usuario):\",\n",
        "        \"**Respuesta final (modo usuario):**\",\n",
        "        \"RESPUESTA FINAL (modo usuario):\",\n",
        "    ]\n",
        "    for p in prefixes:\n",
        "        if t.startswith(p):\n",
        "            t = t[len(p):].strip()\n",
        "    return t\n",
        "\n",
        "def get_raw_state(out: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
        "    if not isinstance(out, dict):\n",
        "        return None\n",
        "    if isinstance(out.get(\"messages\"), list):\n",
        "        return out\n",
        "    rs = out.get(\"raw_state\")\n",
        "    if isinstance(rs, dict):\n",
        "        return rs\n",
        "    dev = out.get(\"dev_out\")\n",
        "    if isinstance(dev, dict) and isinstance(dev.get(\"raw_state\"), dict):\n",
        "        return dev[\"raw_state\"]\n",
        "    return None\n",
        "\n",
        "def extract_thinking(raw_state: Optional[Dict[str, Any]]) -> str:\n",
        "    if not isinstance(raw_state, dict):\n",
        "        return \"\"\n",
        "    msgs = raw_state.get(\"messages\")\n",
        "    if not isinstance(msgs, list):\n",
        "        return \"\"\n",
        "    for m in reversed(msgs):\n",
        "        if not isinstance(m, dict):\n",
        "            continue\n",
        "        if m.get(\"type\") != \"ai\":\n",
        "            continue\n",
        "        ak = m.get(\"additional_kwargs\") or {}\n",
        "        if isinstance(ak, dict) and ak.get(\"pipeline_internal\"):\n",
        "            continue\n",
        "        thinking = ak.get(\"reasoning_content\") or ak.get(\"reasoning\") or ak.get(\"thoughts\") or \"\"\n",
        "        return thinking.strip() if isinstance(thinking, str) else \"\"\n",
        "    return \"\"\n",
        "\n",
        "def extract_summary_deep(raw_state: Optional[Dict[str, Any]], deep_out_text: str) -> str:\n",
        "    if deep_out_text:\n",
        "        return deep_out_text\n",
        "    if not isinstance(raw_state, dict):\n",
        "        return \"\"\n",
        "    summary = raw_state.get(\"summary\") or raw_state.get(\"pipeline_summary\")\n",
        "    if not isinstance(summary, dict):\n",
        "        return \"\"\n",
        "    parts = []\n",
        "    for k in [\"analyzer\", \"planner\", \"executor\", \"catcher\", \"summarizer\", \"final_answer\"]:\n",
        "        v = summary.get(k, \"\")\n",
        "        if isinstance(v, str) and v.strip():\n",
        "            parts.append(f\"**{k.upper()}**\\n\\n{v.strip()}\")\n",
        "    return \"\\n\\n---\\n\\n\".join(parts) if parts else \"\"\n",
        "\n",
        "def extract_tool_runs(out: Dict[str, Any], raw_state: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    if isinstance(raw_state, dict):\n",
        "        tr = raw_state.get(\"tool_runs\")\n",
        "        if isinstance(tr, list):\n",
        "            return tr\n",
        "    tr2 = out.get(\"tool_runs\")\n",
        "    if isinstance(tr2, list):\n",
        "        return tr2\n",
        "    dev = out.get(\"dev_out\")\n",
        "    if isinstance(dev, dict) and isinstance(dev.get(\"tool_runs\"), list):\n",
        "        return dev[\"tool_runs\"]\n",
        "    return []\n",
        "\n",
        "def assistant_messages() -> List[Dict[str, Any]]:\n",
        "    return [m for m in st.session_state.messages if m.get(\"role\") == \"assistant\"]\n",
        "\n",
        "def find_message_by_id(msg_id: Optional[int]) -> Optional[Dict[str, Any]]:\n",
        "    if msg_id is None:\n",
        "        return None\n",
        "    for m in st.session_state.messages:\n",
        "        if m.get(\"id\") == msg_id:\n",
        "            return m\n",
        "    return None\n",
        "\n",
        "def default_selected_id() -> Optional[int]:\n",
        "    a = assistant_messages()\n",
        "    return a[-1][\"id\"] if a else None\n",
        "\n",
        "def card_md(title: str, body_md: str, icon: str = \"â¬›\", hint: str = \"\") -> None:\n",
        "    body_md = body_md or \"_(vacÃ­o)_\"\n",
        "    hint_html = f'<span class=\"hint\">{html.escape(hint)}</span>' if hint else \"\"\n",
        "    # NOTE: body_md here is treated as plain HTML content; for Deep this is OK.\n",
        "    # For Thinking we use code-card below so it looks like \"markdown blocks\".\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "<div class=\"card\">\n",
        "  <div class=\"card-h\">\n",
        "    <div>{icon} {html.escape(title)}</div>\n",
        "    {hint_html}\n",
        "  </div>\n",
        "  <div class=\"card-b\">{body_md}</div>\n",
        "</div>\n",
        "\"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "def card_code(title: str, code_text: str, icon: str = \"ğŸ§ \", hint: str = \"reasoning_content\") -> None:\n",
        "    safe = html.escape(code_text or \"\")\n",
        "    hint_html = f'<span class=\"hint\">{html.escape(hint)}</span>' if hint else \"\"\n",
        "    content = safe if safe.strip() else html.escape(\"_(no viene thinking en este turno)_\")\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "<div class=\"card\">\n",
        "  <div class=\"card-h\">\n",
        "    <div>{icon} {html.escape(title)}</div>\n",
        "    {hint_html}\n",
        "  </div>\n",
        "  <div class=\"card-b\">\n",
        "    <div class=\"codebox\">{content}</div>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "def render_tool_runs(tool_runs: List[Dict[str, Any]]) -> None:\n",
        "    if not tool_runs:\n",
        "        st.markdown(\"_(No se ejecutaron tools en este turno.)_\")\n",
        "        return\n",
        "    st.markdown(\"#### ğŸ›  Tools ejecutadas\")\n",
        "    for i, tr in enumerate(tool_runs, start=1):\n",
        "        if not isinstance(tr, dict):\n",
        "            st.markdown(f\"**{i}. tool_{i}**\")\n",
        "            st.code(str(tr))\n",
        "            continue\n",
        "        name = tr.get(\"name\", f\"tool_{i}\")\n",
        "        args = tr.get(\"args\", {})\n",
        "        output = tr.get(\"output\", \"\")\n",
        "        st.markdown(f\"**{i}. {name}**\")\n",
        "        if args:\n",
        "            st.code(args, language=\"json\")\n",
        "        if output != \"\":\n",
        "            st.markdown(\"**Salida:**\")\n",
        "            st.code(str(output))\n",
        "\n",
        "# If nothing selected yet, default to last assistant\n",
        "if st.session_state.selected_msg_id is None:\n",
        "    st.session_state.selected_msg_id = default_selected_id()\n",
        "\n",
        "# -----------------------------\n",
        "# Top bar\n",
        "# -----------------------------\n",
        "mode_badge = \"tools_strict\" if st.session_state.agent_mode == \"tools_strict\" else \"free_policies\"\n",
        "mode_class = \"accent\" if st.session_state.agent_mode == \"tools_strict\" else \"good\"\n",
        "\n",
        "st.markdown(\n",
        "    f\"\"\"\n",
        "<div class=\"topbar\">\n",
        "  <div class=\"brand\">\n",
        "    <div class=\"logo\">ğŸ§ª</div>\n",
        "    <div>\n",
        "      <div class=\"title\">Agnostic Agent Â· Chat Studio</div>\n",
        "      <div class=\"subtitle\">Feed limpio + Inspector dinÃ¡mico (thinking/deep/dev)</div>\n",
        "    </div>\n",
        "  </div>\n",
        "  <div class=\"badges\">\n",
        "    <span class=\"badge {mode_class}\">ğŸ§­ {mode_badge}</span>\n",
        "    <span class=\"badge\">ğŸ” inspector: on</span>\n",
        "  </div>\n",
        "</div>\n",
        "\"\"\",\n",
        "    unsafe_allow_html=True,\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Main layout: Feed | Inspector\n",
        "# -----------------------------\n",
        "feed_col, insp_col = st.columns([2.2, 1.0], gap=\"large\")\n",
        "\n",
        "# -------- FEED (left) --------\n",
        "with feed_col:\n",
        "    for msg in st.session_state.messages:\n",
        "        role = msg.get(\"role\", \"user\")\n",
        "\n",
        "        if role == \"user\":\n",
        "            with st.chat_message(\"user\"):\n",
        "                st.markdown(f'<div class=\"bubble-user\">{html.escape(msg.get(\"content\",\"\"))}</div>', unsafe_allow_html=True)\n",
        "\n",
        "        elif role == \"assistant\":\n",
        "            out = msg.get(\"out\") or {}\n",
        "            answer = strip_user_prefix(as_text(out.get(\"user_out\")))\n",
        "            raw_state = get_raw_state(out)\n",
        "            tool_runs = extract_tool_runs(out, raw_state)\n",
        "\n",
        "            with st.chat_message(\"assistant\"):\n",
        "                # Pretty answer only\n",
        "                card_md(\n",
        "                    title=\"Respuesta\",\n",
        "                    body_md=html.escape(answer or \"_(sin respuesta)_\").replace(\"\\n\", \"<br>\"),\n",
        "                    icon=\"ğŸ‘¤\",\n",
        "                    hint=f\"id={msg.get('id')}\",\n",
        "                )\n",
        "\n",
        "                c1, c2, c3 = st.columns([1.2, 1.0, 0.8])\n",
        "                with c1:\n",
        "                    st.caption(f\"ğŸ›  tools: {len(tool_runs)}\")\n",
        "                with c2:\n",
        "                    st.caption(\"ğŸ“ Inspector â†’\")\n",
        "                with c3:\n",
        "                    if st.button(\"ğŸ” Inspect\", key=f\"inspect_{msg.get('id')}\", use_container_width=True):\n",
        "                        st.session_state.selected_msg_id = msg.get(\"id\")\n",
        "                        st.toast(f\"Inspector â†’ id={msg.get('id')}\", icon=\"ğŸ”\")\n",
        "                        st.rerun()\n",
        "\n",
        "# -------- INSPECTOR (right) --------\n",
        "with insp_col:\n",
        "    st.markdown('<div class=\"inspector\">', unsafe_allow_html=True)\n",
        "    st.markdown(\"### ğŸ” Inspector\")\n",
        "\n",
        "    a_msgs = assistant_messages()\n",
        "    if not a_msgs:\n",
        "        st.info(\"AÃºn no hay respuestas del agente. Escribe algo para empezar.\")\n",
        "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "    else:\n",
        "        ids = [m[\"id\"] for m in a_msgs]\n",
        "\n",
        "        def label(mid: int) -> str:\n",
        "            m = find_message_by_id(mid) or {}\n",
        "            out = m.get(\"out\") or {}\n",
        "            text = strip_user_prefix(as_text(out.get(\"user_out\"))).replace(\"\\n\", \" \").strip()\n",
        "            text = (text[:60] + \"â€¦\") if len(text) > 60 else text\n",
        "            return f\"id={mid} Â· {text or '(sin texto)'}\"\n",
        "\n",
        "        if st.session_state.selected_msg_id not in ids:\n",
        "            st.session_state.selected_msg_id = ids[-1]\n",
        "\n",
        "        sel = st.selectbox(\n",
        "            \"Mensaje seleccionado\",\n",
        "            options=ids,\n",
        "            index=ids.index(st.session_state.selected_msg_id),\n",
        "            format_func=label,\n",
        "            key=\"inspector_selectbox\",\n",
        "        )\n",
        "        st.session_state.selected_msg_id = sel\n",
        "\n",
        "        m = find_message_by_id(st.session_state.selected_msg_id) or {}\n",
        "        out = m.get(\"out\") or {}\n",
        "        raw_state = get_raw_state(out)\n",
        "\n",
        "        thinking = extract_thinking(raw_state)\n",
        "        deep_txt = extract_summary_deep(raw_state, as_text(out.get(\"deep_out\")))\n",
        "        tool_runs = extract_tool_runs(out, raw_state)\n",
        "\n",
        "        tab_specs: List[Tuple[str, str]] = []\n",
        "        if show_thinking_tab:\n",
        "            tab_specs.append((\"ğŸ§  Thinking\", \"thinking\"))\n",
        "        if show_deep_tab:\n",
        "            tab_specs.append((\"ğŸ§  Deep\", \"deep\"))\n",
        "        if show_dev_tab:\n",
        "            tab_specs.append((\"ğŸ” Dev\", \"dev\"))\n",
        "\n",
        "        tabs = st.tabs([t[0] for t in tab_specs])\n",
        "\n",
        "        for (tab_title, tab_key), tab in zip(tab_specs, tabs):\n",
        "            with tab:\n",
        "                if tab_key == \"thinking\":\n",
        "                    # âœ… Now it looks like a proper \"markdown code block\" card (monospace, wrapped, bordered)\n",
        "                    card_code(\"Pensamiento (thinking)\", thinking, icon=\"ğŸ§ \", hint=\"reasoning_content\")\n",
        "\n",
        "                elif tab_key == \"deep\":\n",
        "                    # Keep as-is (you said you like it)\n",
        "                    # We keep newlines readable and let emphasis render in markdown-ish style\n",
        "                    body = deep_txt or \"_(vacÃ­o)_\"\n",
        "                    # Render markdown emphasis by replacing to HTML-ish layout; deep usually ok as plain text too\n",
        "                    body_html = html.escape(body).replace(\"\\n\", \"<br>\")\n",
        "                    card_md(\"Vista profunda (deep_out / summary)\", body_html, icon=\"ğŸ§ \", hint=\"pipeline\")\n",
        "\n",
        "                elif tab_key == \"dev\":\n",
        "                    # âœ… Dev: tools like before + raw_state collapsed\n",
        "                    render_tool_runs(tool_runs)\n",
        "\n",
        "                    with st.expander(\"ğŸ§¬ raw_state (debug)\", expanded=False):\n",
        "                        if isinstance(raw_state, dict) and raw_state:\n",
        "                            st.json(raw_state)\n",
        "                        else:\n",
        "                            st.markdown(\"_(sin raw_state)_\")\n",
        "\n",
        "        st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Chat input (bottom)\n",
        "# -----------------------------\n",
        "prompt = st.chat_input(\"Escribe tu mensajeâ€¦\")\n",
        "\n",
        "if prompt:\n",
        "    uid = next_id()\n",
        "    st.session_state.messages.append({\"id\": uid, \"role\": \"user\", \"content\": prompt, \"out\": None})\n",
        "\n",
        "    agent = get_or_init_agent(st.session_state.agent_mode)\n",
        "    try:\n",
        "        raw_out = agent.run_turn(prompt, policy_mode=st.session_state.agent_mode)\n",
        "    except TypeError:\n",
        "        raw_out = agent.run_turn(prompt)\n",
        "\n",
        "    out = normalize_output(raw_out)\n",
        "    aid = next_id()\n",
        "    st.session_state.messages.append(\n",
        "        {\"id\": aid, \"role\": \"assistant\", \"content\": strip_user_prefix(as_text(out.get(\"user_out\"))), \"out\": out}\n",
        "    )\n",
        "\n",
        "    st.session_state.selected_msg_id = aid\n",
        "    st.rerun()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0TKvBKKbtti"
      },
      "outputs": [],
      "source": [
        "#@title .streamlit/config.toml\n",
        "!mkdir -p .streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5Lctc9Zb5sd",
        "outputId": "61a2ab98-c2a5-468b-dc77-1725b046a6a0"
      },
      "outputs": [],
      "source": [
        "%%writefile .streamlit/config.toml\n",
        "[theme]\n",
        "base=\"dark\"\n",
        "primaryColor=\"#7c5cff\"\n",
        "backgroundColor=\"#070a14\"\n",
        "secondaryBackgroundColor=\"#10162b\"\n",
        "textColor=\"#e9e9f2\"\n",
        "font=\"sans serif\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "bx-fpi8XBHZt",
        "outputId": "dd69dbda-7858-4756-d9d6-ee19adb65acc"
      },
      "outputs": [],
      "source": [
        "#@title Lanzar servicio Streamlit en Colab (window o iframe)\n",
        "import subprocess, time, os\n",
        "\n",
        "# ---- PARAMS (Colab UI) ----\n",
        "PORT = 8501  #@param {type:\"integer\"}\n",
        "MODE = \"iframe\"  #@param [\"window\", \"iframe\"]\n",
        "IFRAME_HEIGHT = 800  #@param {type:\"integer\"}\n",
        "IFRAME_WIDTH = \"100%\"  #@param {type:\"string\"}\n",
        "\n",
        "# ---- Colab output (solo si existe) ----\n",
        "try:\n",
        "    from google.colab import output\n",
        "    _IN_COLAB = True\n",
        "except Exception:\n",
        "    output = None\n",
        "    _IN_COLAB = False\n",
        "\n",
        "# ---- Mata procesos previos (opcional) ----\n",
        "os.system('pkill -f \"streamlit run streamlit_app.py\" || true')\n",
        "os.system('pkill -f \"streamlit\" || true')\n",
        "\n",
        "# ---- Lanza Streamlit ----\n",
        "cmd = [\n",
        "    \"streamlit\", \"run\", \"streamlit_app.py\",\n",
        "    \"--server.address\", \"0.0.0.0\",\n",
        "    \"--server.port\", str(PORT),\n",
        "    \"--server.headless\", \"true\",\n",
        "    \"--server.enableCORS\", \"false\",\n",
        "    \"--server.enableXsrfProtection\", \"false\",\n",
        "]\n",
        "\n",
        "streamlit_process = subprocess.Popen(cmd)\n",
        "time.sleep(3)\n",
        "\n",
        "# ---- Exponer ----\n",
        "if _IN_COLAB and output is not None:\n",
        "    if MODE == \"iframe\":\n",
        "        output.serve_kernel_port_as_iframe(\n",
        "            PORT,\n",
        "            width=IFRAME_WIDTH,\n",
        "            height=IFRAME_HEIGHT,\n",
        "        )\n",
        "    else:\n",
        "        output.serve_kernel_port_as_window(\n",
        "            PORT,\n",
        "            anchor_text=\"ğŸ”— Abrir app Streamlit en una nueva pestaÃ±a\"\n",
        "        )\n",
        "else:\n",
        "    print(f\"Streamlit corriendo en http://127.0.0.1:{PORT} (no-Colab).\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "RFqZU7tnBLHH",
        "kpKY9D-_A1e0"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}